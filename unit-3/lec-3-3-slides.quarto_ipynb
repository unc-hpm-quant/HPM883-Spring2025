{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Tree-based Methods\"\n",
        "subtitle: \"Unit 3.3\"\n",
        "author: \"Sean Sylvia\"\n",
        "format:\n",
        "  revealjs:\n",
        "    theme: default\n",
        "    slide-number: true\n",
        "    chalkboard: true\n",
        "    transition: fade\n",
        "    progress: true\n",
        "    incremental: false\n",
        "    toc: false\n",
        "    scrollable: false\n",
        "    smaller: true\n",
        "    footer: \"UNC HPM 883 - Advanced Quantitative Methods\"\n",
        "draft: true\n",
        "---\n",
        "\n",
        "\n",
        "## Unit 2: Basic ML Crash Course\n",
        "\n",
        "1. Introduction to ML\n",
        "\n",
        "2. Lasso and friends (Linear High-dimensional Regression)\n",
        "\n",
        "3. Tree-based methods (Nonlinear)\n",
        "\n",
        "# Review\n",
        "\n",
        "## Basic ML Setup\n",
        "\n",
        "1. Flexible functional forms\n",
        "\n",
        "2. Limit expressiveness via **regularization**\n",
        "\n",
        "3. Learn how much to regularize **tuning**\n",
        "\n",
        "::: {.fragment}\n",
        "\n",
        "- What do the features imply about properties of $\\hat{f}$ ?\n",
        "- How can we use $\\hat{f}$ in applied data analyses?\n",
        ":::\n",
        "\n",
        "## The Approximation-Overfit Tradeoff\n",
        "\n",
        "::: {.columns}\n",
        "::: {.column width=\"45%\"}\n",
        "**The Fundamental Challenge**\n",
        "\n",
        "As model complexity increases, we face two competing forces:\n",
        "\n",
        "1. **Approximation error decreases** as we better capture the true underlying function\n",
        "2. **Estimation error increases** as we begin to fit noise in our training data\n",
        "\n",
        "This creates the **bias-variance tradeoff** that defines machine learning:\n",
        "\n",
        "- **Simple models**: High bias, low variance\n",
        "- **Complex models**: Low bias, high variance\n",
        "\n",
        ":::\n",
        "\n",
        "::: {.column width=\"55%\"}\n",
        "![Bias-variance tradeoff visualization. The blue curve represents test error, while the red curve shows training error. The gap widens as model complexity increases, indicating overfitting.](media/bias_variance_tradeoff.png)\n",
        ":::\n",
        ":::\n",
        "\n",
        "## Supervised Learning\n",
        "\n",
        "For supervised learners, we need three things:\n",
        "\n",
        "1. Function Class\n",
        "2. A regularizer\n",
        "3. Optimization algorithms to guide us\n",
        "\n",
        "##  Choosing a regularization parameter using k-fold Cross-validation \n",
        "\n",
        "![](media/k-fold.png)\n",
        "\n",
        "## Full ML Exercise \n",
        "\n",
        "![](media/ml-exercise.png)\n",
        "\n",
        "## The Regularization Spectrum: How We Control Complexity\n",
        "\n",
        "Every model class has its own unique form of regularization that controls the bias-variance tradeoff. Understanding this spectrum reveals the fundamental unity behind seemingly diverse machine learning approaches.\n",
        "\n",
        "::: {.columns}\n",
        "::: {.column width=\"100%\"}\n",
        "<table class=\"regularization-table\">\n",
        "  <thead>\n",
        "    <tr>\n",
        "      <th>Function Class</th>\n",
        "      <th>Regularization Parameters</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <td>Linear</td>\n",
        "      <td>LASSO, ridge, elastic net</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td>Decision/regression trees</td>\n",
        "      <td>Depth, leaves, leaf size, info gain</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td>Random forest</td>\n",
        "      <td>Trees, variables per tree, sample sizes, complexity</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td>Nearest neighbors</td>\n",
        "      <td>Number of neighbors</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td>Kernel regression</td>\n",
        "      <td>Bandwidth</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td>Splines</td>\n",
        "      <td>Number of knots, order</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td>Neural nets</td>\n",
        "      <td>Layers, sizes, connectivity, drop-out, early stopping</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        ":::\n",
        ":::\n",
        "\n",
        "::: {.fragment}\n",
        "> **Cross-cutting insight**: While the specific mechanisms differ, regularization always involves restricting a model's capacity to memorize training data, instead encouraging it to generalize underlying patterns. Tree-based methods share this fundamental principle with linear models, but implement it through structural constraints rather than coefficient penalties.\n",
        ":::\n",
        "\n",
        "## Lasso Regression: Constrained Minimization to Regularize\n",
        "\n",
        "::: {.nonincremental}\n",
        "**Objective**: Minimize the sum of squared errors while keeping coefficients small\n",
        ":::\n",
        "\n",
        "::: {.columns}\n",
        "::: {.column width=\"40%\"}\n",
        "### Constrained Form\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "\\min_{\\beta} &\\sum_{i=1}^{n} \\left(y_i - \\beta_0 - \\sum_{j=1}^{p} \\beta_j x_{ij}\\right)^2\\\\\n",
        "\\text{subject to } &\\sum_{j=1}^{p} |\\beta_j| \\leq t\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "- $t \\geq 0$ is the constraint parameter\n",
        "- Smaller $t$ means more regularization\n",
        "- $t = 0$ forces all $\\beta_j = 0$\n",
        ":::\n",
        "\n",
        "::: {.column width=\"60%\"}\n",
        "![](media/lasso-contours.png){width=150%}\n",
        "\n",
        "*Contours of RSS function and lasso constraint region (diamond). Solution occurs at corners, forcing coefficients to zero.*\n",
        ":::\n",
        ":::\n",
        "\n",
        "## Lasso: Lagrangian Form\n",
        "\n",
        "\n",
        "$$\n",
        "\\min_{\\beta} \\sum_{i=1}^{n} \\left(y_i - \\beta_0 - \\sum_{j=1}^{p} \\beta_j x_{ij}\\right)^2 + \\lambda \\sum_{j=1}^{p} |\\beta_j|\n",
        "$$\n",
        "\n",
        "- $\\lambda \\geq 0$ is the penalty parameter\n",
        "- $\\lambda$ and $t$ have an inverse relationship\n",
        "- As $\\lambda \\rightarrow \\infty$, all $\\beta_j \\rightarrow 0$\n",
        "\n",
        "\n",
        "## Geometric Interpretation\n",
        "\n",
        "::: {.columns}\n",
        "::: {.column width=\"50%\"}\n",
        "- **The constraint region**: $\\sum_{j=1}^{p} |\\beta_j| \\leq t$ forms a diamond (L1 norm)\n",
        "- Unlike ridge regression's circular constraint (L2 norm)\n",
        "- **Key insight**: The corners of the diamond often intersect with axes\n",
        "- This means some coefficients become exactly zero\n",
        ":::\n",
        "\n",
        "::: {.column width=\"50%\"}\n",
        "![](media/lasso-contours.png){width=150%}\n",
        "\n",
        "*Contours of RSS function and lasso constraint region (diamond). Solution occurs at corners, forcing coefficients to zero.*\n",
        ":::\n",
        ":::\n",
        "\n",
        "## Lasso Lambda and coefficient paths (relaxing constraint)\n",
        "\n",
        "![Each line is a coefficient. Lambda is \"relaxed\" moving from left to right](media/lambda.png)\n",
        "\n",
        "## Why Lasso Performs Variable Selection\n",
        "\n",
        "::: {.nonincremental}\n",
        "The L1 penalty's diamond shape makes it likely for solutions to occur at corners where some $\\beta_j = 0$\n",
        ":::\n",
        "\n",
        "::: {.columns}\n",
        "::: {.column width=\"35%\"}\n",
        "- Solution occurs where RSS contours touch constraint region\n",
        "- Corners of diamond intersect with coordinate axes\n",
        "- When solution is at a corner, some coefficients equal zero\n",
        "- **Result**: Automatic variable selection\n",
        ":::\n",
        "\n",
        "::: {.column width=\"65%\"}\n",
        "\n",
        "![](media/lasso_vs_ridge_vs_en.png){width=125%}\n",
        "\n",
        "*Comparison of lasso (diamond) vs. ridge (circle) vs. elastic net (fat diamond thingy) constraint regions.*\n",
        ":::\n",
        ":::\n",
        "\n",
        "## The Lasso Selection Problem\n",
        "\n",
        "::: {.nonincremental}\n",
        "**Challenge**: Different regularization paths can lead to different selected variables\n",
        ":::\n",
        "\n",
        "::: {.fragment}\n",
        "**OLS (All Variables):**  \n",
        "Health = β₀ + β₁·Age + β₂·Income + β₃·Education + β₄·SES + ... + βₙ·X_n + ε\n",
        ":::\n",
        "\n",
        "::: {.fragment}\n",
        "**Lasso (λ = 0.1):**  \n",
        "Health = β₀ + β₁·Age + β₂·Income + β₃·Education + <span class=\"faded\">β₄·SES</span> + ... + βₙ·X_n + ε\n",
        ":::\n",
        "\n",
        "::: {.fragment}\n",
        "**Lasso (λ = 0.2):**  \n",
        "Health = β₀ + β₁·Age + <span class=\"faded\">β₂·Income + β₃·Education</span> + β₄·SES + ... + βₙ·X_n + ε\n",
        ":::\n",
        "\n",
        "::: {.fragment}\n",
        "**Key implications:**\n",
        "\n",
        "- Variable selection depends heavily on choice of λ\n",
        "- Highly correlated predictors compete for selection\n",
        "- Different random seeds in cross-validation → different final models\n",
        "- Selection can be unstable with small changes in data\n",
        ":::\n",
        "\n",
        "\n",
        "```{css, echo=FALSE}\n",
        ".faded {\n",
        "  color: #999999;\n",
        "  text-decoration: line-through;\n",
        "}\n",
        "```"
      ],
      "id": "4e4c2df3"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}