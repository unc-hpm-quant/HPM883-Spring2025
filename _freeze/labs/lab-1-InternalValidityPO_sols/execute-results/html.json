{
  "hash": "99b00a656489b7872b524aff648754e8",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Lab 1 Solutions\nsubtitle: The Hospital of Uncertain Outcomes\nauthor: Sean Sylvia\ndate: \"2025-01-22\"\nslug: lab-1\ncategories: \n    - Lab\n    - Internal Validity\ndescription: \"Internal Validity, Potential Outcomes\"\ndraft: false\nformat: html\neditor: visual\nexecute:\n  eval: true\n---\n\n\n\n:::{.callout-note}\n## Results may differ\nThis is only one of many possible ways to complete this lab. Your final code may look different, which is fine! In fact, it is good practice to experiment with different approaches.\n:::\n\n\n\n# Lab 1: The Hospital of Uncertain Outcomes\n\n## Overview and Learning Objectives\n\nIn this lab, we will explore **internal validity** and the **potential outcomes framework** using simulated health data from the endlessly eventful **St. Null’s Memorial Hospital**. Specifically, we will recreate a scenario where a new intervention (putting patients on ventilators) may or may not reduce patient mortality. As you’ll discover, chaos at the hospital has made it far from straightforward to identify causal effects.\n\nBy the end of this lab, you will be able to:\n\n-   Understand the concept of **potential outcomes** and **causal effects**.\n-   Apply **randomization inference** to estimate treatment effects.\n-   Identify **threats to internal validity** and explore possible solutions.\n-   Implement basic **difference-in-means estimation** in R.\n-   Use the **WebR** package to interactively run and modify code.\n\n## Instructions\n\n1.  Open this `.qmd` file in RStudio or another Quarto-supported editor.\n2.  Follow the guided coding prompts below, completing the missing code blocks.\n3.  Submit your completed lab on **Gradescope** \\[Insert Link Here\\] by **\\[Insert Deadline Here\\]**.\n\n## Scenario\n\n### The Hospital of Uncertain Outcomes\n\nWelcome to St. Null’s Memorial Hospital—an institution where the only constant is confusion. The hospital board—led by the well-meaning but trend-obsessed CEO, **Barnaby Beta**—changes policies so often that nobody knows what’s going on.\n\nWorse yet is Dr. P-Hacker, a “data guru” who prefers p-values to patients. He mines the electronic health records (EHR) until something (anything!) is “significant.” Meanwhile, **Nurse Random** tries to keep everything on track, pointing out that good causal methods can be more important than good vibes. Lastly, **Dr. Doub R. Obust** lurks in the background, waiting for a chance to champion doubly robust methods that might someday save everyone’s sanity.\n\n::: {style=\"float: right; width: 480px; margin: 1em;\"}\n<iframe src=\"https://drive.google.com/file/d/1-F2hSgH6J7OKh0a_VwE8pXz3v9fFZAgT/preview\" width=\"480\" height=\"270\" allow=\"autoplay\">\n\n</iframe>\n:::\n\nYou and your team of budding methodologists are the new consultants hired to impose some order on this bedlam. In each module, you’ll tackle another fiasco at St. Null’s—from overfitted AI catastrophes to weird missing-data mishaps—and attempt to restore some semblance of methodological rigor. Good luck!\n\n## Your Mission\n\n### The Pandemic Mystery at St. Null’s\n\nA mysterious respiratory illness has swept through St. Null’s, leaving every ward scrambling. The question at hand is whether putting these patients on ventilators prolongs their lifespans. CEO Barnaby Beta wants quick answers (“If TikTok can do it, so can we!”). Dr. P-Hacker gleefully promises “instant significance,” claiming all he needs is the hospital’s EHR from the past week.\n\nBut Nurse Random, unimpressed, insists that the hospital’s chaotic, ad hoc ventilator assignments will cloud any conclusions. Dr. Doub R. Obust nods knowingly. They call in your team for an unbiased, data-driven approach.\n\nIn this lab, you’ll **simulate a dataset** of 100,000 patients that captures both “potential outcomes” (i.e., what would happen if a patient were ventilated vs. not ventilated). This magical glimpse at parallel universes is impossible in real-world data, but here it will let us see exactly how different analytic approaches fare in the face of selection bias.\n\n# Install Packages if not installed\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrequired_packages = c(\"fixest\", \"data.table\")\nfor(pkg in required_packages){\n  if(!require(pkg, character.only = TRUE)) install.packages(pkg)\n}\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required package: fixest\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required package: data.table\n```\n\n\n:::\n:::\n\n\n\n## Step 1: Simulating the Dataset\n\nBecause the EHR system at St. Null’s is, in a word, “unreliable” (or, in two words, “utterly broken”), we’ll create our own dataset in R.\n\nRun the following code to generate 100,000 patient records along with **potential outcomes** (`y0` if no ventilator, `y1` if ventilated). Each outcome is the patient’s lifespan (in some made-up units). Note that lifespans below zero are set to zero—any negative numbers would just be an artifact of Dr. P-Hacker’s bizarre data extraction methods.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(fixest)\nlibrary(data.table)\nset.seed(072121)\n\n# 100,000 people with differing levels of covid symptoms\nN_people = 100000\ndf = data.table(person = 1:N_people)\n\n# Potential outcomes (Y0): life-span if no vent\ndf[, y0 := rnorm(N_people, 9.4, 4)]\ndf[y0 < 0, y0 := 0]\n\n# Potential outcomes (Y1): life-span if assigned to vents\ndf[, y1 := rnorm(N_people, 10, 4)]\ndf[y1 < 0, y1 := 0]\n```\n:::\n\n\n\nWe also define the **individual treatment effect** for each patient. Dr. Doub R. Obust is thrilled, because for once, we have both `y0` and `y1` simultaneously—an impossible dream in real life!\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Define individual treatment effect\ndf[, delta := y1 - y0]\n```\n:::\n\n\n\n## Step 2: The Two Doctors\n\nSt. Null’s has two very different doctors assigning ventilators:\n\n-   **Dr. Perfect:** A mystical being who can see into each patient’s future and only gives ventilators to those who would benefit from them.\n-   **Dr. Bad:** The name says it all. This doctor assigns ventilators randomly—could be beneficial, could not be. Who knows?\n\n### Step 2a: Assigning Doctors\n\nFirst, we randomly assign each patient to one of these two doctors. (No wonder this hospital is chaotic...)\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Assign doctors randomly\ndf[, doctor := sample(c(\"perfect\", \"bad\"), N_people, replace = TRUE)]\n```\n:::\n\n\n\n### Step 2b: Assigning Ventilators\n\nNext, each doctor does what they do best. Dr. Perfect uses clairvoyance to treat only those who stand to gain (`delta > 0`). Dr. Bad flips a metaphorical coin:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Perfect doctor assigns vents only to those who benefit\ndf[doctor == \"perfect\", vents := (delta > 0)]\n\n# Random doctor assigns vents randomly\ndf[doctor == \"bad\", vents := sample(c(TRUE, FALSE), .N, replace = TRUE)]\n```\n:::\n\n\n\nIt’s not exactly a model of ethical clarity, but it certainly demonstrates the complications of “treatment assignment” in the real world (or the real St. Null’s world).\n\n## Step 3: Computing Causal Parameters\n\nNow, let’s calculate the key causal parameters:\n\n-   **Average Treatment Effect (ATE)**: The overall difference in outcomes if everyone were ventilated vs. if no one were ventilated.\n-   **Average Treatment Effect on the Treated (ATT)**: The effect of ventilation on those who actually *received* ventilation.\n-   **Average Treatment Effect on the Untreated (ATU)**: The effect of ventilation on those who were *not* ventilated.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate all aggregate Causal Parameters (ATE, ATT, ATU)\nate = df[, mean(delta)]\natt = df[vents == TRUE, mean(delta)]\natu = df[vents == FALSE, mean(delta)]\n\ncat(sprintf(\"ATE = %.03f\n\", ate))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nATE = 0.602\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(sprintf(\"ATT = %.03f\n\", att))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nATT = 2.748\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(sprintf(\"ATU = %.03f\n\", atu))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nATU = -1.711\n```\n\n\n:::\n:::\n\n\n\nDr. P-Hacker would stop right here and rejoice: “We have all the significance we need!” But hold your celebratory balloon drop—there’s more to do.\n\n## Step 4: Selection Bias and Realized Outcomes\n\nAlthough the dataset has both `y0` and `y1`, in the real world, a patient’s outcome is observed under only one condition (treated or untreated). Let’s capture *which* outcome we’d actually see based on the ventilator assignment:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Use the switching equation to select realized outcomes from potential outcomes based on treatment assignment\ndf[, y := vents * y1 + (1 - vents) * y0]\n```\n:::\n\n\n\n### Selection Bias Calculation\n\nWe’ll see if there is selection bias by comparing the expected lifespan of ventilated patients *had they not been ventilated* to the expected lifespan of non-ventilated patients.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate EY0 for vent group and no vent group\ney01 = df[vents == TRUE, mean(y0)]  \ney00 = df[vents == FALSE, mean(y0)] \n\n# Calculate selection bias\nselection_bias = (ey01 - ey00)\n\ncat(sprintf(\n  \"Selection Bias = %.03f - %.03f = %.03f \n\", \n  ey01, ey00, selection_bias\n))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSelection Bias = 8.334 - 10.574 = -2.240 \n```\n\n\n:::\n:::\n\n\n\nIf Dr. Perfect is involved, we’d expect some big differences here. Dr. P-Hacker would probably ignore that and claim victory anyway. (He likes ignoring inconvenient truths.)\n\n## Step 5: Comparing Outcomes Between Groups\n\nOne of the simplest ways to estimate the treatment effect is to look at the **Simple Difference in Outcomes (SDO)**—the difference in the observed mean outcome between those who got the treatment (ventilators) and those who did not.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate the share of units treated with vents (pi)\npi = mean(df$vents)\n\n# Manually calculate the simple difference in mean health outcomes\ney1 = df[vents == TRUE, mean(y)]\ney0 = df[vents == FALSE, mean(y)]\nsdo = ey1 - ey0\n\ncat(sprintf(\n  \"Simple Difference-in-Outcomes = %.03f - %.03f = %.03f \n\", \n  ey1, ey0, sdo\n))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSimple Difference-in-Outcomes = 11.082 - 10.574 = 0.509 \n```\n\n\n:::\n:::\n\n\n\nDr. P-Hacker would run around shouting: “Aha! This difference proves the intervention works!” or “Aha! It’s not significant!” depending on the p-value. Let’s see if we can do better.\n\n## Step 6: Estimating the Effect with Regression\n\nTo confirm our findings, let’s do an **Ordinary Least Squares (OLS)** regression of the realized outcome on the ventilator indicator:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Estimate the treatment effect using OLS\nreg = feols(\n  y ~ vents, data = df, \n  vcov = \"hc1\"\n)\n\ncat(\"\n\")\n```\n\n```{.r .cell-code}\nprint(reg)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nOLS estimation, Dep. Var.: y\nObservations: 100,000\nStandard-errors: Heteroskedasticity-robust \n             Estimate Std. Error  t value  Pr(>|t|)    \n(Intercept) 10.573703   0.017534 603.0511 < 2.2e-16 ***\nventsTRUE    0.508554   0.024209  21.0072 < 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 3.82331   Adj. R2: 0.004388\n```\n\n\n:::\n:::\n\n\n\nThis approach, while straightforward, is still subject to any biases from non-random assignment—like, say, Dr. Perfect or Dr. Bad being in charge.\n\n## Step 7: Validating the Decomposition\n\nAt this point, we’d like to see if the **Simple Difference in Outcomes (SDO)** can be broken down into our measured parameters. You’ll fill in a table comparing **Dr. Perfect** to **Dr. Bad**, computing the ATE, ATT, ATU, selection bias, SDO, and so on.\n\nBelow is a quick consistency check to see if the SDO lines up with our theoretical decomposition:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Decomposition check\nsdo_check = ate + selection_bias + (1 - pi) * (att - atu)\n\ncat(sprintf(\"SDO Check = %.03f \n\", sdo_check))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSDO Check = 0.509 \n```\n\n\n:::\n:::\n\n\n\nWhen you fill out the table, you should include:\n\n| Perfect Doctor       | Bad Doctor | Causal Parameter |\n|----------------------|------------|------------------|\n| ATE                  |            |                  |\n| ATT                  |            |                  |\n| ATU                  |            |                  |\n| Selection bias terms |            |                  |\n| E\\[Y0 \\| D=1\\]       |            |                  |\n| E\\[Y0 \\| D=0\\]       |            |                  |\n| Selection bias       |            |                  |\n| Calculations         |            |                  |\n| Pi (share on vents)  |            |                  |\n| SDO manual           |            |                  |\n| SDO OLS              |            |                  |\n| SDO Decomposition    |            |                  |\n| Obs                  |            |                  |\n\n### Reflection\n\nFinally, reflect on these questions:\n\n-   Is the **Simple Difference in Outcomes (SDO)** enough to identify the **ATE**, **ATT**, or **ATU**?\n-   How does **selection bias** play into interpreting the SDO?\n-   What lessons would Nurse Random want Dr. P-Hacker to learn about data and design?\n\n(Extra credit if you can imagine the dramatic showdown when Dr. Doub R. Obust finally unveils a doubly robust method that saves the day—but that’s a story for a future lab!)\n\nThat’s it! You’ve run the gauntlet of the Hospital of Uncertain Outcomes and lived to tell the tale. Now submit your work, and good luck diagnosing more of St. Null’s methodological maladies!\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}