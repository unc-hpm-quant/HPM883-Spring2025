[
  {
    "objectID": "unit-2/lec-2-1.html",
    "href": "unit-2/lec-2-1.html",
    "title": "Unit 2: Design of Experiments",
    "section": "",
    "text": "Welcome, dear students, to another joyous adventure in the realm of experimental design! In our previous sessions, we tackled the fundamentals of statistical conclusion validity and dipped our toes into power calculations and simulation. We even brushed up against the perils of clustering, which appears A LOT in health services research since this is how health care is organized – think hospitals, clinics, communities, social networks, etc.\nNow, we turn the tables. Instead of passively accepting whatever nature (or large administrative database) throws at us, we’re in control. Unlike our secondary data analysis friends, we get to design our experiments to make the most of the resources we have. Specifically, we’re interested in maximizing our abilitly to learn (i.e. statistical power) subject to our constraints (e.g. budget, logistical).\nOf course, nothing is for free; this is both a blessing and a curse. Tradeoffs abound as usual. We need a framework for thinking about how to optimally weigh costs and benefits. If only there was an entire discipline devoted to this…..OH, WAIT! (Yes, my friends, you are all economists now. You’re welcome.) As an experimentalist, you can optimize your design choices in ways our secondary-data-using colleagues can only dream of (or envy, or curse, depending on their temperament). So let’s dig in, shall we?\n\n\n\n\n\n\nNote\n\n\n\nNote: Suboptimal design choices won’t necessarily ruin your study’s internal validity, but they will keep you from making the best inference possible. And that might cost you that sweet, sweet grant renewal next year."
  },
  {
    "objectID": "unit-2/lec-2-1.html#introduction",
    "href": "unit-2/lec-2-1.html#introduction",
    "title": "Unit 2: Design of Experiments",
    "section": "",
    "text": "Welcome, dear students, to another joyous adventure in the realm of experimental design! In our previous sessions, we tackled the fundamentals of statistical conclusion validity and dipped our toes into power calculations and simulation. We even brushed up against the perils of clustering, which appears A LOT in health services research since this is how health care is organized – think hospitals, clinics, communities, social networks, etc.\nNow, we turn the tables. Instead of passively accepting whatever nature (or large administrative database) throws at us, we’re in control. Unlike our secondary data analysis friends, we get to design our experiments to make the most of the resources we have. Specifically, we’re interested in maximizing our abilitly to learn (i.e. statistical power) subject to our constraints (e.g. budget, logistical).\nOf course, nothing is for free; this is both a blessing and a curse. Tradeoffs abound as usual. We need a framework for thinking about how to optimally weigh costs and benefits. If only there was an entire discipline devoted to this…..OH, WAIT! (Yes, my friends, you are all economists now. You’re welcome.) As an experimentalist, you can optimize your design choices in ways our secondary-data-using colleagues can only dream of (or envy, or curse, depending on their temperament). So let’s dig in, shall we?\n\n\n\n\n\n\nNote\n\n\n\nNote: Suboptimal design choices won’t necessarily ruin your study’s internal validity, but they will keep you from making the best inference possible. And that might cost you that sweet, sweet grant renewal next year."
  },
  {
    "objectID": "unit-2/lec-2-1.html#optimal-experimental-design-insights-from-econ-101",
    "href": "unit-2/lec-2-1.html#optimal-experimental-design-insights-from-econ-101",
    "title": "Unit 2: Design of Experiments",
    "section": "1. Optimal Experimental Design: Insights from Econ 101",
    "text": "1. Optimal Experimental Design: Insights from Econ 101\n\n1.1 Our Objective\nThinking back to Econ 101,1 recall that we can pose an optimization problem as maximizing (or minimizing) an objective function subject to constraints. In our case, we’ll use this to set up our experimental design problem, i.e,\n\nObjective function: Statistical power,\nSubject to: Budget constraints.\n\nIn other words, we want to choose our design to maximize power subject to our budget (or other) constraints. It turns out that there are loads of things in our control; usually the only things that aren’t are feasibility and the budget we have to work with.\nConcept Map\n\n\nCode\nflowchart LR\n   %% Nodes\n    A((\"To calculate optimal sample sizes\")):::navy\n    B[\"Desired significance level\"]:::carolinaBlue\n    C[\"Desired statistical power\"]:::carolinaBlue\n    D[\"Minimum detectable effect size\"]:::carolinaBlue\n    E[\"Experimental budget and treatment costs\"]:::carolinaBlue\n    F[\"How the data will be analyzed\"]:::carolinaBlue\n    G[\"Available pre-treatment covariates\"]:::carolinaBlue\n    \n    H[\"Unit of assignment\"]:::teal\n    I[\"Number of distinct outcomes of interest\"]:::teal\n    \n    J([\"Design with clustered assignment\"]):::tarHeelBlue\n    K([\"Design for multiple hypothesis testing\"]):::tarHeelBlue\n\n    %% Edges\n    A --&gt; B\n    A --&gt; C\n    A --&gt; D\n    A --&gt; E\n    A --&gt; F\n    A --&gt; G\n    \n    F --&gt; H\n    F --&gt; I\n    \n    H --&gt; J\n    I --&gt; K\n\n    %% UNC Brand Colors\n    classDef navy fill:#13294B,stroke:#13294B,stroke-width:1px,color:#FFFFFF\n    classDef carolinaBlue fill:#4B9CD3,stroke:#13294B,stroke-width:1px,color:#FFFFFF\n    classDef teal fill:#00788C,stroke:#13294B,stroke-width:1px,color:#FFFFFF\n    classDef tarHeelBlue fill:#7BAFD4,stroke:#13294B,stroke-width:1px,color:#FFFFFF\n\n\n\n\n\nflowchart LR\n   %% Nodes\n    A((\"To calculate optimal sample sizes\")):::navy\n    B[\"Desired significance level\"]:::carolinaBlue\n    C[\"Desired statistical power\"]:::carolinaBlue\n    D[\"Minimum detectable effect size\"]:::carolinaBlue\n    E[\"Experimental budget and treatment costs\"]:::carolinaBlue\n    F[\"How the data will be analyzed\"]:::carolinaBlue\n    G[\"Available pre-treatment covariates\"]:::carolinaBlue\n    \n    H[\"Unit of assignment\"]:::teal\n    I[\"Number of distinct outcomes of interest\"]:::teal\n    \n    J([\"Design with clustered assignment\"]):::tarHeelBlue\n    K([\"Design for multiple hypothesis testing\"]):::tarHeelBlue\n\n    %% Edges\n    A --&gt; B\n    A --&gt; C\n    A --&gt; D\n    A --&gt; E\n    A --&gt; F\n    A --&gt; G\n    \n    F --&gt; H\n    F --&gt; I\n    \n    H --&gt; J\n    I --&gt; K\n\n    %% UNC Brand Colors\n    classDef navy fill:#13294B,stroke:#13294B,stroke-width:1px,color:#FFFFFF\n    classDef carolinaBlue fill:#4B9CD3,stroke:#13294B,stroke-width:1px,color:#FFFFFF\n    classDef teal fill:#00788C,stroke:#13294B,stroke-width:1px,color:#FFFFFF\n    classDef tarHeelBlue fill:#7BAFD4,stroke:#13294B,stroke-width:1px,color:#FFFFFF\n\n\n\n\n\n\n\n\n1.2 A Simple Setup\nTo make this concrete, imagine you’ve received a research grant (yay!) or have a wealthy aunt who’s willing to bankroll your next foray into experimental design. You have two arms in your study:\n\nA control group (no intervention).\n\nA treatment group (some fancy new health intervention).\n\nAnd you have a single, continuous outcome measure, say:\n\\[\nY_i = \\text{Health and Happiness Index}\n\\]\nNow, your big question: How many participants do you need? How do you split that precious sample between treatment and control?\n\n\n1.3 The Big Three Elements\nWhen computing your required sample size (or deciding the “optimal” split), there are three main ingredients:\n\nSignificance level (\\(\\alpha\\)): The probability of a false positive (rejecting the null when it’s actually true).\n\nMinimum Detectable Effect (MDE): The smallest true effect size you want to be able to detect with high probability.\n\nPower (\\(1 - \\beta\\)): The probability of detecting a true effect (i.e., rejecting the null when it’s false).\n\n\n\n\n\n\n\nWarning\n\n\n\nThe MDE is not the effect you expect to see. It’s the smallest effect you care to rule in or rule out. People often mix these up, leading to underpowered studies, heartbreak, and wasted coffee budgets."
  },
  {
    "objectID": "unit-2/lec-2-1.html#the-variance-of-the-average-treatment-effect-ate-and-the-minimum-detectable-effect-mde",
    "href": "unit-2/lec-2-1.html#the-variance-of-the-average-treatment-effect-ate-and-the-minimum-detectable-effect-mde",
    "title": "Unit 2: Design of Experiments",
    "section": "2. The Variance of the Average Treatment Effect (ATE) and the Minimum Detectable Effect (MDE)",
    "text": "2. The Variance of the Average Treatment Effect (ATE) and the Minimum Detectable Effect (MDE)\nAt this point, you may be wondering: “Why do we keep obsessing about the variance of our estimated effect?” Because variance is basically the Grim Reaper of statistical power—bigger variance, bigger standard errors, lower power. So let’s peek under the hood of our ATE estimator.\n\n2.1 Setting Up the Outcome Model\nConsider the outcome \\(Y_i\\) for subject \\(i\\). Under treatment \\(D=1\\) and control \\(D=0\\), \\(Y_i\\) is influenced by:\n\nObservable variables \\(\\mathbf{X}_i\\).\n\nAn unobserved effect \\(\\alpha_i\\) (think “innate personal quirks”).\n\nA person-specific treatment effect \\(\\tau_i\\), whose expectation is zero (\\(\\mathbb{E}[\\tau_i] = 0\\)). (We typically assume the average of these individual effects is our main parameter, \\(\\bar{\\tau}\\).)\n\nAn error term \\(\\varepsilon_i\\), assumed to be i.i.d. (pure luck-of-the-draw stuff).\n\nSo a possible model could be:\n\\[\nY_{i} \\;=\\; \\alpha_i \\;+\\; \\mathbf{X}_i \\,\\beta \\;+\\; \\bar{\\tau} \\,D_i \\;+\\; \\tau_i\\,D_i \\;+\\; \\varepsilon_i.\n\\tag{1}\n\\]\nHere, \\(\\bar{\\tau}\\) is the “average” treatment effect across individuals (the main star of our show), and \\(\\tau_i\\) captures the idiosyncratic difference around that average.\nBecause randomization ensures \\(D_i\\) is (in expectation) independent of \\(\\alpha_i, \\tau_i,\\) and \\(\\varepsilon_i\\), your estimator for the ATE:\n\\[\n\\hat{\\tau}\n\\;=\\;\n\\mathbb{E}[Y_i \\mid D_i=1]\n\\;-\\;\n\\mathbb{E}[Y_i \\mid D_i=0],\n\\]\nis unbiased. Translation: we’re not systematically off the mark. If the true effect is 2, we’re not going to estimate 1.6 or 2.7 just because the allocation was rigged.\nBut to actually detect \\(\\hat{\\tau}\\) in a statistical test, we need the noise to be sufficiently small relative to the signal. Enter the variance of the estimated ATE:\n\\[\n\\mathrm{Var}(\\hat{\\tau})\n\\;=\\;\n\\frac{\\sigma^2}{N}\n\\;=\\;\n\\frac{\\mathrm{Var}(\\varepsilon_i)}{\\;N \\,\\times\\, \\mathrm{Var}(D_i)\\,}.\n\\tag{2}\n\\]\nHere,\n\n\\(\\mathrm{Var}(\\varepsilon_i)\\): is the variance of the unobserved “noise” in outcomes.\n\\(N\\): is the total number of units (subjects).\n\n\\(\\mathrm{Var}(D_i)\\): is the variance of your treatment assignment indicator. If the treatment is binary and \\(p\\) is the fraction in treatment, then \\(\\mathrm{Var}(D_i) = p(1-p)\\).\n\nHence, \\(\\mathrm{Var}(\\hat{\\tau})\\) is:\n\nIncreasing in \\(\\mathrm{Var}(\\varepsilon_i)\\): More “noise” = bigger standard errors.\n\nDecreasing in \\(N\\): More data = smaller standard errors.\n\nDecreasing in \\(\\mathrm{Var}(D_i)\\)\n\nIf you only have one treatment arm with proportion (p) under treatment, then ((D_i) = p(1-p)). So yes, that half-and-half split is not just for black-and-white cookies—it’s also a straightforward way to keep variance in check.\nThat’s the core story. Once we drag real-world complexities in—like different costs per treatment participant, cluster randomization, or multiple waves of data—things get more involved. (Stay tuned!) But for now, remember:\n\nThe formula \\(\\mathrm{Var}(\\hat{\\tau}) = \\sigma^2 / [N \\,\\mathrm{Var}(D)]\\) is your guiding light.\n\nKeep that variance down and your power up."
  },
  {
    "objectID": "unit-2/lec-2-1.html#the-minimum-detectible-effect-mde",
    "href": "unit-2/lec-2-1.html#the-minimum-detectible-effect-mde",
    "title": "Unit 2: Design of Experiments",
    "section": "2.2 The Minimum Detectible Effect (MDE)",
    "text": "2.2 The Minimum Detectible Effect (MDE)\nNow for our simple case, let’s assume that a single treatment results in (conditional) outcomes \\(Y_0\\) and \\(Y_1\\) that are normally distributed for simplicity, i.e.:\n\\[\nY_{i0} | X_i \\sim \\mathcal{N}(\\mu_0, \\sigma^2) if D_i=0;\nY_{i1} | X_i \\sim \\mathcal{N}(\\mu_1, \\sigma^2) if D_i=1.\n\\]\nHere, \\(\\mu_0\\) and \\(\\mu_1\\) are the treatment effects, and \\(\\sigma^2\\) is the variance of the outcome.\nWe can now define the minimum detectable effect (MDE) as the smallest effect \\(\\mu_1 - \\mu_0\\) that we can detect with a specified level of statistical power. Remember, to calculate statistical power, we need to specify a null hypothesis and an alternative hypothesis:\n\\[\nH_0: \\mu_0 = 0\nH_1: \\mu_1 \\neq 0\n\\]\nIf observations are independent, then the difference in sample means (\\(\\bar{Y}_0 - \\bar{Y}_1\\)) is our estimator of \\(\\mu_1 - \\mu_0\\). we can define the MDE as:\n\\[\n\\text{MDE} = z_{1-\\alpha/2} \\sqrt{\\frac{\\sigma^2}{n_1} + \\frac{\\sigma^2}{n_0}} \\;+\\; z_{1-\\beta} \\sqrt{\\frac{\\sigma^2}{n_1} + \\frac{\\sigma^2}{n_0}}.\n\\]\n(Exact form depends on one- or two-sided tests, but the gist is the same.)\n\n\n\n\n\n\nNote\n\n\n\nTo derive the above, start by defining the probability (\\(\\alpha\\)) of a Type I error and the probability (\\(\\beta\\)) of a Type II error as a function of \\(\\bar{Y}_0 - \\bar{Y}_1\\):\n\\[\nz_{1-\\alpha/2} = \\frac{\\bar{Y}_0 - \\bar{Y}_1}{\\sqrt{\\frac{\\sigma_0^2}{n_0} + \\frac{\\sigma_1 ^2}{n_1}}} \\implies\n\\bar{Y}_0 - \\bar{Y}_1 \\;=\\; z_{1-\\alpha/2} \\sqrt{\\frac{\\sigma_0^2}{n_0} + \\frac{\\sigma_1 ^2}{n_1}}\n\\]\nwhere \\(n_0\\) and \\(n_1\\) are the sample sizes in each group and \\(\\sigma_0^2\\) and \\(\\sigma_1^2\\) are the (conditional) variances of the outcomes in each group.\nThe probability, \\(\\beta\\), of a Type II error is:\n\\[\nz_{\\beta} = \\frac{\\bar{Y}_0 - \\bar{Y}_1 - MDE}{\\sqrt{\\frac{\\sigma_0^2}{n_0} + \\frac{\\sigma_1 ^2}{n_1}}} \\implies\n\\bar{Y}_0 - \\bar{Y}_1 \\;=\\; MDE - z_{\\beta} \\sqrt{\\frac{\\sigma_0^2}{n_0} + \\frac{\\sigma_1 ^2}{n_1}}\n\\]\nand the MDE is:\n\\[\n\\text{MDE} = z_{1-\\alpha/2} \\sqrt{\\frac{\\sigma^2}{n_1} + \\frac{\\sigma^2}{n_0}} \\;+\\; z_{\\beta} \\sqrt{\\frac{\\sigma^2}{n_1} + \\frac{\\sigma^2}{n_0}}.\n\\]\n\n\nGraphically, the MDE is effect size \\(\\tau\\) such that power has the same cutoff as the significance level:\n\n\n\nType 1 and Type 2 Errors\n\n\nNow to reiterate a very important point that people mess up all the time: The MDE is not the expected effect size. It is the minimum effect size that you want to be able to detect. Think about how much underpowered research is out there because people make this simple mistake."
  },
  {
    "objectID": "unit-2/lec-2-1.html#figuring-out-optimal-sample-sizes-a.k.a.-minimizing-the-mde",
    "href": "unit-2/lec-2-1.html#figuring-out-optimal-sample-sizes-a.k.a.-minimizing-the-mde",
    "title": "Unit 2: Design of Experiments",
    "section": "3. Figuring Out Optimal Sample Sizes (a.k.a. Minimizing the MDE)",
    "text": "3. Figuring Out Optimal Sample Sizes (a.k.a. Minimizing the MDE)\nLet’s turn now to the practical question on everyone’s mind: How many participants should I recruit in each arm of my study? Or put more formally, How do I minimize the Minimum Detectable Effect (MDE) given constraints on my time, budget, and sanity?\nAssume that the variances are equal in the treatment and control group. We can rewrite the MDE formula as:\n\\[\n\\text{MDE}\n\\;=\\;\n\\bigl(z_{1-\\alpha/2} + z_{1-\\beta}\\bigr)\n\\sqrt{\n  \\frac{\\sigma^2}{n_0} + \\frac{\\sigma^2}{n_1}\n},\n\\]\nwhere - \\(n_1\\) is the sample size in treatment,\n- \\(n_0\\) is the sample size in control, and\n- \\(\\sigma^2\\) is the (common) variance of the outcome.\nBecause the stuff before the square root is just a constant, minimizing the \\(\\text{MDE}\\) with respect to \\((n_0, n_1)\\) subject to a total sample size \\(N = n_0 + n_1\\), boils down to minimizing\n\\[\n\\sqrt{\\frac{1}{n_0} + \\frac{1}{n_1}}.\n\\]\nThen because\n\\[\n\\frac{1}{n_0} + \\frac{1}{n_1}\n\\;=\\;\n\\frac{n_0 + n_1}{n_0 \\,n_1}\n\\;=\\;\n\\frac{N}{n_0\\,n_1},\n\\]\nwe want tomaximize \\(n_0\\,n_1\\) given \\(n_0 + n_1 = N\\).2\nHence, for equal variances and no cost differences, we end up with the classic result:\n\\[\nn_0^*\n\\;=\\;\nn_1^*\n\\;=\\;\n\\frac{N}{2}.\n\\]\nSo a 50–50 split is optimal under those assumptions. Easy enough!\n\nBut what if the variance in the treatment group, \\(\\sigma_1^2\\), differs from that in the control group, \\(\\sigma_0^2\\)? Then the formula for the MDE looks more like:\n\\[\n\\text{MDE}\n\\;=\\;\n\\bigl(z_{1-\\alpha/2} + z_{1-\\beta}\\bigr)\n\\sqrt{\n  \\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_0^2}{n_0}\n}.\n\\]\nMinimizing that suggests you should oversample the group with the higher variance. (Intuition: you get more “statistical bang” for each extra participant in the group that has the largest contribution to the total standard error.)\nOf course, we rarely know \\(\\sigma_0^2\\) and \\(\\sigma_1^2\\) exactly in advance, so we often do a best guess from pilot studies, previous literature, or sheer optimism (spoiler: that last one sometimes backfires)."
  },
  {
    "objectID": "unit-2/lec-2-1.html#enter-the-budget-constraint-when-aunties-not-that-rich",
    "href": "unit-2/lec-2-1.html#enter-the-budget-constraint-when-aunties-not-that-rich",
    "title": "Unit 2: Design of Experiments",
    "section": "4. Enter the Budget Constraint (When Auntie’s Not That Rich)",
    "text": "4. Enter the Budget Constraint (When Auntie’s Not That Rich)\nIn real life, each participant in the treatment group might cost more because you have to fund the intervention (hire staff, buy supplies, bribe them with candy, etc.). Meanwhile, enrolling a control participant might be cheaper—but still not free, because you need them to fill out surveys or come in for labs. Let’s denote:\n\n\\(c_0\\): the cost per control participant,\n\n\\(c_1\\): the cost per treatment participant, and\n\n\\(M\\): your total budget.\n\nNow your problem is:\n\\[\n\\min_{n_0,\\,n_1}\n\\text{MDE}\n\\quad\n\\text{subject to}\n\\quad\nc_0\\,n_0 + c_1\\,n_1 \\;\\le\\; M.\n\\]\nSolving for \\(n_0\\) and \\(n_1\\) yields:\n\\[\n\\frac{n_1}{n_0}\n\\;=\\;\n\\sqrt{\n  \\frac{\\sigma_1^2\\,c_0}{\\sigma_0^2\\,c_1}\n},\n\\]\nwhich tells us:\n\nIf \\(c_1\\) is much higher than \\(c_0\\) (treatment is expensive!), you’ll want fewer participants in treatment.\n\nIf \\(\\sigma_0^2 &gt; \\sigma_1^2\\) (control variance is larger), you might want more control participants.\n\nEssentially, you compare the ratio of your marginal benefit (reducing variance) to your marginal cost (how expensive it is to add participants to each arm). You buy the “cheapest variance reduction” first—like a true economist.\n\nPractical Implications\n\nNo Free Lunch: Even if you have philanthropic relatives, resources are finite. Incorporating cost differences can shift your allocation away from the standard 50–50.\n\nPilot or Prior Data: Because the formula involves \\(\\sigma_0^2\\) and \\(\\sigma_1^2\\), it helps to get decent estimates of group variances so you don’t design suboptimally.\n\nSensitivity Checks: If you’re not sure about the cost ratio or the variance ratio, do a few “what if” analyses or simulations to see how your MDE changes when you tweak these assumptions.\n\n\n\n\n\n\n\nNote\n\n\n\nPro Tip: In many health interventions, the cost difference can be huge—especially if the treatment requires lots of staff time, medical supplies, or specialized devices. Don’t ignore that in your design.\n\n\n\n\nToo much math?\nI know this is a lot of math, so to help you visualize what’s going on, I’ve created a Shiny app that lets you play around with different values and see how the “optimal” \\(n_0\\) and \\(n_1\\) changes. Play around with the parameters and see what happens.\nYou can access the app here: Shiny App\nUnderstanding these trade‐offs is the heart of optimal experimental design. If your control group is nearly free, sample the heck out of it; if your intervention is super expensive, you might want fewer treatment participants but enough to be adequately powered."
  },
  {
    "objectID": "unit-2/lec-2-1.html#design-extensions",
    "href": "unit-2/lec-2-1.html#design-extensions",
    "title": "Unit 2: Design of Experiments",
    "section": "5. Design Extensions",
    "text": "5. Design Extensions\n\n5.1 Dichotomous Treatment with a Binomial Outcome\nWhen the outcome is binary (e.g., success/failure) and the null is that treatment = control, variance depends on the underlying mean. If \\(\\bar{p}_1\\) and \\(\\bar{p}_0\\) differ, you get different variances. Optimal allocation tries to oversample the arm whose proportion is nearer 0.5, because that’s where variance is highest.\n\n\n5.2 Multiple Treatment Arms or “Dose-Response” Designs\nNow until now we’ve just considered one treatment group and one control group. There are some cases where instead of just estimating an effect of a treatment, we might be interested in estimating a dose response curve. So this would be like asking what is the effect of going from a dose of 100 milligrams to 200 milligrams, etc.\nNow if we only care about a linear effect, or more precisely, we are assuming that there’s a linear effect in the dose, we really only need two arms of the experiment, e.g. a zero dose (control) and 200 mg. This puts us back into the treatment and control scenario we saw before.\nBut often, and actually usually, it’s not a good idea to assume a linear effect. For example, the effect of going from zero to 100 milligrams might be a lot larger than the effect of taking a patient from 100 milligrams to 200 milligrams.\nNow it turns out here as you expand treatment levels, the sample allocation is not equal even if you’re assuming the same variances and have equal costs. Not assuming any funny business with differences in outcome variance or unequal costs, optimal allocations are as follows for different polynomials:\n\n\n\n\n\n\n\n\nHighest & Only Polynomial Order\nNumber of Treatment Cells\nSample Allocation\n\n\n\n\n1\n2\n{1/2, 0, 1/2}\n\n\n2\n3\n{1/4, 1/2, 1/4}\n\n\n3\n4\n{1/6, 1/3, 1/3, 1/6}\n\n\n…\n…\n…\n\n\n10\n11\n{1/20, 1/10, …, 1/10, 1/20}\n\n\n\n\n\n5.3 Clustered Designs\nIn many health economics studies, entire clinics, schools, or communities get randomized. Let \\(\\rho\\) be the intracluster correlation coefficient (ICC). A higher \\(\\rho\\) means participants within a cluster look more alike, so you effectively have fewer independent observations. That means you need a bigger total sample (and more clusters) to achieve the same power. The formula for the number of clusters needed often looks like:\n\n\n\n\n\n\nCluster Randomized Designs\n\n\n\n\\[\nn \\;\\approx\\; \\left(\\frac{z_{1-\\alpha/2} + z_{1-\\beta}}{MDE}\\right)^2\n\\bigl(1 + (\\bar{m}-1)\\rho\\bigr)\\sigma^2,\n\\] where \\(MDE\\) is your effect size and \\(\\bar{m}\\) is the average cluster size."
  },
  {
    "objectID": "unit-2/lec-2-1.html#tips-and-tricks-to-increase-power",
    "href": "unit-2/lec-2-1.html#tips-and-tricks-to-increase-power",
    "title": "Unit 2: Design of Experiments",
    "section": "6. Tips and Tricks to Increase Power",
    "text": "6. Tips and Tricks to Increase Power\nLucky for us, there are several strategies to squeeze more power out of your design. We’ll focus on a few big ones.\n\n6.1 Designing Your Study to Maximize Compliance\nEnsure high take-up rates of the treatment. Low participation dilutes the observable effect, making detection challenging. If only half the sample adopts the treatment, you would need four times as many units to detect the same effect as with full participation.3\nConsider strategies like smaller but more motivated populations, or supportive interventions that encourage use. Implementation scientists are your friends here (just feed them coffee and data).\n\n\n6.2 Choosing (Less Noisy) Outcome Measures\nIf your chosen outcome is extremely noisy (maybe self-reported “happiness” on a 0–100 scale?), you’ll need an enormous sample to detect moderate effects.\nAccurate measurements reduce variability in your data. Employ techniques such as embedding consistency checks in surveys, using anchoring and triangulation methods, and considering multiple questions to assess the same concept, thereby averaging out noise.\nAnother trick is to measure outcomes repeatedly and average them. For example, do two lab tests for the same biomarker and average them. Or measure mental health weekly for 4 weeks. Each additional measurerement can reduce the measurement error component.\n\n\n6.3 Multiple Waves of Baseline and Endline Data\nCollecting more than one wave of baseline and endline data can yield substantial power gains if your outcome is either noisy or only weakly autocorrelated. Here are the key ideas from the paper by McKenzie in the reading:\n\nWhy does it help?\n\nWhen outcomes are less autocorrelated (think sporadic monthly income or ephemeral daily health measures), a single baseline and a single endline might not capture the dynamic changes well. Multiple waves help average out the noise.\n\nYou can pick up more precise estimates of changes over time and reduce the standard error.\n\nPractical considerations\n\nWith a fixed budget, you often face a trade-off: Do I survey the same individuals multiple times, or do I get more individuals in fewer waves?\n\nIf your outcome is highly autocorrelated (e.g., certain test scores, stable biometrics), you might not gain as much from multiple waves.\n\nIf your outcome is noisy and less autocorrelated—like short-term business profits or daily stress measurements—multiple waves can dramatically reduce variance.\n\nImplementation\n\nIn practice, plan for multiple data collection rounds, at least for the key outcome variables. If your budget is tight, think carefully about the ratio of cross-sectional coverage (number of individuals) to time-series coverage (number of repeated measurements).\n\nAnalytical methods\n\nUsing repeated measures properly often entails repeated-measures ANOVA or difference-in-differences approaches (when you have a baseline plus multiple follow-ups).\n\nIf the treatment might change the autocorrelation of the outcome, consider that in your power calculations (there are advanced methods for this, but the main takeaway: be mindful of potential changes in variance structure).\n\n\nIn short: More waves = more data points = smaller standard errors (usually). That’s a formula for improved power if the correlation structure isn’t working against you.\n\n\n6.4 Including Covariates in the Estimation Model\nIf you can incorporate pre-treatment covariates that predict outcomes, you can reduce residual variance. Even though randomization ensures that on average groups are balanced, controlling for strong predictors of the outcome yields a more precise estimate.\n\nPre-treatment outcomes are gold if your outcome is stable over time.\n\nStratification variables used in random assignment can (and often should) be controlled in analysis.\n\nBaseline characteristics that strongly predict your outcome.\n\nBe warned: never include post-treatment variables that might be affected by the intervention. That’s a sure-fire path to bias.\n\n\n6.5 Implement Stratified Randomization\nBy dividing the sample into strata based on characteristics related to the outcome and randomizing within these strata, you can control for confounding variables and reduce variance, leading to more precise estimates.\n\n\n\n\n\n\nNote\n\n\n\nThis will be a major topic in the next unit."
  },
  {
    "objectID": "unit-2/lec-2-1.html#concluding-thoughts",
    "href": "unit-2/lec-2-1.html#concluding-thoughts",
    "title": "Unit 2: Design of Experiments",
    "section": "7. Concluding Thoughts",
    "text": "7. Concluding Thoughts\nExperimentation is exciting because you control the design. Yet with great power comes great responsibility: you have to juggle sample sizes, cost constraints, intracluster correlations, multiple time points, and the dreaded compliance issue. The big takeaway is to think through these choices up front rather than scramble in the final hour.\n\nOptimal sample size depends on the usual suspects: significance level, power, MDE, variance, and cost.\nAllocation across treatment arms is rarely a simple 50/50 if costs or variances differ.\nClustering matters—a lot—especially in health policy research.\nCovariates, compliance, outcome selection, multiple waves are your friends when battling the tyranny of limited resources.\n\nThat’s it for now. Next time, we’ll look at more advanced randomization strategies (e.g., stratified, matched-pair, and adaptive designs) that can further improve your power."
  },
  {
    "objectID": "unit-2/lec-2-1.html#footnotes",
    "href": "unit-2/lec-2-1.html#footnotes",
    "title": "Unit 2: Design of Experiments",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIf you were daydreaming in Econ 101, fear not. We’ll keep things simple.↩︎\nif you want to maximize the product of two nonnegative numbers with a fixed sum, you set them equal.↩︎\nsee: World Bank Blog↩︎"
  },
  {
    "objectID": "unit-2/unit-2-design-1.html",
    "href": "unit-2/unit-2-design-1.html",
    "title": "Unit 2.1: Optimal Experimental Design",
    "section": "",
    "text": "Read:\n\nList, Sadoff, Wagner, “So you want to run an experiment, now what? Some simple rules of thumb for optimal experimental design”\nOptional:\n\nSection 4 of Duflo, Glennerster, and Kremer, “USING RANDOMIZATION IN DEVELOPMENT ECONOMICS RESEARCH: A TOOLKIT”\nMcKenzie,Beyond baseline and follow-up: The case for more T in experiments\nWorld Bank Blog: Seven Ways to Improve Statistical Power without Increasing n\n\n\n\n\n\n\n\nLecture Notes: Optimal Experimental Design\n\nLecture Slides\n\nOptimal Experimental Design Shiny App\n\n\n\n\n\nNone yet!",
    "crumbs": [
      "Unit 2: Design of Experiments",
      "Unit 2.0: Optimal Experimental Design"
    ]
  },
  {
    "objectID": "unit-2/unit-2-design-1.html#unit-overview",
    "href": "unit-2/unit-2-design-1.html#unit-overview",
    "title": "Unit 2.1: Optimal Experimental Design",
    "section": "",
    "text": "Read:\n\nList, Sadoff, Wagner, “So you want to run an experiment, now what? Some simple rules of thumb for optimal experimental design”\nOptional:\n\nSection 4 of Duflo, Glennerster, and Kremer, “USING RANDOMIZATION IN DEVELOPMENT ECONOMICS RESEARCH: A TOOLKIT”\nMcKenzie,Beyond baseline and follow-up: The case for more T in experiments\nWorld Bank Blog: Seven Ways to Improve Statistical Power without Increasing n\n\n\n\n\n\n\n\nLecture Notes: Optimal Experimental Design\n\nLecture Slides\n\nOptimal Experimental Design Shiny App\n\n\n\n\n\nNone yet!",
    "crumbs": [
      "Unit 2: Design of Experiments",
      "Unit 2.0: Optimal Experimental Design"
    ]
  },
  {
    "objectID": "computing.html",
    "href": "computing.html",
    "title": "Computing",
    "section": "",
    "text": "This page is a guide for “best practices” for writing code and working with data. It is a work in progress, so please let Sean or the TA know if you have any suggestions for improvement.\n\n\n\n\n\n\nThis is meant as a general quide. For specific instructions on completing labs and assignments, please refer to the lab and assignment instructions.\n\n\n\n\n\n\nIPA Data Science, Engineering, and Technology Handbook\nCoding for Economists\n\n\n\n\n\n\nA package manager helps to standardize how you install and update software on your computer. Generally, you want to use a package manager to install any programs that are used globally on your computer. By “globally”, we mean that it is a program that is used across many projects and computing environments.\n\n\nIf using a Windows computer with Windows 10 or later, use the Windows Package Manager, winget.\nExample:\n# Install a single program (e.g. GitHub for command line)\nwinget install GitHub.cli\n\n\n\nIf using a macOS computer, use Homebrew.\nExample:\n# Install a single program (e.g. GitHub for command line)\nbrew install gh\n\n\n\n\n# Windows Install r\nwinget install --id R-project.R\n\n# macOS Install R\nbrew install --cask r\n\n\n\n\n\n\n\nScared of R? Not to worry! You’ll pick it up as we go - I promise! During class we will go through code slowly at first and spend some time explaining what each line of code does. You’ll be R literate in no time!\nIf you do want to learn a bit of R on your own, here are some excellent resources:\n\nR for Data Science by Hadley Wickham\n\n\n\nNope, you cannot turn in assignments in Stata, sorry!  Why? It’s not just that it’s much harder to run the class when we’re using different software (it is!), but there are several reasons that I strongly believe it is best for you to become comfortable with R:\n\nR is FREEEEE\nR has an open source ecosystem. This means\n\nImmediate access to cutting-edge statistical methods through community contributions\nExtensive library of packages for specialized analyses\nIntegration capabilities with other tools like Git, Python, and database systems\n\nR is just better for advanced data analysis\n\nAbility to handle multiple datasets simultaneously\nSuperior graphical capabilities\nBetter tools for web scraping, JSON parsing, and database querying\n\nIn industry, evidence that R has 5x greater demand than Stata on the job market\n\n\n\n\n\n\n\nLearning Strategy\n\n\n\nRather than directly mapping Stata commands to R, it’s more effective to first understand R’s fundamental concepts and then gradually build up to specific tasks like data management and regression models.\n\n\nHere are some excellent tools for Stata people to learn R:\n\nStata2R Website\nWorld Bank DIME R for Advanced Stata Users Workshop",
    "crumbs": [
      "Computing"
    ]
  },
  {
    "objectID": "computing.html#useful-guides",
    "href": "computing.html#useful-guides",
    "title": "Computing",
    "section": "",
    "text": "IPA Data Science, Engineering, and Technology Handbook\nCoding for Economists",
    "crumbs": [
      "Computing"
    ]
  },
  {
    "objectID": "computing.html#computer-setup",
    "href": "computing.html#computer-setup",
    "title": "Computing",
    "section": "",
    "text": "A package manager helps to standardize how you install and update software on your computer. Generally, you want to use a package manager to install any programs that are used globally on your computer. By “globally”, we mean that it is a program that is used across many projects and computing environments.\n\n\nIf using a Windows computer with Windows 10 or later, use the Windows Package Manager, winget.\nExample:\n# Install a single program (e.g. GitHub for command line)\nwinget install GitHub.cli\n\n\n\nIf using a macOS computer, use Homebrew.\nExample:\n# Install a single program (e.g. GitHub for command line)\nbrew install gh\n\n\n\n\n# Windows Install r\nwinget install --id R-project.R\n\n# macOS Install R\nbrew install --cask r",
    "crumbs": [
      "Computing"
    ]
  },
  {
    "objectID": "computing.html#using-r",
    "href": "computing.html#using-r",
    "title": "Computing",
    "section": "",
    "text": "Scared of R? Not to worry! You’ll pick it up as we go - I promise! During class we will go through code slowly at first and spend some time explaining what each line of code does. You’ll be R literate in no time!\nIf you do want to learn a bit of R on your own, here are some excellent resources:\n\nR for Data Science by Hadley Wickham\n\n\n\nNope, you cannot turn in assignments in Stata, sorry!  Why? It’s not just that it’s much harder to run the class when we’re using different software (it is!), but there are several reasons that I strongly believe it is best for you to become comfortable with R:\n\nR is FREEEEE\nR has an open source ecosystem. This means\n\nImmediate access to cutting-edge statistical methods through community contributions\nExtensive library of packages for specialized analyses\nIntegration capabilities with other tools like Git, Python, and database systems\n\nR is just better for advanced data analysis\n\nAbility to handle multiple datasets simultaneously\nSuperior graphical capabilities\nBetter tools for web scraping, JSON parsing, and database querying\n\nIn industry, evidence that R has 5x greater demand than Stata on the job market\n\n\n\n\n\n\n\nLearning Strategy\n\n\n\nRather than directly mapping Stata commands to R, it’s more effective to first understand R’s fundamental concepts and then gradually build up to specific tasks like data management and regression models.\n\n\nHere are some excellent tools for Stata people to learn R:\n\nStata2R Website\nWorld Bank DIME R for Advanced Stata Users Workshop",
    "crumbs": [
      "Computing"
    ]
  },
  {
    "objectID": "unit-1/HPM883 introductory analysis.html",
    "href": "unit-1/HPM883 introductory analysis.html",
    "title": "HPM883 introductory survey analysis",
    "section": "",
    "text": "install.packages(c(\"dplyr\", \"skimr\", \"cluster\", \"tidyr\", \"ggplot2\", \"knitr\", \"Hmisc\"))\n\nlibrary(dplyr)\nlibrary(skimr)\nlibrary(cluster)\nlibrary(tidyr)\nlibrary(ggplot2)\nlibrary(knitr)\nlibrary(Hmisc)"
  },
  {
    "objectID": "unit-1/HPM883 introductory analysis.html#calculate-borda-count-scores-for-q12how-do-you-like-to-learn",
    "href": "unit-1/HPM883 introductory analysis.html#calculate-borda-count-scores-for-q12how-do-you-like-to-learn",
    "title": "HPM883 introductory survey analysis",
    "section": "Calculate Borda count scores for Q12:How do you like to learn?",
    "text": "Calculate Borda count scores for Q12:How do you like to learn?\n\n### Creat data frame of ranking questions\nq12ranking_df &lt;- data.frame(introsurvey_data$Q1, introsurvey_data$Q12_1, introsurvey_data$Q12_2, introsurvey_data$Q12_3, introsurvey_data$Q12_4, introsurvey_data$Q12_5, introsurvey_data$Q12_6)\n\n### label ranking questions\nlabel(q12ranking_df$introsurvey_data.Q1) &lt;- \"Full Name\"\nlabel(q12ranking_df$introsurvey_data.Q12_1,) &lt;- \"How do you like to learn: Hands-on guided coding practice\"\nlabel(q12ranking_df$introsurvey_data.Q12_2) &lt;- \"How do you like to learn: Group discussions\"\nlabel(q12ranking_df$introsurvey_data.Q12_3) &lt;- \"How do you like to learn: Visual explanations/graphs\"\nlabel(q12ranking_df$introsurvey_data.Q12_4) &lt;- \"How do you like to learn: Independent reading\"\nlabel(q12ranking_df$introsurvey_data.Q12_5) &lt;- \"How do you like to learn: Independent exercises\"\nlabel(q12ranking_df$introsurvey_data.Q12_6) &lt;- \"How do you like to learn: Sleeping with a textbook under my pillow\"\n\n### Function to calculate Borda count:\nq12calculate_borda &lt;- function(q12rankings_df) {\n### Get number of candidates (excluding voter column if present)\nq12n_candidates &lt;- ncol(q12rankings_df)\nif(\"Q1\" %in% colnames(q12rankings_df)) {\n  q12n_candidates &lt;- q12n_candidates - 1\n  }\n  \n### Initialize scores dictionary\nq12scores &lt;- rep(0, q12n_candidates)\nnames(q12scores) &lt;- colnames(q12rankings_df)[!colnames(q12rankings_df) %in% \"Q1\"]\n  \n### Calculate points for each ranking\nfor(i in 1:nrow(q12rankings_df)) {\n  q12ranks &lt;- as.numeric(q12rankings_df[i, !colnames(q12rankings_df) %in% \"Q1\"])\n  \n### Assign points: n-rank points for each position\n  q12points &lt;- q12n_candidates - q12ranks\n  q12scores &lt;- q12scores + q12points\n  }\n  \n### Create results dataframe\n  q12_results &lt;- data.frame(\n    Q12_Candidate = names(q12scores),\n    Q12_Score = q12scores\n  )\n  \n### Sort by score in descending order\n  q12_results &lt;- q12_results[order(-q12_results$Q12_Score),]\n  \n  return(q12_results)\n}\n\n### Calculate Borda count scores\nq12_results &lt;- q12calculate_borda(q12ranking_df)\n\n### Rename candidates\nq12_results$Q12_Candidate[q12_results$Q12_Candidate == \"introsurvey_data.Q1\"] &lt;- \"Full Name\"\nq12_results$Q12_Candidate[q12_results$Q12_Candidate == \"introsurvey_data.Q12_1\"] &lt;- \"How do you like to learn: Hands-on guided coding practice\"\nq12_results$Q12_Candidate[q12_results$Q12_Candidate == \"introsurvey_data.Q12_2\"] &lt;- \"How do you like to learn: Group discussions\"\nq12_results$Q12_Candidate[q12_results$Q12_Candidate == \"introsurvey_data.Q12_3\"] &lt;- \"How do you like to learn: Visual explanations/graphs\"\nq12_results$Q12_Candidate[q12_results$Q12_Candidate == \"introsurvey_data.Q12_4\"] &lt;- \"How do you like to learn: Independent reading\"\nq12_results$Q12_Candidate[q12_results$Q12_Candidate == \"introsurvey_data.Q12_5\"] &lt;- \"How do you like to learn: Independent exercises\"\nq12_results$Q12_Candidate[q12_results$Q12_Candidate == \"introsurvey_data.Q12_6\"] &lt;- \"How do you like to learn: Sleeping with a textbook under my pillow\"\n\n### View Borda count scores\nprint(q12_results)"
  },
  {
    "objectID": "unit-1/HPM883 introductory analysis.html#calculate-borda-count-scores-for-q19-please-rank-the-following-topics-in-order-of-preference",
    "href": "unit-1/HPM883 introductory analysis.html#calculate-borda-count-scores-for-q19-please-rank-the-following-topics-in-order-of-preference",
    "title": "HPM883 introductory survey analysis",
    "section": "Calculate Borda count scores for Q19: Please rank the following topics in order of preference",
    "text": "Calculate Borda count scores for Q19: Please rank the following topics in order of preference\n\n### Creat data frame of ranking questions\nq19ranking_df &lt;- data.frame(introsurvey_data$Q1, introsurvey_data$Q19_1, introsurvey_data$Q19_2, introsurvey_data$Q19_3, introsurvey_data$Q19_4, introsurvey_data$Q19_5, introsurvey_data$Q19_6, introsurvey_data$Q19_7, introsurvey_data$Q19_8, introsurvey_data$Q19_9)\n\n### label ranking questions\nlabel(q19ranking_df$introsurvey_data.Q1) &lt;- \"Full Name\"\nlabel(q19ranking_df$introsurvey_data.Q19_1,) &lt;- \"Topics in order of preference: Policy Learning\"\nlabel(q19ranking_df$introsurvey_data.Q19_2) &lt;- \"Topics in order of preference: Other\"\nlabel(q19ranking_df$introsurvey_data.Q19_3) &lt;- \"Topics in order of preference: Text-as-data\"\nlabel(q19ranking_df$introsurvey_data.Q19_4) &lt;- \"Topics in order of preference: Other\"\nlabel(q19ranking_df$introsurvey_data.Q19_5) &lt;- \"Topics in order of preference: Surrogate Indexes\"\nlabel(q19ranking_df$introsurvey_data.Q19_6) &lt;- \"Topics in order of preference: Double/De-biased Machine Learning\"\nlabel(q19ranking_df$introsurvey_data.Q19_7) &lt;- \"Topics in order of preference: Adaptive Experimental Design\"\nlabel(q19ranking_df$introsurvey_data.Q19_8) &lt;- \"Topics in order of preference: Embeddings\"\nlabel(q19ranking_df$introsurvey_data.Q19_9) &lt;- \"Topics in order of preference: Other\"\n\n### Drop the student who did not fill this question\nq19ranking_df &lt;- q19ranking_df %&gt;% filter(q19ranking_df$introsurvey_data.Q1 != \"Christian Osei\")\nq19ranking_df &lt;- q19ranking_df %&gt;% filter(q19ranking_df$introsurvey_data.Q1 != \"Yeshaben Patel\")\n\n### Function to calculate Borda count:\nq19calculate_borda &lt;- function(q19rankings_df) {\n### Get number of candidates (excluding voter column if present)\nq19n_candidates &lt;- ncol(q19rankings_df)\nif(\"Q1\" %in% colnames(q19rankings_df)) {\n  q19n_candidates &lt;- q19n_candidates - 1\n  }\n  \n### Initialize scores dictionary\nq19scores &lt;- rep(0, q19n_candidates)\nnames(q19scores) &lt;- colnames(q19rankings_df)[!colnames(q19rankings_df) %in% \"Q1\"]\n  \n### Calculate points for each ranking\nfor(i in 1:nrow(q19rankings_df)) {\n  q19ranks &lt;- as.numeric(q19rankings_df[i, !colnames(q19rankings_df) %in% \"Q1\"])\n  \n### Assign points: n-rank points for each position\n  q19points &lt;- q19n_candidates - q19ranks\n  q19scores &lt;- q19scores + q19points\n  }\n  \n### Create results dataframe\n  q19_results &lt;- data.frame(\n    Q19_Candidate = names(q19scores),\n    Q19_Score = q19scores\n  )\n  \n### Sort by score in descending order\n  q19_results &lt;- q19_results[order(-q19_results$Q19_Score),]\n  \n  return(q19_results)\n}\n\n### Calculate Borda count scores\nq19_results &lt;- q19calculate_borda(q19ranking_df)\n\n### Rename candidates\nq19_results$Q19_Candidate[q19_results$Q19_Candidate == \"introsurvey_data.Q1\"] &lt;- \"Full Name\"\nq19_results$Q19_Candidate[q19_results$Q19_Candidate == \"introsurvey_data.Q19_1\"] &lt;- \"Topics in order of preference: Policy Learning\"\nq19_results$Q19_Candidate[q19_results$Q19_Candidate == \"introsurvey_data.Q19_2\"] &lt;- \"Topics in order of preference: Other\"\nq19_results$Q19_Candidate[q19_results$Q19_Candidate == \"introsurvey_data.Q19_3\"] &lt;- \"Topics in order of preference: Text-as-data\"\nq19_results$Q19_Candidate[q19_results$Q19_Candidate == \"introsurvey_data.Q19_4\"] &lt;- \"Topics in order of preference: Other\"\nq19_results$Q19_Candidate[q19_results$Q19_Candidate == \"introsurvey_data.Q19_5\"] &lt;- \"Topics in order of preference: Surrogate Indexes\"\nq19_results$Q19_Candidate[q19_results$Q19_Candidate == \"introsurvey_data.Q19_6\"] &lt;- \"Topics in order of preference: Double/De-biased Machine Learning\"\nq19_results$Q19_Candidate[q19_results$Q19_Candidate == \"introsurvey_data.Q19_7\"] &lt;- \"Topics in order of preference: Adaptive Experimental Design\"\nq19_results$Q19_Candidate[q19_results$Q19_Candidate == \"introsurvey_data.Q19_8\"] &lt;- \"Topics in order of preference: Embeddings\"\nq19_results$Q19_Candidate[q19_results$Q19_Candidate == \"introsurvey_data.Q19_9\"] &lt;- \"Topics in order of preference: Other\"\n\n### View Borda count scores\nprint(q19_results)"
  },
  {
    "objectID": "unit-1/lec-1-1.html",
    "href": "unit-1/lec-1-1.html",
    "title": "Unit 1: Internal Validity",
    "section": "",
    "text": "Internal validity is the cornerstone of experimental design, crucial for establishing a causal relationship between treatment and outcome. It ensures that the observed effects in an experiment can be confidently attributed to the treatment rather than other confounding factors. This lecture explores the core principles and challenges of internal validity, including the role of randomization, exclusion restrictions, and the assignment mechanism. We also discuss practical applications and the complexities of ensuring compliance and observability in experimental settings."
  },
  {
    "objectID": "unit-1/lec-1-1.html#scenario-1",
    "href": "unit-1/lec-1-1.html#scenario-1",
    "title": "Unit 1: Internal Validity",
    "section": "Scenario 1",
    "text": "Scenario 1\nSuppose you are considering whether a new diet is linked to lower risk of inflammatory arthritis.\nYou observe that in a given sample: - A small fraction of individuals on the diet have inflammatory arthritis. - A large fraction of individuals not on the diet have inflammatory arthritis.\nBased on this, you recommend that everyone pursue this new diet, but rates of inflammatory arthritis are unaffected.\nWhat happened?"
  },
  {
    "objectID": "unit-1/lec-1-1.html#scenario-2",
    "href": "unit-1/lec-1-1.html#scenario-2",
    "title": "Unit 1: Internal Validity",
    "section": "Scenario 2",
    "text": "Scenario 2\nConsider a policy designed to reduce emergency room visits by offering free preventive care check-ups. Policymakers observed that communities with higher uptake of preventive care have fewer emergency visits. They implement the policy nationwide, expecting a significant reduction in emergency room visits. However, the policy shows no measurable impact.\nWhat happened?"
  },
  {
    "objectID": "unit-1/lec-1-1.html#explanation",
    "href": "unit-1/lec-1-1.html#explanation",
    "title": "Unit 1: Internal Validity",
    "section": "Explanation",
    "text": "Explanation\nIn each case, you were unable to see what would have happened to each individual if the alternative action had been applied.\nIn scenario 1, it could be that individuals who chose the diet may already have healthier lifestyles, including better exercise and reduced stress levels, which are known to reduce the risk of inflammatory arthritis. The observed differences may be due to these factors rather than the diet itself. Without randomization or controlling for confounding factors, you cannot attribute causality to the diet. The association observed may not reflect the causal effect of the diet.\nIn scenario 2, the observed association between preventive care and lower emergency room visits may reflect that communities with higher uptake of preventive care might already have better healthcare access, socioeconomic conditions, or health awareness—all factors that independently reduce emergency visits. Implementing the policy broadly, without considering these confounders, does not account for the variation in baseline conditions.\nThe lack of this information is what prevents inference about causation from association."
  },
  {
    "objectID": "unit-1/lec-1-1.html#randomization",
    "href": "unit-1/lec-1-1.html#randomization",
    "title": "Unit 1: Internal Validity",
    "section": "Randomization",
    "text": "Randomization\nAs we saw, the key source of bias arises from the assignment mechanism, or why a unit is assigned to treatment or control. Endogeneity can arise from units being assigned logically or based on percieved benefit, which is a function of their characteristics. There are two basic approaches to dealing with endogeneity:\n\nModel-based methods: Model the selection bias and then remove it mechanically\nDesign-based methods: Use the design of the experiment and randomization to remove selection bias\n\nIn randomized experiments, the assignment mechanism is controlled, and therefore known, by the researcher. This is a unique and key difference from naturally-occuring data where the assignment mechanisms is neither controlled nor typically known. The experimental apporach is often referred to as the “gold standard” for estimating casual effects because randomization can be made to ensure that the assignment is independent of individual characteristics, eliminating bias.\nTo satisfy, the basic restrictions on the assignment mechanism for randomized experiments are:\n\nThree Conditions for a Valid Assignment Mechanism\n\nNon-zero probability: Each individual has a positive probability of being assigned to treatment or control.\nIndividualism: Assignments are independent across individuals and do not depend on others’ potential outcomes.\nUnconfoundedness: Treatment assignment is orthogonal to potential outcomes.\n\nThese conditions ensure that observed differences between groups are attributable to the treatment rather than pre-existing differences."
  },
  {
    "objectID": "unit-1/lec-1-1.html#footnotes",
    "href": "unit-1/lec-1-1.html#footnotes",
    "title": "Unit 1: Internal Validity",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis example is adapted from Scott Cunninham’s example in the Mixtape book.↩︎"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Course Schedule",
    "section": "",
    "text": "This page contains the schedule, topics, content and assigments for the semester.\n\n\n\n\n\n\nNote\n\n\n\nThis schedule will be updated as the semester progresses.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nweek\ndow\ndate\nsession\nunit\ntopic\nprepare\nclass\nlab\nlab_sa\ndue\n\n\n\n1\nW\n8 January\nLec\nFoundations\nWelcome; Key Concepts\n\n\n\n\n\n\n\n\n\n2\nM\n13 January\nLec\nFoundations\nGuest Lecture: Running a Randomized Trial\n\n\n\n\n\nIntroductory Survey\n\n\n\nW\n15 January\n-\nFoundations\nReproducible Research\n\n\n\n\n\n\n\n\n\n3\nM\n20 January\n-\nNo Class: MLK Day\n\n\n\n\n\n\n\n\n\nW\n22 January\n-\nInternal Validity\n\n\n\n\n\nClass Canceled\n\n\n4\nM\n27 January\nLec\nInternal Validity\nGeneralized Potential Outcomes Framework; Exclusion Restrictions\n\n\n\n\n\n\n\n\n\n\nW\n29 January\nLab\nInternal Validity\nLab 1: Hospital of Uncertain Outcomes\n\n\n\n\n\n\n\n\n5\nM\n3 February\nLab\nInternal Validity\nLab 1: Hospital of Uncertain Outcomes, Solutions\n\n\n\n\n\nLab 1 Report\n\n\n\nW\n5 February\nLec\nInternal Validity\nStatistical Conclusion Validity; Power\n\n\n\n\n\n\n\n\n\n6\nM\n10 February\n-\nNo Class: Well-being Day\n\n\n\n\n\n\n\n\n\nW\n12 February\nLab\nInternal Validity\nLab 2: Power by Simulation\n\n\n\n\n\n\n\n\n7\nM\n17 February\n-\nInternal Validity\n\n\n\n\n\nLab 2 Report\n\n\n\nW\n19 February\nLec\nDesign of Expeirments\n\n\n\n\n\n\n\n\n8\nM\n24 February\nLec\nDesign of Expeirments\nOptimal Design of Experiments\n\n\n\n\n\n\n\n\n\n\nW\n26 February\nLec\nDesign of Expeirments\nRandomization Techniques; Lab 3 (On your Own)\n\n\n\n\n\n\n\n9\nM\n3 March\nLec\nPreparations\nEthics, Registration and Pre-Analysis Plans\n\n\n\n\n\n\n\n\nW\n5 March\n-\nMIDTERM 1\nExam will cover all material up to and including Feb 26\n\n\n\n\nLab 3 Report (due Friday)\n\n\n10\nM\n10 March\n-\nSpring Break\nHave fun!\n\n\n\n\n\n\n\n\nW\n12 March\n-\nSpring Break\nHave fun!\n\n\n\n\n\n\n\n11\nM\n17 March\nLec\nIntroduction to Machine Learning\nLasso; Penalized Regression; Cross-Validation\n\n\n\n\n\n\n\n\nW\n19 March\nLec\nIntroduction to Machine Learning\nRandom Trees and Random Forests\n\n\n\n\nLab 4 Report (due Friday)\n\n\n12\nM\n24 March\nLec\nHeterogeneity and Moderation\nHeterogeneous Treatment Effects\n\n\n\n\n\n\n\n\nW\n26 March\nLab\nHeterogeneity and Moderation\nLab 4: HTE and Causal Forests\n\n\n\n\n\n\n\n13\nM\n31 March\nLec\nViolations of Internal Validity\nSUTVA and Observability\n\n\n\n\n\n\n\n\nW\n2 April\nLec\nViolations of Internal Validity\nOne and Two-sided Compliance; TOT (LATE)\n\n\n\n\nLab 5 Report (due Friday)\n\n\n14\nM\n7 April\nLab\nViolations of Internal Validity\nLab 5: TOT\n\n\n\n\n\n\n\n\nW\n9 April\n\nMIDTERM 2\n\n\n\n\n\nLab 6 Report\n\n\n15\nM\n14 April\nLec\nMediation and External Validity\nMediation Analysis and Generalizability\n\n\n\n\n\n\n\n\nW\n16 April\nLab\nMediation and External Validity\nLab 6: Theory of Change\n\n\n\n\nProject Pre-Analysis Plan\n\n\n16\nM\n21 April\n\nBonus Topics\nTBD\n\n\n\n\n\n\n\n\nW\n23 April\n\nBonus Topics\nTBD\n\n\n\n\n\n\n\n17\nM\n28 April\n\nWrap-up\nTBD\n\n\n\n\nFinal Project Manuscript & Replication Repository\n\n\n\nW\n30 April\n\nFINAL EXAM",
    "crumbs": [
      "Course Schedule"
    ]
  },
  {
    "objectID": "course-overview.html",
    "href": "course-overview.html",
    "title": "HPM 883Spring 2025",
    "section": "",
    "text": "HPM 883 is an advanced graduate-level course designed to equip PhD students in the Health Policy and Management program with sophisticated quantitative research skills. This course represents the third installment in the quantitative methods sequence, building on the foundational knowledge acquired in HPM 881 and HPM 882.\nThroughout the term, students will delve into advanced quantitative methods pertinent to health services research with a focus on the use of experimental methods and machine learning for causal inference. The course will provide a thorough introduction to machine learning techniques and explore the burgeoning domain of causal machine learning.",
    "crumbs": [
      "Course Information",
      "Overview"
    ]
  },
  {
    "objectID": "course-overview.html#advanced-quantitative-methods-for-health-policy-and-management",
    "href": "course-overview.html#advanced-quantitative-methods-for-health-policy-and-management",
    "title": "HPM 883Spring 2025",
    "section": "",
    "text": "HPM 883 is an advanced graduate-level course designed to equip PhD students in the Health Policy and Management program with sophisticated quantitative research skills. This course represents the third installment in the quantitative methods sequence, building on the foundational knowledge acquired in HPM 881 and HPM 882.\nThroughout the term, students will delve into advanced quantitative methods pertinent to health services research with a focus on the use of experimental methods and machine learning for causal inference. The course will provide a thorough introduction to machine learning techniques and explore the burgeoning domain of causal machine learning.",
    "crumbs": [
      "Course Information",
      "Overview"
    ]
  },
  {
    "objectID": "unit-0/unit-0-foundations-1.html",
    "href": "unit-0/unit-0-foundations-1.html",
    "title": "Unit 0: Foundations",
    "section": "",
    "text": "This unit will provide a foundation for the rest of the course. It will cover the following topics:\n\nWhy experiments and ML for Causal Inference\nCourse structure and tools\nA field experiment in health services research\nComputing for Reproducible Research",
    "crumbs": [
      "Unit 0: Foundations",
      "Unit 0.0: Introduction"
    ]
  },
  {
    "objectID": "unit-0/unit-0-foundations-1.html#unit-overview",
    "href": "unit-0/unit-0-foundations-1.html#unit-overview",
    "title": "Unit 0: Foundations",
    "section": "",
    "text": "This unit will provide a foundation for the rest of the course. It will cover the following topics:\n\nWhy experiments and ML for Causal Inference\nCourse structure and tools\nA field experiment in health services research\nComputing for Reproducible Research",
    "crumbs": [
      "Unit 0: Foundations",
      "Unit 0.0: Introduction"
    ]
  },
  {
    "objectID": "unit-0/unit-0-foundations-1.html#class-1-course-introduction",
    "href": "unit-0/unit-0-foundations-1.html#class-1-course-introduction",
    "title": "Unit 0: Foundations",
    "section": "Class 1: Course Introduction",
    "text": "Class 1: Course Introduction\n\nPreparation\n\nRead: Harrison and List (2004)\n\nPerusall Link: Harrison and List (2004)\n\n\n\n\n\n\n\n\nPerusall\n\n\n\nPerusall is a free online platform that allows you to collaboratively annotate content with your classmates. Here is a link to the Perusall page for this course: HPM 883. If needed, the class enrollment code is SYLVIA-ZXTWH.\n\nAlthough it is a good way to engage with your classmates around the material, it is not required that you comment on Perusall. If you wish, you can just download the readings directly.\n\n\n\nACI Chapters 0 and 2\n\n\n\nIn Class\n\nLecture slides\n\n\n\nLab/Homework\n\n10-15 min Introductory Class Survey (Due Jan 15)\n\n\n\nAdditional Materials",
    "crumbs": [
      "Unit 0: Foundations",
      "Unit 0.0: Introduction"
    ]
  },
  {
    "objectID": "unit-0/unit-0-foundations-3.html",
    "href": "unit-0/unit-0-foundations-3.html",
    "title": "Unit 0: Foundations",
    "section": "",
    "text": "Learn importance of reproducible research\nLearn key principles of reproducible research\nLearn how to design and implement a reproducible research workflow\n\nVersion control using git and github\nLiterate programming using Quarto\nPackage managemenet using renv",
    "crumbs": [
      "Unit 0: Foundations",
      "Unit 0.2: Computing for Reproducible Research"
    ]
  },
  {
    "objectID": "unit-0/unit-0-foundations-3.html#learning-objectives",
    "href": "unit-0/unit-0-foundations-3.html#learning-objectives",
    "title": "Unit 0: Foundations",
    "section": "",
    "text": "Learn importance of reproducible research\nLearn key principles of reproducible research\nLearn how to design and implement a reproducible research workflow\n\nVersion control using git and github\nLiterate programming using Quarto\nPackage managemenet using renv",
    "crumbs": [
      "Unit 0: Foundations",
      "Unit 0.2: Computing for Reproducible Research"
    ]
  },
  {
    "objectID": "unit-0/unit-0-foundations-3.html#class-will-be-remote-only",
    "href": "unit-0/unit-0-foundations-3.html#class-will-be-remote-only",
    "title": "Unit 0: Foundations",
    "section": "Class will be remote-only",
    "text": "Class will be remote-only\nJoin from the comfort of your own home!\nZoom link: https://unc.zoom.us/j/98379053109",
    "crumbs": [
      "Unit 0: Foundations",
      "Unit 0.2: Computing for Reproducible Research"
    ]
  },
  {
    "objectID": "unit-0/unit-0-foundations-3.html#preparation",
    "href": "unit-0/unit-0-foundations-3.html#preparation",
    "title": "Unit 0: Foundations",
    "section": "Preparation",
    "text": "Preparation\n\n1. Read/Watch/Listen:\n\nReview Computing page\nSkim: Reproducibility Lecture\n\n\n\n2. Computing Setup:\nTo ensure a smooth experience during class, please follow these steps:\n\nInstall R (v4.x or later)\n\nCRAN Download or via your preferred package manager.\n\nInstall RStudio or a similar IDE\n\nRStudio Download\n\nInstall Quarto\n\nQuarto Installation – ensures you can render .qmd files.\n\nInstall Git\n\nConfirm Git is accessible from the command line:\ngit --version\nWindows: Git for Windows\n\nmacOS: Use Xcode Command Line Tools or Homebrew.\n\nCreate a GitHub Account\n\nGitHub Signup – used for version control and code sharing.\n\nSet Up Credentials (SSH keys or PAT)\n\nHelps push/pull to GitHub without repeated logins.\n\nGitHub Docs: Connecting to GitHub with SSH\n\nInstall Required R Packages\n\nIn R/RStudio, run:\ninstall.packages(c(\"tidyverse\",\n                   \"renv\",\n                   \"devtools\",\n                   \"broom\",\n                   \"infer\",\n                   \"rmarkdown\",\n                   \"quarto\"))\n\nOptional but Recommended: Set Up Renv\n\nWe will use renv to manage R package versions.\n\nRenv Documentation\n\n\nOnce you have completed the above steps, you should be ready for our in-class activities.",
    "crumbs": [
      "Unit 0: Foundations",
      "Unit 0.2: Computing for Reproducible Research"
    ]
  },
  {
    "objectID": "unit-0/unit-0-foundations-3.html#in-class",
    "href": "unit-0/unit-0-foundations-3.html#in-class",
    "title": "Unit 0: Foundations",
    "section": "In Class",
    "text": "In Class\n\nReproducibility Lecture\nZoom Recording\nRCT Analysis Template Repository",
    "crumbs": [
      "Unit 0: Foundations",
      "Unit 0.2: Computing for Reproducible Research"
    ]
  },
  {
    "objectID": "unit-0/unit-0-foundations-3.html#additional-resources",
    "href": "unit-0/unit-0-foundations-3.html#additional-resources",
    "title": "Unit 0: Foundations",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nHappy with Git and GitHub for the UseR\nBryan, Jennifer, 2018. Excuse Me, Do You Have a Moment to Talk About Version Control?, The American Statistician.",
    "crumbs": [
      "Unit 0: Foundations",
      "Unit 0.2: Computing for Reproducible Research"
    ]
  },
  {
    "objectID": "unit-0/lec-0-3.html",
    "href": "unit-0/lec-0-3.html",
    "title": "Foundations: Reproducible Research",
    "section": "",
    "text": "Reproducible research is, quite frankly, one of the most important aspects of scientific work—and also one of the most neglected. Whether you’re a grad student wrangling code after midnight or a seasoned PI rediscovering a half-forgotten analysis, reproducibility ensures your brilliant findings don’t vanish into the digital abyss. Below, we’ll review why you should care, and how you can implement reproducibility so thoroughly that even a time-traveling version of yourself from five years ago could decipher your code."
  },
  {
    "objectID": "unit-0/lec-0-3.html#a.-why-reproducibility-matters",
    "href": "unit-0/lec-0-3.html#a.-why-reproducibility-matters",
    "title": "Foundations: Reproducible Research",
    "section": "A. Why Reproducibility Matters",
    "text": "A. Why Reproducibility Matters\n\nDefinition\n\nIn quantitative research, reproducibility means another researcher—or your future self with only half a memory of what you did—can take the same data and code, then arrive at the same results. No secret incantations, arcane folder structures, or chanting under the full moon required.\n\nMotivations\n\n\nTransparency: Ever heard the phrase “Trust me, I’m a scientist”? Yeah, that doesn’t fly so well if your code is locked away in a cryptically named folder like “analysis_final_FINAL_noReally.r”. By exposing how you clean, analyze, and report data, you bolster confidence in your results.\nCollaboration: If multiple people have to dig through your code to figure out why you did something, at least make it a pleasant excavation. A reproducible workflow means you can share your project with colleagues, and they’ll only have mild confusion instead of sheer panic.\nEfficiency: Imagine re-running your entire analysis with a single command. Now contrast that with rummaging through 57 scripts, each called “analysis2_v2_copy.R”. Reproducibility helps you avoid the second scenario—and possibly a meltdown.\nError Detection: Because let’s face it, even the best of us occasionally type mean when we mean median. With reproducible code, errors can be spotted quickly and corrected before your findings end up on the front page of the Journal of Irreproducible Results.\n\nReproducibility, in short, saves your sanity and safeguards your scientific honor."
  },
  {
    "objectID": "unit-0/lec-0-3.html#b.-core-tools-and-concepts",
    "href": "unit-0/lec-0-3.html#b.-core-tools-and-concepts",
    "title": "Foundations: Reproducible Research",
    "section": "B. Core Tools and Concepts",
    "text": "B. Core Tools and Concepts\nSo how do we make all this happen? We bring in the cavalry: version control, literate programming, environment management, a logical project structure, and tidy documentation. Let’s get to it.\n1. Version Control (Git + GitHub)\nVersion control is like a well-ordered diary for your project: it tracks every change—no matter how small—across time, preserving your code’s evolutionary history in tidy “commits.”\n\nTracks Changes: Every minor tweak, even the moment you changed a comma to a semicolon and saved your entire analysis from ruin.\nFacilitates Collaboration: Gone are the days of emailing zip files named “Project_Latest.zip” back and forth. Now, you can break things collaboratively on GitHub and blame it on the merge conflict.\nCommit History: “Who changed my code last Thursday?” Git knows. Git always knows.\n\nThink of GitHub as your code’s remote spa retreat—safe, relaxing, and occasionally throwing small hissy fits called “merge conflicts.”\n2. Literate Programming (Quarto, R Markdown, knitr)\nSome folks write code in one file, then paste results into a Word document, and so on. Then they wonder why their final paper includes the wrong p-values. Enter literate programming.\n\nIntegrated Code + Narrative: Quarto, R Markdown, knitr, and friends let you keep code, text, and figures in one place, so you never again get lost in the labyrinth of “Oh wait, which file made that plot?”\nMinimizes Copy-Paste Errors: If your results are automatically woven into your final report, you can’t “forget to update Table 3.” All you do is re-run the document, and—voilà!—your latest analysis magically appears.\nSeamless Feedback Loop: Make a change, watch the effect ripple through the entire document. It’s like having the world’s fastest (and least sarcastic) collaborator.\n3. Environment Management (renv, or other)\nPicture this: You wrote a brilliant script six months ago, but now it refuses to run because some obscure package updated. Environment management tools let you freeze your code in time.\n\nCaptures R Package Versions: Tools like renv let you record all the packages (and their versions) you used, so re-installing them later won’t feel like Jumanji in dependency land.\nConsistency Over Time: If you want your analyses to work in five years—or on your advisor’s laptop next Tuesday—document your environment so it can be replicated exactly.\nFewer “It Worked On My Machine” Excuses: Because telling your collaborators to “just figure it out” is not a good look.\n4. Project Structure\nIf your current method involves flinging scripts and data into a single folder named “Stuff,” we might have a gentle suggestion: adopt a standardized project structure.\n\nData Folder: A sanctuary for raw files, processed data, or your deep, dark secrets—just keep it consistent.\nR Scripts or Quarto Documents: Store code for data cleaning, analysis, and visualization in separate scripts or well-labeled notebooks. It’s okay to have multiple files—just label them in a way that future you can actually decipher.\nOutput Folder: All those shiny plots and tables? Keep them in one place. Resist the urge to edit them manually; they should be generated by your scripts, not conjured by hand.\nREADME: The wise old gatekeeper that explains your entire project to anyone who stops by (including you, after you forget everything).\n\nA tidy folder structure is a gift to yourself. And your future co-authors. And your future self’s sanity.\n5. Documentation\nDocumentation is the special sauce that holds everything together—or, in some cases, the glaring absence that leads to frantic Slack messages at 2 a.m.\n\nCommit Messages: If your commit message says “Update stuff,” your collaborators may weep softly. Be descriptive—like “Fix bug in logistic regression loop that caused meltdown.”\nCode Comments: Add them wherever something might confuse your labmates or your brain on a Monday morning. The more complicated the code, the more we need small notes to guide us.\nREADME or docs/ Folder: Provide an at-a-glance explanation of the project’s purpose, the dataset, how to replicate results, and any known issues. It’s your public service announcement to the world.\n\nIf you’re feeling particularly poetic, documentation is the short story you write about your data, so that others can appreciate its plot—without rummaging through all the raw scripts to figure out who the villain is (spoiler: it’s usually missing data).\nTakeaway\nReproducible research is about ensuring that data and code aren’t locked in the dusty attic of your personal computer—half-labeled and wholly misunderstood. By relying on tools like Git, Quarto, and renv, and by maintaining a disciplined project structure and documentation process, you guarantee that your results are both credible and easy to revisit."
  },
  {
    "objectID": "unit-0/lec-0-3.html#hands-on-pratice",
    "href": "unit-0/lec-0-3.html#hands-on-pratice",
    "title": "Foundations: Reproducible Research",
    "section": "Hands-on Pratice",
    "text": "Hands-on Pratice\nNow let’s get our hands dirty with some of the tools for reproducibility. We’ll be applying reproducible research practices to a simulated RCT dataset. You’ll learn to clone a GitHub repository, explore the project structure, run an analysis using Quarto, and commit changes back to GitHub."
  },
  {
    "objectID": "unit-0/lec-0-3.html#guided-computer-setup",
    "href": "unit-0/lec-0-3.html#guided-computer-setup",
    "title": "Foundations: Reproducible Research",
    "section": "0. Guided Computer Setup",
    "text": "0. Guided Computer Setup\n\nComputing Setup\nIntro to Github"
  },
  {
    "objectID": "unit-0/lec-0-3.html#github-template-repository",
    "href": "unit-0/lec-0-3.html#github-template-repository",
    "title": "Foundations: Reproducible Research",
    "section": "1. GitHub Template Repository",
    "text": "1. GitHub Template Repository\n\n\nClone the Repository\n\nGo to the provided URL or use git clone https://github.com/unc-hpm-quant/rct-analysis-template in the terminal.\nIn RStudio: “File” &gt; “New Project” &gt; “Version Control” &gt; “Git” and paste the repo URL.\n\n\nReview Project Structure\n\n   rct-analysis-template/\n   ├── README.md\n   ├── .gitignore\n   ├── renv.lock\n   ├── data/\n   │   └── rct_sim_data.csv\n   ├── analysis/\n   │   ├── analysis.qmd\n   │   └── helpers.R\n   └── output/\n\n\nInstall Dependencies\n\nIf renv is used, run renv::restore() in the project to sync package versions."
  },
  {
    "objectID": "unit-0/lec-0-3.html#analyzing-the-simulated-rct-data",
    "href": "unit-0/lec-0-3.html#analyzing-the-simulated-rct-data",
    "title": "Foundations: Reproducible Research",
    "section": "2. Analyzing the Simulated RCT Data",
    "text": "2. Analyzing the Simulated RCT Data\n\n\nData: rct_sim_data.csv includes columns\n\nsubject_id\ntreatment\noutcome\nage\ngender, etc.\n\n\n\nGoal: Estimate the average treatment effect, create summaray tables, and visualize distributions.\n\n2.1 Open analysis.qmd\n1.  Look at the top YAML and code chunks.\n2.  Notice how code and text are interwoven.\n2.2 Render the Document\n\nIn RStudio, click “Render” or run:\n\n\nquarto::quarto_render(\"analysis/analysis.qmd\")\n\nThe output (HTML, PDF, etc.) will appear in your output folder or in the same directory."
  },
  {
    "objectID": "unit-0/lec-0-3.html#modify-and-commit",
    "href": "unit-0/lec-0-3.html#modify-and-commit",
    "title": "Foundations: Reproducible Research",
    "section": "3. Modify and Commit",
    "text": "3. Modify and Commit\n\nMake a Small Change\n\nFor example, add a simple plot of outcome vs. treatment:\n\nggplot(rct_data, aes(x = factor(treatment), y = outcome)) +\n  geom_boxplot()\n\n\nRe-run the document to see the new figure in the rendered output.\nCommit and Push\n\n\nStage your changes:\n\n\ngit add analysis/analysis.qmd\ngit commit -m \"Added treatment-outcome boxplot\"\ngit push origin main\n\n\nView on GitHub: Confirm your commit is visible and see the updated code."
  },
  {
    "objectID": "unit-0/lec-0-3.html#branching-and-pull-requests-for-bigger-or-test-changes",
    "href": "unit-0/lec-0-3.html#branching-and-pull-requests-for-bigger-or-test-changes",
    "title": "Foundations: Reproducible Research",
    "section": "4. Branching and Pull Requests for bigger or test changes",
    "text": "4. Branching and Pull Requests for bigger or test changes\n\nCreate a new branch\n\n\ngit checkout -b new-plot\n\n\nAdd Covariates: Update the analysis to adjust for age and gender in a regression model."
  },
  {
    "objectID": "unit-0/lec-0-3.html#wrap-up",
    "href": "unit-0/lec-0-3.html#wrap-up",
    "title": "Foundations: Reproducible Research",
    "section": "5. Wrap-Up",
    "text": "5. Wrap-Up\nReflect: How did version control and Quarto documents streamline your workflow?\nNext Steps: If you want to dig further into reproducible data analysis practices, you can explore advanced topics like Docker, code review workflows, and CI/CD (continuous integration and deployment)."
  },
  {
    "objectID": "Resources/Pre-AnalysisPlanTemplate.html",
    "href": "Resources/Pre-AnalysisPlanTemplate.html",
    "title": "Pre-Analysis Plan: Title of the Study",
    "section": "",
    "text": "Note\n\n\n\nTemplate for a Pre-Analysis Plan (PAP) for a randomized experiment. This is modified from an original template created by Alejandro Ganimian, available here.\nOther Helpful Resources:\nFor guidance on pre-analysis plans, refer to\n\nthe World Bank’s DIME Wiki: Pre-Analysis Plan - DIME Wiki\nThe J-Pal Research Resources Website: J-Pal Research Resources\n\nFor examples of pre-analysis plans, explore the AEA’s RCT Registry: AEA RCT Registry. Here are some of mine:\n\nPay by Design Trial\nAnemia P4P Trial"
  },
  {
    "objectID": "Resources/Pre-AnalysisPlanTemplate.html#abstract",
    "href": "Resources/Pre-AnalysisPlanTemplate.html#abstract",
    "title": "Pre-Analysis Plan: Title of the Study",
    "section": "Abstract",
    "text": "Abstract\n\nIn 1-2 sentences, what does the study entail?\nIn 1-2 sentences, why is this study important/relevant?"
  },
  {
    "objectID": "Resources/Pre-AnalysisPlanTemplate.html#motivation",
    "href": "Resources/Pre-AnalysisPlanTemplate.html#motivation",
    "title": "Pre-Analysis Plan: Title of the Study",
    "section": "Motivation",
    "text": "Motivation\n\nWhat is the main problem/question motivating the study?\nHow has this problem/question been addressed thus far?\nHow is this study different from prior research on this problem/question?\nWhy is the context that you have chosen for this study appropriate?"
  },
  {
    "objectID": "Resources/Pre-AnalysisPlanTemplate.html#research-questions",
    "href": "Resources/Pre-AnalysisPlanTemplate.html#research-questions",
    "title": "Pre-Analysis Plan: Title of the Study",
    "section": "Research Questions",
    "text": "Research Questions\n\nWhat are the main research questions the study seeks to answer?"
  },
  {
    "objectID": "Resources/Pre-AnalysisPlanTemplate.html#sampling",
    "href": "Resources/Pre-AnalysisPlanTemplate.html#sampling",
    "title": "Pre-Analysis Plan: Title of the Study",
    "section": "Sampling",
    "text": "Sampling\n\nSampling Frame\n\nWhat is the eligible population for the study?\n\nWhat are the main characteristics of this population?\n\nWhat is the expected sample for the study?\n\nWhat is the expected sample size?\nHow does the expected sample differ from the population?\n\n\n\n\nStatistical Power\n\nWhat is the effect size you will be able to detect?\n\nWhat are your assumptions about your alpha-level?\nWhat are your assumptions about your statistical power?\nWhat are your assumptions about variability in your effect size?\nHow many sites will you have?\nHow many people will you have in each site?\nWhat share of the variance do you expect to predict with your covariates?\n\nHow sensitive is your effect size to changes in your parameters?\n\n\n\nAssignment to Treatment\n\nHow will individuals be assigned to treatment and control conditions?\nWhat is the source of exogenous variation in your study?\n\n\n\nAttrition from the Sample\n\nDo you anticipate any form of attrition from the sample?\n\nIf so, what share of the sample do you anticipate will attrit?\nOn what evidence are you basing your expectations about attrition?\nHow realistic are your expectations about attrition?\n\nWhat can you do to prevent/remedy sample attrition?\nHow does expected attrition change your power calculations?"
  },
  {
    "objectID": "Resources/Pre-AnalysisPlanTemplate.html#fieldwork",
    "href": "Resources/Pre-AnalysisPlanTemplate.html#fieldwork",
    "title": "Pre-Analysis Plan: Title of the Study",
    "section": "Fieldwork",
    "text": "Fieldwork\n\nInstruments\n\nWhat data collection instruments will you employ?\n\nWhat (groups of) indicators will each instrument cover?\nHow was each instrument developed?\nHave each instrument been used before?\nIf so, by whom? If not, are you piloting it?\nWhat are the main advantages/disadvantages of each instrument?\n\n\n\n\nData Collection\n\nHow long will the entire data collection process take from start to finish?\nWhat does the data collection entail?\nWhat steps will be taken to keep the data collected confidential at this stage?\n\n\n\nData Processing\n\nHow long will data processing take from start to finish?\nWhat does the data processing entail?\nWhat steps will be taken to keep the processed data confidential?\nWho has ownership over the processed data?\nHow will the data be used/stored after the study at this stage?"
  },
  {
    "objectID": "Resources/Pre-AnalysisPlanTemplate.html#variables",
    "href": "Resources/Pre-AnalysisPlanTemplate.html#variables",
    "title": "Pre-Analysis Plan: Title of the Study",
    "section": "Variables",
    "text": "Variables\n\nWhat are the main variables of interest in your study?\n\nHow is each of them defined in your dataset?"
  },
  {
    "objectID": "Resources/Pre-AnalysisPlanTemplate.html#balancing-checks",
    "href": "Resources/Pre-AnalysisPlanTemplate.html#balancing-checks",
    "title": "Pre-Analysis Plan: Title of the Study",
    "section": "Balancing Checks",
    "text": "Balancing Checks\n\nHow will you check balance between treatment and control groups?\n\nWhat is the specification that you will run?\nWhat variables will you include in these balancing checks?\n\nHow will you check balance between attritors and non-attritors?\n\nWhat is the specification that you will run?\nWhat variables will you include in these balancing checks?"
  },
  {
    "objectID": "Resources/Pre-AnalysisPlanTemplate.html#treatment-effects",
    "href": "Resources/Pre-AnalysisPlanTemplate.html#treatment-effects",
    "title": "Pre-Analysis Plan: Title of the Study",
    "section": "Treatment Effects",
    "text": "Treatment Effects\n\nIntent to Treat\n\nHow will you estimate the (causal) effect of the offer of the treatment?\n\nWhat is the specification that you will run?\nWhat controls will you include in your specification?\n\n\n\n\nTreatment on the Treated\n\nHow will you estimate the (causal) effect of the receipt of the treatment?\n\nWhat is the specification that you will run?\nWhat controls will you include in your specification?"
  },
  {
    "objectID": "Resources/Pre-AnalysisPlanTemplate.html#heterogeneous-effects",
    "href": "Resources/Pre-AnalysisPlanTemplate.html#heterogeneous-effects",
    "title": "Pre-Analysis Plan: Title of the Study",
    "section": "Heterogeneous Effects",
    "text": "Heterogeneous Effects\n\nWhich groups do you anticipate will display heterogeneous effects?\nWhat is the broad theory of action that leads you to anticipate these effects?\n\n\nIntent to Treat\n\nHow will you estimate the heterogeneous effects of the offer of the treatment?\n\nWhat are the specifications that you will run?\nWhat controls will you include in your specification?\n\n\n\n\nTreatment on the Treated\n\nHow will you estimate the heterogeneous effects of the receipt of the treatment?\n\nWhat are the specifications that you will run?\nWhat controls will you include in your specification?"
  },
  {
    "objectID": "Resources/Pre-AnalysisPlanTemplate.html#standard-error-adjustments",
    "href": "Resources/Pre-AnalysisPlanTemplate.html#standard-error-adjustments",
    "title": "Pre-Analysis Plan: Title of the Study",
    "section": "Standard Error Adjustments",
    "text": "Standard Error Adjustments\n\nHow will you account for clustering in your data?\nHow will you address false positives from multiple hypothesis testing?\n\nIf you plan to adjust your standard errors, what adjustment procedure will you use? (e.g., Family Wise Error Rate, False Discovery Rates, etc.)\nIf you plan to aggregate multiple variables into an index, which variables will you aggregate and how?\nHow will you deal with outcomes with limited variation?"
  },
  {
    "objectID": "instructors.html",
    "href": "instructors.html",
    "title": "Instructors",
    "section": "",
    "text": "Sean Sylvia is an Associate Professor in the Department of Health Policy and Management in the Gillings School of Global Public Health at UNC Chapel Hill.\nOffice Hours: Dr. Sylvia will hold office hours on Wednesdays from 1-2pm. Please book an appointment at least 24 hours ahead of time using this LINK.",
    "crumbs": [
      "Course Information",
      "Instructors"
    ]
  },
  {
    "objectID": "instructors.html#instructor-sean-sylvia-ph.d.",
    "href": "instructors.html#instructor-sean-sylvia-ph.d.",
    "title": "Instructors",
    "section": "",
    "text": "Sean Sylvia is an Associate Professor in the Department of Health Policy and Management in the Gillings School of Global Public Health at UNC Chapel Hill.\nOffice Hours: Dr. Sylvia will hold office hours on Wednesdays from 1-2pm. Please book an appointment at least 24 hours ahead of time using this LINK.",
    "crumbs": [
      "Course Information",
      "Instructors"
    ]
  },
  {
    "objectID": "instructors.html#teaching-assistant-yumeng-du",
    "href": "instructors.html#teaching-assistant-yumeng-du",
    "title": "Instructors",
    "section": "Teaching Assistant: Yumeng Du",
    "text": "Teaching Assistant: Yumeng Du\nYumeng Du is a Ph.D. student in the Department of Health Policy and Management in the Gillings School of Global Public Health at UNC Chapel Hill.\nOffice Hours: Yumeng will hold office hours on Mondays from 1-2pm. Please book an appointment at least 24 hours ahead of time using this LINK.",
    "crumbs": [
      "Course Information",
      "Instructors"
    ]
  },
  {
    "objectID": "labs/lab-1-InternalValidityPO.html",
    "href": "labs/lab-1-InternalValidityPO.html",
    "title": "Lab 1",
    "section": "",
    "text": "In this lab, we will explore internal validity and the potential outcomes framework using simulated health data from the endlessly eventful St. Null’s Memorial Hospital. Specifically, we will recreate a scenario where a new intervention (putting patients on ventilators) may or may not reduce patient mortality. As you’ll discover, chaos at the hospital has made it far from straightforward to identify causal effects.\nBy the end of this lab, you will be able to:\n\nUnderstand the concept of potential outcomes and causal effects.\nApply randomization inference to estimate treatment effects.\nIdentify threats to internal validity and explore possible solutions.\nImplement basic difference-in-means estimation in R.\nUse the WebR package to interactively run and modify code.\n\n\nOpen this .qmd file in RStudio or another Quarto-supported editor.\nFollow the guided coding prompts below, completing the missing code blocks.\nSubmit your completed lab on Gradescope by Feb 3 at 11:59pm.\n\n\nWelcome to St. Null’s Memorial Hospital—an institution where the only constant is confusion. The hospital board—led by the well-meaning but trend-obsessed CEO, Barnaby Beta—changes policies so often that nobody knows what’s going on.\nWorse yet is Dr. P-Hacker, a “data guru” who prefers p-values to patients. He mines the electronic health records (EHR) until something (anything!) is “significant.” Meanwhile, Nurse Random tries to keep everything on track, pointing out that good causal methods can be more important than good vibes. Lastly, Dr. Doub R. Obust lurks in the background, waiting for a chance to champion doubly robust methods that might someday save everyone’s sanity.\n\n\n\n\nYou and your team of budding methodologists are the new consultants hired to impose some order on this bedlam. In each module, you’ll tackle another fiasco at St. Null’s—from overfitted AI catastrophes to weird missing-data mishaps—and attempt to restore some semblance of methodological rigor. Good luck!\n\n\nA mysterious respiratory illness has swept through St. Null’s, leaving every ward scrambling. The question at hand is whether putting these patients on ventilators prolongs their lifespans. CEO Barnaby Beta wants quick answers (“If TikTok can do it, so can we!”). Dr. P-Hacker gleefully promises “instant significance,” claiming all he needs is the hospital’s EHR from the past week.\nBut Nurse Random, unimpressed, insists that the hospital’s chaotic, ad hoc ventilator assignments will cloud any conclusions. Dr. Doub R. Obust nods knowingly. They call in your team for an unbiased, data-driven approach.\nIn this lab, you’ll simulate a dataset of 100,000 patients that captures both “potential outcomes” (i.e., what would happen if a patient were ventilated vs. not ventilated). This magical glimpse at parallel universes is impossible in real-world data, but here it will let us see exactly how different analytic approaches fare in the face of selection bias.",
    "crumbs": [
      "Unit 1: Internal Validity",
      "Lab 1: The Hospital of Uncertain Outcomes"
    ]
  },
  {
    "objectID": "labs/lab-1-InternalValidityPO.html#overview-and-learning-objectives",
    "href": "labs/lab-1-InternalValidityPO.html#overview-and-learning-objectives",
    "title": "Lab 1",
    "section": "",
    "text": "In this lab, we will explore internal validity and the potential outcomes framework using simulated health data from the endlessly eventful St. Null’s Memorial Hospital. Specifically, we will recreate a scenario where a new intervention (putting patients on ventilators) may or may not reduce patient mortality. As you’ll discover, chaos at the hospital has made it far from straightforward to identify causal effects.\nBy the end of this lab, you will be able to:\n\nUnderstand the concept of potential outcomes and causal effects.\nApply randomization inference to estimate treatment effects.\nIdentify threats to internal validity and explore possible solutions.\nImplement basic difference-in-means estimation in R.\nUse the WebR package to interactively run and modify code.",
    "crumbs": [
      "Unit 1: Internal Validity",
      "Lab 1: The Hospital of Uncertain Outcomes"
    ]
  },
  {
    "objectID": "labs/lab-1-InternalValidityPO.html#instructions",
    "href": "labs/lab-1-InternalValidityPO.html#instructions",
    "title": "Lab 1",
    "section": "",
    "text": "Open this .qmd file in RStudio or another Quarto-supported editor.\nFollow the guided coding prompts below, completing the missing code blocks.\nSubmit your completed lab on Gradescope by Feb 3 at 11:59pm.",
    "crumbs": [
      "Unit 1: Internal Validity",
      "Lab 1: The Hospital of Uncertain Outcomes"
    ]
  },
  {
    "objectID": "labs/lab-1-InternalValidityPO.html#scenario",
    "href": "labs/lab-1-InternalValidityPO.html#scenario",
    "title": "Lab 1",
    "section": "",
    "text": "Welcome to St. Null’s Memorial Hospital—an institution where the only constant is confusion. The hospital board—led by the well-meaning but trend-obsessed CEO, Barnaby Beta—changes policies so often that nobody knows what’s going on.\nWorse yet is Dr. P-Hacker, a “data guru” who prefers p-values to patients. He mines the electronic health records (EHR) until something (anything!) is “significant.” Meanwhile, Nurse Random tries to keep everything on track, pointing out that good causal methods can be more important than good vibes. Lastly, Dr. Doub R. Obust lurks in the background, waiting for a chance to champion doubly robust methods that might someday save everyone’s sanity.\n\n\n\n\nYou and your team of budding methodologists are the new consultants hired to impose some order on this bedlam. In each module, you’ll tackle another fiasco at St. Null’s—from overfitted AI catastrophes to weird missing-data mishaps—and attempt to restore some semblance of methodological rigor. Good luck!",
    "crumbs": [
      "Unit 1: Internal Validity",
      "Lab 1: The Hospital of Uncertain Outcomes"
    ]
  },
  {
    "objectID": "labs/lab-1-InternalValidityPO.html#your-mission",
    "href": "labs/lab-1-InternalValidityPO.html#your-mission",
    "title": "Lab 1",
    "section": "",
    "text": "A mysterious respiratory illness has swept through St. Null’s, leaving every ward scrambling. The question at hand is whether putting these patients on ventilators prolongs their lifespans. CEO Barnaby Beta wants quick answers (“If TikTok can do it, so can we!”). Dr. P-Hacker gleefully promises “instant significance,” claiming all he needs is the hospital’s EHR from the past week.\nBut Nurse Random, unimpressed, insists that the hospital’s chaotic, ad hoc ventilator assignments will cloud any conclusions. Dr. Doub R. Obust nods knowingly. They call in your team for an unbiased, data-driven approach.\nIn this lab, you’ll simulate a dataset of 100,000 patients that captures both “potential outcomes” (i.e., what would happen if a patient were ventilated vs. not ventilated). This magical glimpse at parallel universes is impossible in real-world data, but here it will let us see exactly how different analytic approaches fare in the face of selection bias.",
    "crumbs": [
      "Unit 1: Internal Validity",
      "Lab 1: The Hospital of Uncertain Outcomes"
    ]
  },
  {
    "objectID": "labs/lab-1-InternalValidityPO.html#step-1-simulating-the-dataset",
    "href": "labs/lab-1-InternalValidityPO.html#step-1-simulating-the-dataset",
    "title": "Lab 1",
    "section": "Step 1: Simulating the Dataset",
    "text": "Step 1: Simulating the Dataset\nBecause the EHR system at St. Null’s is, in a word, “unreliable” (or, in two words, “utterly broken”), we’ll create our own dataset in R.\nRun the following code to generate 100,000 patient records along with potential outcomes (y0 if no ventilator, y1 if ventilated). Each outcome is the patient’s lifespan (in some made-up units). Note that lifespans below zero are set to zero—any negative numbers would just be an artifact of Dr. P-Hacker’s bizarre data extraction methods.\n\nlibrary(fixest)\nlibrary(data.table)\nset.seed(20200403)\n\n# 100,000 people with differing levels of covid symptoms\nN_people = 100000\ndf = data.table(person = 1:N_people)\n\n# Potential outcomes (Y0): life-span if no vent\ndf[, y0 := rnorm(N_people, 9.4, 4)]\ndf[y0 &lt; 0, y0 := 0]\n\n# Potential outcomes (Y1): life-span if assigned to vents\ndf[, y1 := rnorm(N_people, 10, 4)]\ndf[y1 &lt; 0, y1 := 0]\n\nWe also define the individual treatment effect for each patient. Dr. Doub R. Obust is thrilled, because for once, we have both y0 and y1 simultaneously—an impossible dream in real life!\n\n# Define individual treatment effect",
    "crumbs": [
      "Unit 1: Internal Validity",
      "Lab 1: The Hospital of Uncertain Outcomes"
    ]
  },
  {
    "objectID": "labs/lab-1-InternalValidityPO.html#step-2-the-two-doctors",
    "href": "labs/lab-1-InternalValidityPO.html#step-2-the-two-doctors",
    "title": "Lab 1",
    "section": "Step 2: The Two Doctors",
    "text": "Step 2: The Two Doctors\nSt. Null’s has two very different doctors assigning ventilators:\n\n\nDr. Perfect: A mystical being who can see into each patient’s future and only gives ventilators to those who would benefit from them.\n\nDr. Bad: The name says it all. This doctor assigns ventilators randomly—could be beneficial, could not be. Who knows?\n\nStep 2a: Assigning Doctors\nFirst, we randomly assign each patient to one of these two doctors. (No wonder this hospital is chaotic…)\n\n# Assign doctors randomly\n\nStep 2b: Assigning Ventilators\nNext, each doctor does what they do best. Dr. Perfect uses clairvoyance to treat only those who stand to gain (delta &gt; 0). Dr. Bad flips a metaphorical coin:\n\n# Perfect doctor assigns vents only to those who benefit\n\n\n# Random doctor assigns vents randomly\n\nIt’s not exactly a model of ethical clarity, but it certainly demonstrates the complications of “treatment assignment” in the real world (or the real St. Null’s world).",
    "crumbs": [
      "Unit 1: Internal Validity",
      "Lab 1: The Hospital of Uncertain Outcomes"
    ]
  },
  {
    "objectID": "labs/lab-1-InternalValidityPO.html#step-3-computing-causal-parameters",
    "href": "labs/lab-1-InternalValidityPO.html#step-3-computing-causal-parameters",
    "title": "Lab 1",
    "section": "Step 3: Computing Causal Parameters",
    "text": "Step 3: Computing Causal Parameters\nNow, let’s calculate the key causal parameters:\n\n\nAverage Treatment Effect (ATE): The overall difference in outcomes if everyone were ventilated vs. if no one were ventilated.\n\nAverage Treatment Effect on the Treated (ATT): The effect of ventilation on those who actually received ventilation.\n\nAverage Treatment Effect on the Untreated (ATU): The effect of ventilation on those who were not ventilated.\n\n\n# Calculate all aggregate Causal Parameters (ATE, ATT, ATU)\n\nDr. P-Hacker would stop right here and rejoice: “We have all the significance we need!” But hold your celebratory balloon drop—there’s more to do.",
    "crumbs": [
      "Unit 1: Internal Validity",
      "Lab 1: The Hospital of Uncertain Outcomes"
    ]
  },
  {
    "objectID": "labs/lab-1-InternalValidityPO.html#step-4-selection-bias-and-realized-outcomes",
    "href": "labs/lab-1-InternalValidityPO.html#step-4-selection-bias-and-realized-outcomes",
    "title": "Lab 1",
    "section": "Step 4: Selection Bias and Realized Outcomes",
    "text": "Step 4: Selection Bias and Realized Outcomes\nAlthough the dataset has both y0 and y1, in the real world, a patient’s outcome is observed under only one condition (treated or untreated). Let’s capture which outcome we’d actually see based on the ventilator assignment:\n\n# Use the switching equation to select realized outcomes from potential outcomes based on treatment assignment\n\nSelection Bias Calculation\nWe’ll see if there is selection bias by comparing the expected lifespan of ventilated patients had they not been ventilated to the expected lifespan of non-ventilated patients.\n\n# Calculate EY0 for vent group and no vent group\n\nIf Dr. Perfect is involved, we’d expect some big differences here. Dr. P-Hacker would probably ignore that and claim victory anyway. (He likes ignoring inconvenient truths.)",
    "crumbs": [
      "Unit 1: Internal Validity",
      "Lab 1: The Hospital of Uncertain Outcomes"
    ]
  },
  {
    "objectID": "labs/lab-1-InternalValidityPO.html#step-5-comparing-outcomes-between-groups",
    "href": "labs/lab-1-InternalValidityPO.html#step-5-comparing-outcomes-between-groups",
    "title": "Lab 1",
    "section": "Step 5: Comparing Outcomes Between Groups",
    "text": "Step 5: Comparing Outcomes Between Groups\nOne of the simplest ways to estimate the treatment effect is to look at the Simple Difference in Outcomes (SDO)—the difference in the observed mean outcome between those who got the treatment (ventilators) and those who did not.\n\n# Calculate the share of units treated with vents (pi)\n\n\n# Manually calculate the simple difference in mean health outcomes\n\nDr. P-Hacker would run around shouting: “Aha! This difference proves the intervention works!” or “Aha! It’s not significant!” depending on the p-value. Let’s see if we can do better.",
    "crumbs": [
      "Unit 1: Internal Validity",
      "Lab 1: The Hospital of Uncertain Outcomes"
    ]
  },
  {
    "objectID": "labs/lab-1-InternalValidityPO.html#step-6-estimating-the-effect-with-regression",
    "href": "labs/lab-1-InternalValidityPO.html#step-6-estimating-the-effect-with-regression",
    "title": "Lab 1",
    "section": "Step 6: Estimating the Effect with Regression",
    "text": "Step 6: Estimating the Effect with Regression\nTo confirm our findings, let’s do an Ordinary Least Squares (OLS) regression of the realized outcome on the ventilator indicator:\n\n# Estimate the treatment effect using OLS\n\nThis approach, while straightforward, is still subject to any biases from non-random assignment—like, say, Dr. Perfect or Dr. Bad being in charge.",
    "crumbs": [
      "Unit 1: Internal Validity",
      "Lab 1: The Hospital of Uncertain Outcomes"
    ]
  },
  {
    "objectID": "labs/lab-1-InternalValidityPO.html#step-7-validating-the-decomposition",
    "href": "labs/lab-1-InternalValidityPO.html#step-7-validating-the-decomposition",
    "title": "Lab 1",
    "section": "Step 7: Validating the Decomposition",
    "text": "Step 7: Validating the Decomposition\nAt this point, we’d like to see if the Simple Difference in Outcomes (SDO) can be broken down into our measured parameters. You’ll fill in a table comparing Dr. Perfect to Dr. Bad, computing the ATE, ATT, ATU, selection bias, SDO, and so on.\nBelow is a quick consistency check to see if the SDO lines up with our theoretical decomposition:\n\n# Decomposition check\n\nWhen you fill out the table, you should include:\n\n\nPerfect Doctor\nBad Doctor\nCausal Parameter\n\n\n\nATE\n\n\n\n\nATT\n\n\n\n\nATU\n\n\n\n\nSelection bias terms\n\n\n\n\nE[Y0 | D=1]\n\n\n\n\nE[Y0 | D=0]\n\n\n\n\nSelection bias\n\n\n\n\nCalculations\n\n\n\n\nPi (share on vents)\n\n\n\n\nSDO manual\n\n\n\n\nSDO OLS\n\n\n\n\nSDO Decomposition\n\n\n\n\nObs\n\n\n\n\n\nReflection\nFinally, reflect on these questions:\n\nIs the Simple Difference in Outcomes (SDO) enough to identify the ATE, ATT, or ATU?\nHow does selection bias play into interpreting the SDO?\nWhat lessons would Nurse Random want Dr. P-Hacker to learn about data and design?\n\n(Extra credit if you can imagine the dramatic showdown when Dr. Doub R. Obust finally unveils a doubly robust method that saves the day—but that’s a story for a future lab!)\nThat’s it! You’ve run the gauntlet of the Hospital of Uncertain Outcomes and lived to tell the tale. Now submit your work, and good luck diagnosing more of St. Null’s methodological maladies!",
    "crumbs": [
      "Unit 1: Internal Validity",
      "Lab 1: The Hospital of Uncertain Outcomes"
    ]
  },
  {
    "objectID": "labs/lab-2-Power-sols.html",
    "href": "labs/lab-2-Power-sols.html",
    "title": "Lab 2: Power by Simulation",
    "section": "",
    "text": "In this lab, you will explore the concept of statistical conclusion validity by conducting power calculations via simulation. Specifically, you will:\n\nConduct a power simulation for a simple randomized experiment without clustering (i.e., ignoring the fact that participants may be grouped within clinics).\nExtend that simulation to account for clustering (i.e., clinics as the unit of randomization).\nExamine the impact of sample size, effect size, and clustering on statistical power, and visualize how minimum detectable effect sizes (MDE) change with sample size.\n\nBy the end of this lab, you will have a deeper understanding of:\n\nHow sources of uncertainty (sampling variation, variance in potential outcomes across participants, and measurement error) affect your study’s outcomes.\nWhy we must beware of Type I (false positive) and Type II (false negative) errors—especially if Dr. P-Hacker is anywhere near our data.\nHow p-values should (and should not!) be interpreted.\nHow to simulate data that include cluster-level effects and adjust for these effects in your analysis."
  },
  {
    "objectID": "labs/lab-2-Power-sols.html#overview-and-learning-objectives",
    "href": "labs/lab-2-Power-sols.html#overview-and-learning-objectives",
    "title": "Lab 2: Power by Simulation",
    "section": "",
    "text": "In this lab, you will explore the concept of statistical conclusion validity by conducting power calculations via simulation. Specifically, you will:\n\nConduct a power simulation for a simple randomized experiment without clustering (i.e., ignoring the fact that participants may be grouped within clinics).\nExtend that simulation to account for clustering (i.e., clinics as the unit of randomization).\nExamine the impact of sample size, effect size, and clustering on statistical power, and visualize how minimum detectable effect sizes (MDE) change with sample size.\n\nBy the end of this lab, you will have a deeper understanding of:\n\nHow sources of uncertainty (sampling variation, variance in potential outcomes across participants, and measurement error) affect your study’s outcomes.\nWhy we must beware of Type I (false positive) and Type II (false negative) errors—especially if Dr. P-Hacker is anywhere near our data.\nHow p-values should (and should not!) be interpreted.\nHow to simulate data that include cluster-level effects and adjust for these effects in your analysis."
  },
  {
    "objectID": "labs/lab-2-Power-sols.html#the-hospital-drama-begins",
    "href": "labs/lab-2-Power-sols.html#the-hospital-drama-begins",
    "title": "Lab 2: Power by Simulation",
    "section": "The Hospital Drama Begins",
    "text": "The Hospital Drama Begins\nWelcome to St. Null’s Memorial Hospital, where a brand-new quality improvement intervention is about to be tested. The hospital’s network of clinics, collectively called The Null Distribution, is famous for its dedication to meticulous data collection—and also for some questionable statistical practices performed by a rather infamous staff member.\n\n\nCEO Barnaby Beta has championed a new, cost-intensive intervention aimed at improving patient satisfaction.\n\nNurse Random insists on conducting a proper randomized control trial (RCT), but quickly realizes Dr. P-Hacker might have already meddled with the initial power calculations.\n\nDr. P-Hacker is known to declare victory (“Significant at the 5% level!”) before even cleaning the data, and is rumored to own a golden “p &lt; 0.05” sign that he waves in staff meetings.\n\nDr. Doub R. Obust is the voice of reason, reminding everyone of fundamental statistical principles.\n\nIn this lab, you (the consultants) have been summoned to salvage the situation. Let’s see how this plays out."
  },
  {
    "objectID": "labs/lab-2-Power-sols.html#the-plot-thickens-power-calculations-without-clustering",
    "href": "labs/lab-2-Power-sols.html#the-plot-thickens-power-calculations-without-clustering",
    "title": "Lab 2: Power by Simulation",
    "section": "The Plot Thickens: Power Calculations Without Clustering",
    "text": "The Plot Thickens: Power Calculations Without Clustering\nScene 1: The Mysterious Spreadsheet\nNurse Random bursts into the conference room, clutching a color-coded spreadsheet.\n\nNurse Random: “Dr. P-Hacker, your power calculations look suspiciously high. Did you account for the fact that we’re randomizing by clinic, not by individual patient?”\n\n\nDr. P-Hacker: “Of course not, Nurse Random. Look, a random patient is a random patient—no matter which clinic they come from! Besides, I love high power. It makes me feel like my study can conquer the world!”\n\n\nDr. Doub R. Obust (rolling eyes): “We’re going to need to run a simulation that properly reflects how the intervention is assigned at the clinic level, not just among individual patients. Otherwise, your calculations will be as overconfident as your new P-value neon sign.”\n\nNonetheless, Dr. P-Hacker shares his “method” with you first. Let’s start by replicating his approach (ignoring clustering). This will be our baseline—what not to do if clinics are truly the unit of randomization.\nStep 1: Flawed Power Simulation (Ignoring Clustering)\nWe’ll simulate a dataset as if individual patients are randomly assigned to treatment or control. We’ll compute the statistical power for detecting a true treatment effect of a specified size, ignoring any clinic-level differences.\n\n\n\n\n\n\nTask 1: Flawed Simulation Setup\n\n\n\nFill in the code below to acheive a power of around 0.8 (may not be exact, but get as close as you can):\n\nSet a sample size for the total number of patients.\n\nSpecify a treatment effect (effect).\n\nDecide on the proportion of participants to receive treatment (prop).\n\nSelect a significance level (t_alpha).\n\nRun multiple simulations (sim.size).\n\nThen, run a linear regression on each simulated dataset, gather the p-values, and compute how often the null hypothesis is rejected (i.e., estimate power).\nNote: The code below is not executable. You need to change the eval: false to eval: true to make it work and fill in the blanks.\n\n\n\nlibrary(data.table)\nset.seed(123456)\n\n# Define parameters\nsample_size &lt;- 790     # e.g. 500\neffect &lt;- 0.2          # e.g. 0.2\nprop &lt;- 0.5            # e.g. 0.5\nt_alpha &lt;- 0.05        # e.g. 0.05\nsim.size &lt;- 2000       # e.g. 2000\n\n# Initialize storage for results\nreject_t &lt;- numeric(sim.size)\n\n# Simulation loop\nfor (i in 1:sim.size) {\n  dt &lt;- data.table(id = 1:sample_size)\n  \n  # Assign treatment individually (incorrect for cluster randomization!)\n  dt[, treatment := rbinom(.N, 1, prop)]\n  \n  # Simulate outcome (10 is baseline, 'effect' is added if treatment=1)\n  dt[, outcome := 10 + effect * treatment + rnorm(.N, mean = 0, sd = 1)]\n  \n  # Estimate the effect\n  fit &lt;- lm(outcome ~ treatment, data = dt)\n  p_value &lt;- summary(fit)$coefficients[2,4]\n  \n  # Check if we reject H0: (p-value &lt; alpha)\n  reject_t[i] &lt;- ifelse(p_value &lt; t_alpha, 1, 0)\n}\n\n# Compute estimated power\npower_flawed &lt;- mean(reject_t)\ncat(\"Estimated Power (Ignoring Clustering):\", power_flawed, \"\\n\")\n\nEstimated Power (Ignoring Clustering): 0.8035 \n\n\nDiscussion of Step 1\n\n\nNurse Random sighs: “We got an estimated power of 0.8 (yours might be slightly different). But do we trust this number?”\n\nDr. Doub R. Obust chimes in: “Nope. We’re ignoring that patients within the same clinic might be more similar to each other than to patients in other clinics. Our standard errors are artificially small.”\n\nAt this point, CEO Barnaby Beta perks up: “Artificially small standard errors? That means we’re basically claiming more precision than we actually have, right?”\nYes, indeed. If we disregard the clustering of patients, we risk making a Type I error more often than we realize. Dr. P-Hacker, in typical fashion, responds:\n\nDr. P-Hacker: “Type I error? Isn’t that just when we see something interesting that’s obviously there?!”Dr. Doub R. Obust (groaning): “No. A Type I error is a false positive—we conclude there is an effect even though, in truth, there isn’t. Like thinking you’ve discovered a rare golden banana flavor at the cafeteria soda machine when really it’s just seltzer water with a weird label!”"
  },
  {
    "objectID": "labs/lab-2-Power-sols.html#understanding-the-sources-of-uncertainty",
    "href": "labs/lab-2-Power-sols.html#understanding-the-sources-of-uncertainty",
    "title": "Lab 2: Power by Simulation",
    "section": "Understanding the Sources of Uncertainty",
    "text": "Understanding the Sources of Uncertainty\nBefore we fix our simulation, Dr. Doub R. Obust insists that you reflect on why ignoring clinic-level clustering is a problem. He ticks off sources of variability that a real experiment faces:\n\n\nSampling Variation: Even if you have a large population of patients, the sample you select is just one draw from a bigger population. Different samples might give different estimates.\n\n\nMeasurement Error: If your measurement tool for patient well-being is noisy (e.g., patients often mis-report how they feel, or staff record data incorrectly), it introduces extra variability that can muddy your effect estimates.\n\nVariance in Potential Outcomes: Not all patients respond to treatments in the same way. Some might be strongly affected, some not at all—leading to variation in the outcomes. This includes clustering: Patients in the same clinic share certain characteristics, environmental factors, or staff practices that can affect their potential outcomes. This correlation must be accounted for in the design and analysis.\n\n\nNurse Random: “So if we ignore that last point—clustering—our estimate of the variability (and thus the standard error) is off, and we might incorrectly claim significance. That’s basically p-hacking 101!”"
  },
  {
    "objectID": "labs/lab-2-Power-sols.html#power-calculations-with-clustering",
    "href": "labs/lab-2-Power-sols.html#power-calculations-with-clustering",
    "title": "Lab 2: Power by Simulation",
    "section": "Power Calculations With Clustering",
    "text": "Power Calculations With Clustering\nScene 2: A (Cluster-)Randomized Trial\nNow we come to the correct approach: our unit of randomization is the clinic, not the individual. That means each clinic either receives the intervention or does not, and all patients in a clinic share the same treatment assignment.\nStep 2: Incorporating Clustering\nWe’ll model:\n\n\nnum_clusters = number of clinics.\n\n\ncluster_size = number of patients per clinic.\n\n\nicc (Intraclass Correlation Coefficient): A measure of how strongly patients in the same clinic resemble each other. High ICC means patients within a clinic are more correlated.\n\nWe’ll generate clinic-level “random effects” and individual-level error terms to reflect both measurement error and the variability in potential outcomes across individuals.\n\n\n\n\n\n\n\nTask 2: Correcting the Simulation for Clustering\n\n\n\nFill in the code below to acheive a power of 0.8:\n\nDefine the number of clinics and the number of patients per clinic.\n\nAssign each entire clinic to treatment or control.\n\nIncorporate cluster-level random effects.\n\nFit the model but adjust standard errors for clustering.\n\n\n\n\n\nlibrary(multiwayvcov)  # for cluster-robust standard errors\nlibrary(lmtest)        # for coeftest\n\nLoading required package: zoo\n\n\n\nAttaching package: 'zoo'\n\n\nThe following objects are masked from 'package:data.table':\n\n    yearmon, yearqtr\n\n\nThe following objects are masked from 'package:base':\n\n    as.Date, as.Date.numeric\n\n# Define cluster parameters\nnum_clusters &lt;- 241    # e.g. 50\ncluster_size &lt;- 20     # e.g. 10\ntotal_sample &lt;- num_clusters * cluster_size\nicc &lt;- 0.2             # e.g. 0.4\n\n# Variances\nind_err_var &lt;- 1\n# cluster_err_var is derived from the intraclass correlation coefficient\ncluster_err_var &lt;- (icc * ind_err_var) / (1 - icc)\n\nsim.size &lt;- 2000        # e.g. 2000\neffect &lt;- 0.2           # e.g. 0.2\nprop &lt;- 0.5             # e.g. 0.5\nt_alpha &lt;- 0.05         # e.g. 0.05\n\nreject_t &lt;- numeric(sim.size)\n\nset.seed(654321)\nfor (i in 1:sim.size) {\n  \n  # Create a data table with one row per cluster, repeated for each patient\n  clusters &lt;- data.table(\n    cluster = rep(1:num_clusters, each = cluster_size)\n  )\n  \n  # Generate a cluster-level random effect\n  clusters[, cluster_error := rep(\n    rnorm(num_clusters, mean = 0, sd = sqrt(cluster_err_var)),\n    each = cluster_size\n  )]\n  \n  # Randomize at the clinic level\n  clusters[, treatment := rep(\n    rbinom(num_clusters, 1, prop),\n    each = cluster_size\n  )]\n  \n  # Add an individual-level error\n  clusters[, individual_error := rnorm(.N, mean = 0, sd = sqrt(ind_err_var))]\n  \n  # Final outcome for each individual\n  clusters[, outcome := 10 + effect * treatment + cluster_error + individual_error]\n  \n  # Fit a naive linear model (ignoring clustering in standard errors)\n  fit &lt;- lm(outcome ~ treatment, data = clusters)\n  \n  # Adjust standard errors for clustering\n  robust_SE &lt;- cluster.vcov(fit, clusters$cluster, df_correction = TRUE)\n  robust_coef &lt;- coeftest(fit, robust_SE)\n  \n  # Get the p-value for the treatment coefficient\n  p_value &lt;- robust_coef[2,4]\n  reject_t[i] &lt;- ifelse(p_value &lt; t_alpha, 1, 0)\n}\n\n# Compute estimated power with clustering\npower_clustered &lt;- mean(reject_t)\ncat(\"Estimated Power (Clustered):\", power_clustered, \"\\n\")\n\nEstimated Power (Clustered): 0.82 \n\n\nDiscussion of Step 2\nDr. Doub R. Obust looks at the new power estimate and remarks:\n\n“As you can see, once we account for clustering, the power is (usually) lower than in the flawed approach. That’s because those cluster-level similarities effectively reduce the amount of independent information we have. Our standard errors are bigger, so it’s harder to find significance unless the effect size or sample size is larger.”\n\nCEO Barnaby Beta shakes his head: “But that means our study might be underpowered if we stick to our current budget!”\nNurse Random replies with a grin: “Don’t worry, we can plan more carefully. Otherwise, if we run a smaller study, we risk a Type II error—failing to reject the null hypothesis when there really is an effect. You know, like leaving the gold standard intervention on the shelf because we didn’t gather enough data to prove it works.”\nDr. P-Hacker interjects:\n\n“And there’s always p &lt; 0.10, right? We can move our threshold for significance to get more ‘positive’ results!”Nurse Random (scolding): “That’s literally p-hacking. Please step away from the analysis, Doctor.”"
  },
  {
    "objectID": "labs/lab-2-Power-sols.html#the-truth-about-p-values",
    "href": "labs/lab-2-Power-sols.html#the-truth-about-p-values",
    "title": "Lab 2: Power by Simulation",
    "section": "The Truth About p-Values",
    "text": "The Truth About p-Values\nA quick comedic break for a lesson on p-values:\n\n\nDr. P-Hacker keeps shouting that p &lt; 0.05 means there’s a 5% chance the null hypothesis is true.\n\n\nNurse Random corrects him: “No, no, no. A p-value is the probability of observing a result at least as extreme as ours if the null is true. It’s NOT the probability the null is true. You can’t interpret it that way.”\n\n\nDr. P-Hacker: “I was so sure it was the probability that I was right.”Dr. Doub R. Obust: “We live and learn, Doctor.”"
  },
  {
    "objectID": "labs/lab-2-Power-sols.html#plotting-the-minimum-detectable-effect-mde",
    "href": "labs/lab-2-Power-sols.html#plotting-the-minimum-detectable-effect-mde",
    "title": "Lab 2: Power by Simulation",
    "section": "Plotting the Minimum Detectable Effect (MDE)",
    "text": "Plotting the Minimum Detectable Effect (MDE)\nBecause CEO Barnaby Beta wants to know how large an effect must be to have a reasonable chance of detection, you might want to simulate across a range of effect sizes and/or sample sizes. You can then plot the resulting power curves to see what the Minimum Detectable Effect (MDE) is for a given power requirement.\n\n\n\n\n\n\nTask 3: Create an MDE Plot\n\n\n\n\nLoop over a grid of effect sizes (or sample sizes).\n\nFor each value, compute power using the cluster-based simulation approach.\n\nPlot the effect size on the x-axis and the corresponding power on the y-axis.\n\n\n\n\nlibrary(ggplot2)\n\n# Simulation parameters for clustering\nnum_clusters &lt;- 241    # Number of clinics\ncluster_size &lt;- 20     # Patients per clinic\ntotal_sample &lt;- num_clusters * cluster_size\nicc &lt;- 0.2             # Intraclass correlation coefficient\n\n# Variance parameters\nind_err_var &lt;- 1\ncluster_err_var &lt;- (icc * ind_err_var) / (1 - icc)\n\nsim.size &lt;- 2000       # Number of simulation iterations per effect size\nprop &lt;- 0.5            # Proportion of clinics assigned to treatment\nt_alpha &lt;- 0.05        # Significance level\n\n# Define a grid of effect sizes\neffect_sizes &lt;- seq(0, 0.5, by = 0.05) \nresults &lt;- data.table(effect = effect_sizes, power = NA_real_)\n\nset.seed(072111)  # For reproducibility\n\n# Loop over each effect size\nfor (e in seq_along(effect_sizes)) {\n  current_effect &lt;- effect_sizes[e]\n  reject_t &lt;- numeric(sim.size)\n  \n  for (i in 1:sim.size) {\n    # Create a data table with one row per patient, with clinic identifier\n    clusters &lt;- data.table(cluster = rep(1:num_clusters, each = cluster_size))\n    \n    # Generate cluster-level random effects\n    clusters[, cluster_error := rep(rnorm(num_clusters, mean = 0, \n                                            sd = sqrt(cluster_err_var)), \n                                     each = cluster_size)]\n    \n    # Randomize treatment at the clinic level (all patients in a clinic get the same assignment)\n    clusters[, treatment := rep(rbinom(num_clusters, 1, prop), each = cluster_size)]\n    \n    # Add individual-level random error\n    clusters[, individual_error := rnorm(.N, mean = 0, sd = sqrt(ind_err_var))]\n    \n    # Generate the outcome variable: baseline of 10 plus treatment effect and both errors\n    clusters[, outcome := 10 + current_effect * treatment + cluster_error + individual_error]\n    \n    # Fit the linear model (naively, without clustering adjustments)\n    fit &lt;- lm(outcome ~ treatment, data = clusters)\n    \n    # Adjust standard errors for clustering\n    robust_SE &lt;- cluster.vcov(fit, clusters$cluster, df_correction = TRUE)\n    robust_coef &lt;- coeftest(fit, robust_SE)\n    \n    # Check if the p-value for the treatment coefficient is below the threshold\n    p_value &lt;- robust_coef[2, 4]\n    reject_t[i] &lt;- ifelse(p_value &lt; t_alpha, 1, 0)\n  }\n  \n  # Compute the estimated power (proportion of rejections) for this effect size\n  results$power[e] &lt;- mean(reject_t)\n}\n\n# Create the plot: Power vs. Effect Size\nggplot(results, aes(x = effect, y = power)) +\n  geom_line() +\n  geom_point() +\n  labs(\n    title = \"Power vs. Effect Size\",\n    x = \"Effect Size\",\n    y = \"Power\"\n  ) +\n  theme_minimal()"
  },
  {
    "objectID": "labs/lab-2-Power-sols.html#final-words-from-the-hospital-staff",
    "href": "labs/lab-2-Power-sols.html#final-words-from-the-hospital-staff",
    "title": "Lab 2: Power by Simulation",
    "section": "Final Words from the Hospital Staff",
    "text": "Final Words from the Hospital Staff\nCEO Barnaby Beta: “Alright, so if we need to ensure we have enough clinics and participants to achieve our desired power, we may need a bigger budget than expected. Let’s not forget that ignoring clustering would have given us a false sense of security in our power. Now we know better.”\nNurse Random: “And no more Type I or Type II error confusion, Dr. P-Hacker. We must keep our definitions straight if we’re to have any credibility around here!”\nDr. P-Hacker (sighing): “Fine, fine. Guess I’ll tone down the p-value hype. But I’m keeping my neon sign.”\nDr. Doub R. Obust (with a grin): “We’ll allow the neon sign, as long as you promise to interpret it correctly.”"
  },
  {
    "objectID": "labs/lab-2-Power-sols.html#submission-instructions",
    "href": "labs/lab-2-Power-sols.html#submission-instructions",
    "title": "Lab 2: Power by Simulation",
    "section": "Submission Instructions",
    "text": "Submission Instructions\n\nMake sure your .qmd file knits successfully (to .pdf or .html).\n\nUpload your compiled document to Gradescope.\n\n\nInclude your discussion of the results, your code, and your responses to the callout sections.\n\nRemember, the moral of the story: Always check who (or what) is being randomized, and account for clustering when necessary! Otherwise, your study conclusions might be as random as Dr. P-Hacker’s next dinner choice."
  },
  {
    "objectID": "labs/lab-1-InternalValidityPO_sols.html",
    "href": "labs/lab-1-InternalValidityPO_sols.html",
    "title": "Lab 1 Solutions",
    "section": "",
    "text": "Results may differ\n\n\n\nThis is only one of many possible ways to complete this lab. Your final code may look different, which is fine! In fact, it is good practice to experiment with different approaches."
  },
  {
    "objectID": "labs/lab-1-InternalValidityPO_sols.html#overview-and-learning-objectives",
    "href": "labs/lab-1-InternalValidityPO_sols.html#overview-and-learning-objectives",
    "title": "Lab 1 Solutions",
    "section": "Overview and Learning Objectives",
    "text": "Overview and Learning Objectives\nIn this lab, we will explore internal validity and the potential outcomes framework using simulated health data from the endlessly eventful St. Null’s Memorial Hospital. Specifically, we will recreate a scenario where a new intervention (putting patients on ventilators) may or may not reduce patient mortality. As you’ll discover, chaos at the hospital has made it far from straightforward to identify causal effects.\nBy the end of this lab, you will be able to:\n\nUnderstand the concept of potential outcomes and causal effects.\nApply randomization inference to estimate treatment effects.\nIdentify threats to internal validity and explore possible solutions.\nImplement basic difference-in-means estimation in R.\nUse the WebR package to interactively run and modify code."
  },
  {
    "objectID": "labs/lab-1-InternalValidityPO_sols.html#instructions",
    "href": "labs/lab-1-InternalValidityPO_sols.html#instructions",
    "title": "Lab 1 Solutions",
    "section": "Instructions",
    "text": "Instructions\n\nOpen this .qmd file in RStudio or another Quarto-supported editor.\nFollow the guided coding prompts below, completing the missing code blocks.\nSubmit your completed lab on Gradescope [Insert Link Here] by [Insert Deadline Here]."
  },
  {
    "objectID": "labs/lab-1-InternalValidityPO_sols.html#scenario",
    "href": "labs/lab-1-InternalValidityPO_sols.html#scenario",
    "title": "Lab 1 Solutions",
    "section": "Scenario",
    "text": "Scenario\nThe Hospital of Uncertain Outcomes\nWelcome to St. Null’s Memorial Hospital—an institution where the only constant is confusion. The hospital board—led by the well-meaning but trend-obsessed CEO, Barnaby Beta—changes policies so often that nobody knows what’s going on.\nWorse yet is Dr. P-Hacker, a “data guru” who prefers p-values to patients. He mines the electronic health records (EHR) until something (anything!) is “significant.” Meanwhile, Nurse Random tries to keep everything on track, pointing out that good causal methods can be more important than good vibes. Lastly, Dr. Doub R. Obust lurks in the background, waiting for a chance to champion doubly robust methods that might someday save everyone’s sanity.\n\n\n\n\nYou and your team of budding methodologists are the new consultants hired to impose some order on this bedlam. In each module, you’ll tackle another fiasco at St. Null’s—from overfitted AI catastrophes to weird missing-data mishaps—and attempt to restore some semblance of methodological rigor. Good luck!"
  },
  {
    "objectID": "labs/lab-1-InternalValidityPO_sols.html#your-mission",
    "href": "labs/lab-1-InternalValidityPO_sols.html#your-mission",
    "title": "Lab 1 Solutions",
    "section": "Your Mission",
    "text": "Your Mission\nThe Pandemic Mystery at St. Null’s\nA mysterious respiratory illness has swept through St. Null’s, leaving every ward scrambling. The question at hand is whether putting these patients on ventilators prolongs their lifespans. CEO Barnaby Beta wants quick answers (“If TikTok can do it, so can we!”). Dr. P-Hacker gleefully promises “instant significance,” claiming all he needs is the hospital’s EHR from the past week.\nBut Nurse Random, unimpressed, insists that the hospital’s chaotic, ad hoc ventilator assignments will cloud any conclusions. Dr. Doub R. Obust nods knowingly. They call in your team for an unbiased, data-driven approach.\nIn this lab, you’ll simulate a dataset of 100,000 patients that captures both “potential outcomes” (i.e., what would happen if a patient were ventilated vs. not ventilated). This magical glimpse at parallel universes is impossible in real-world data, but here it will let us see exactly how different analytic approaches fare in the face of selection bias."
  },
  {
    "objectID": "labs/lab-1-InternalValidityPO_sols.html#step-1-simulating-the-dataset",
    "href": "labs/lab-1-InternalValidityPO_sols.html#step-1-simulating-the-dataset",
    "title": "Lab 1 Solutions",
    "section": "Step 1: Simulating the Dataset",
    "text": "Step 1: Simulating the Dataset\nBecause the EHR system at St. Null’s is, in a word, “unreliable” (or, in two words, “utterly broken”), we’ll create our own dataset in R.\nRun the following code to generate 100,000 patient records along with potential outcomes (y0 if no ventilator, y1 if ventilated). Each outcome is the patient’s lifespan (in some made-up units). Note that lifespans below zero are set to zero—any negative numbers would just be an artifact of Dr. P-Hacker’s bizarre data extraction methods.\n\nlibrary(fixest)\nlibrary(data.table)\nset.seed(072121)\n\n# 100,000 people with differing levels of covid symptoms\nN_people = 100000\ndf = data.table(person = 1:N_people)\n\n# Potential outcomes (Y0): life-span if no vent\ndf[, y0 := rnorm(N_people, 9.4, 4)]\ndf[y0 &lt; 0, y0 := 0]\n\n# Potential outcomes (Y1): life-span if assigned to vents\ndf[, y1 := rnorm(N_people, 10, 4)]\ndf[y1 &lt; 0, y1 := 0]\n\n\n\n\n\n\n\nExplanation:\n\n\n\n\nThe necessary packages (fixest and data.table) are loaded.\nThe set.seed(072121) ensures reproducibility, meaning the random numbers generated will be the same every time the code is run.\nA dataset df is created using data.table with 100,000 individuals (each represented by a row).\n\nSimulating Potential Outcomes:\n\n\ny0 represents the expected lifespan (in years) if no ventilator treatment is given. It is drawn from a normal distribution with:\n\nMean: 9.4 years\nStandard deviation: 4 years\n\n\nAny y0 values below 0 (i.e., negative lifespans) are set to zero.\ny1 represents the expected lifespan if assigned to a ventilator. It is similarly drawn from a normal distribution but with a slightly higher mean of 10 years.\nAgain, negative lifespan values are replaced with 0.\n\n\n\n\n\nWe also define the individual treatment effect for each patient. Dr. Doub R. Obust is thrilled, because for once, we have both y0 and y1 simultaneously—an impossible dream in real life!\n\n# Define individual treatment effect\ndf[, delta := y1 - y0]\n\n\n\n\n\n\n\nExplanation:\n\n\n\n\nA new column delta is created, which represents the individual treatment effect (ITE) for each person.\nThe ITE is calculated as the difference between the potential outcome under treatment (y1) and the potential outcome under control (y0).\nThis measures how much additional lifespan (if any) the ventilator treatment provides.\n\n\n\n\n\n\n\n\n\nAlternative Approach\n\n\n\n\n\nThe column can be created using mutate() from dplyr if using tidyverse:\nlibrary(dplyr)\ndf &lt;- df %&gt;% mutate(delta = y1 - y0)"
  },
  {
    "objectID": "labs/lab-1-InternalValidityPO_sols.html#step-2-the-two-doctors",
    "href": "labs/lab-1-InternalValidityPO_sols.html#step-2-the-two-doctors",
    "title": "Lab 1 Solutions",
    "section": "Step 2: The Two Doctors",
    "text": "Step 2: The Two Doctors\nSt. Null’s has two very different doctors assigning ventilators:\n\n\nDr. Perfect: A mystical being who can see into each patient’s future and only gives ventilators to those who would benefit from them.\n\nDr. Bad: The name says it all. This doctor assigns ventilators randomly—could be beneficial, could not be. Who knows?\n\nStep 2a: Assigning Doctors\nFirst, we randomly assign each patient to one of these two doctors. (No wonder this hospital is chaotic…)\n\n# Assign doctors randomly\ndf[, doctor := sample(c(\"perfect\", \"bad\"), N_people, replace = TRUE)]\n\n\n\n\n\n\n\nExplanation\n\n\n\n\n\nsample(c(\"perfect\", \"bad\"), N_people, replace = TRUE)\n\nsample(x, size, replace) randomly selects size elements from the vector x.\nHere, x = c(\"perfect\", \"bad\"), meaning we are choosing between \"perfect\" and \"bad\".\nsize = N_people ensures that we assign a doctor to all N_people individuals.\nreplace = TRUE allows values to be selected independently, meaning each person is assigned a doctor without affecting others.\n\n\n\ndf[, doctor := ...] (Data.table Syntax)\n\ndf[, column_name := value] is data.table’s syntax for adding or modifying a column.\nHere, doctor is created and populated with \"perfect\" or \"bad\" based on the sample() function.\n\n\n\n\n\n\n\n\n\n\n\nAlternative Approach:\n\n\n\n\n\nUsing mutate() from dplyr:\ndf &lt;- df %&gt;%\n  mutate(doctor = sample(c(\"perfect\", \"bad\"), N_people, replace = TRUE))\n\n\nInstead of using sample(), one could use runif() :\ndf[, doctor := ifelse(runif(N_people) &gt; 0.5, \"perfect\", \"bad\")]\nThis approach provides more flexibility if you want to adjust the probability of assigning each type of doctor.\n\n\n\n\nStep 2b: Assigning Ventilators\nNext, each doctor does what they do best. Dr. Perfect uses clairvoyance to treat only those who stand to gain (delta &gt; 0). Dr. Bad flips a metaphorical coin:\n\n# Perfect doctor assigns vents only to those who benefit\ndf[doctor == \"perfect\", vents := (delta &gt; 0)]\n\n# Random doctor assigns vents randomly\ndf[doctor == \"bad\", vents := sample(c(TRUE, FALSE), .N, replace = TRUE)]\n\n\n\n\n\n\n\nExplanation:\n\n\n\n\n\ndf[doctor == \"perfect\", vents := (delta &gt; 0)]\n\ndf[...] is data.table’s way of selecting rows where doctor == \"perfect\".\nvents := (delta &gt; 0) assigns TRUE if delta &gt; 0, meaning treatment is given if the patient benefits.\nThe := operator modifies the column in place, making it more memory-efficient than base R.\n\n\n\ndf[doctor == \"bad\", vents := sample(c(TRUE, FALSE), .N, replace = TRUE)]\n\n.N represents the number of rows in the subset (doctor == \"bad\"), ensuring the right number of values is generated.\nsample(c(TRUE, FALSE), .N, replace = TRUE) randomly assigns TRUE (ventilator given) or FALSE (no ventilator) to these individuals.\nEach person is assigned independently due to replace = TRUE.\n\n\n\n\n\nIt’s not exactly a model of ethical clarity, but it certainly demonstrates the complications of “treatment assignment” in the real world (or the real St. Null’s world)."
  },
  {
    "objectID": "labs/lab-1-InternalValidityPO_sols.html#step-3-computing-causal-parameters",
    "href": "labs/lab-1-InternalValidityPO_sols.html#step-3-computing-causal-parameters",
    "title": "Lab 1 Solutions",
    "section": "Step 3: Computing Causal Parameters",
    "text": "Step 3: Computing Causal Parameters\nNow, let’s calculate the key causal parameters:\n\n\nAverage Treatment Effect (ATE): The overall difference in outcomes if everyone were ventilated vs. if no one were ventilated.\n\nAverage Treatment Effect on the Treated (ATT): The effect of ventilation on those who actually received ventilation.\n\nAverage Treatment Effect on the Untreated (ATU): The effect of ventilation on those who were not ventilated.\n\n\n# Calculate all aggregate Causal Parameters (ATE, ATT, ATU)\nate = df[, mean(delta)]\natt = df[vents == TRUE, mean(delta)]\natu = df[vents == FALSE, mean(delta)]\n\ncat(sprintf(\"ATE = %.03f\n\", ate))\n\nATE = 0.602\n\ncat(sprintf(\"ATT = %.03f\n\", att))\n\nATT = 2.748\n\ncat(sprintf(\"ATU = %.03f\n\", atu))\n\nATU = -1.711\n\n\n\n\n\n\n\n\nExplanation:\n\n\n\n\n\ndf[, mean(delta)]\n\nmean(delta) computes the average of delta, which represents the Average Treatment Effect (ATE).\nSince no filtering is applied, it considers all individuals in the dataset.\n\n\n\ndf[vents == TRUE, mean(delta)]\n\ndf[...] selects rows where vents == TRUE (patients who received ventilation).\nmean(delta) then computes the Average Treatment Effect on the Treated (ATT).\n\n\n\ndf[vents == FALSE, mean(delta)]\n\nThis selects individuals who were not treated (vents == FALSE).\nmean(delta) calculates the Average Treatment Effect on the Untreated (ATU).\n\n\n\ncat(sprintf(\"ATE = %.03f\\n\", ate))\n\nsprintf(\"ATE = %.03f\\n\", ate) formats ate to 3 decimal places.\ncat() prints the formatted result to the console.\n\n\n\n\n\nDr. P-Hacker would stop right here and rejoice: “We have all the significance we need!” But hold your celebratory balloon drop—there’s more to do."
  },
  {
    "objectID": "labs/lab-1-InternalValidityPO_sols.html#step-4-selection-bias-and-realized-outcomes",
    "href": "labs/lab-1-InternalValidityPO_sols.html#step-4-selection-bias-and-realized-outcomes",
    "title": "Lab 1 Solutions",
    "section": "Step 4: Selection Bias and Realized Outcomes",
    "text": "Step 4: Selection Bias and Realized Outcomes\nAlthough the dataset has both y0 and y1, in the real world, a patient’s outcome is observed under only one condition (treated or untreated). Let’s capture which outcome we’d actually see based on the ventilator assignment:\n\n# Use the switching equation to select realized outcomes from potential outcomes based on treatment assignment\ndf[, y := vents * y1 + (1 - vents) * y0]\n\n\n\n\n\n\n\nExplanation :\n\n\n\n\n\nvents * y1 + (1 - vents) * y0\n\nThis applies the switching equation, which determines the observed outcome (y) based on whether an individual received treatment.\nIf vents == TRUE (1), the observed outcome is y1 (the treated potential outcome).\nIf vents == FALSE (0), the observed outcome is y0 (the untreated potential outcome).\n\nMathematically, this follows:\n\ny=vents×y1+(1−vents)×y0\n\n\n\n\n\ndf[, y := ...]\n\nThis is data.table syntax for creating or modifying a column in place.\nThe new column y represents the observed lifespan for each individual.\n\n\n\n\n\nSelection Bias Calculation\nWe’ll see if there is selection bias by comparing the expected lifespan of ventilated patients had they not been ventilated to the expected lifespan of non-ventilated patients.\n\n# Calculate EY0 for vent group and no vent group\ney01 = df[vents == TRUE, mean(y0)]  \ney00 = df[vents == FALSE, mean(y0)] \n\n# Calculate selection bias\nselection_bias = (ey01 - ey00)\n\ncat(sprintf(\n  \"Selection Bias = %.03f - %.03f = %.03f \n\", \n  ey01, ey00, selection_bias\n))\n\nSelection Bias = 8.334 - 10.574 = -2.240 \n\n\n\n\n\n\n\n\nDetailed Explanation of Commands:\n\n\n\n\n\ndf[vents == TRUE, mean(y0)]\n\nThis calculates the expected y0 (lifespan without ventilation) for people who actually received a ventilator.\nIt measures the counterfactual lifespan if the treated group had not received treatment.\n\n\n\ndf[vents == FALSE, mean(y0)]\n\nThis calculates the expected y0 for people who did not receive a ventilator.\nIt represents their actual untreated lifespan.\n\n\n\nSelection Bias Calculation\n\nselection_bias = (ey01 - ey00) compares these two means.\nIf selection into treatment is not random, then the untreated potential outcome (y0) may differ between the treated and untreated groups.\n\n\n\nFormatted Output Using sprintf()\n\nsprintf() formats the output to three decimal places.\ncat() prints the formatted text to the console.\n\n\n\n\n\nIf Dr. Perfect is involved, we’d expect some big differences here. Dr. P-Hacker would probably ignore that and claim victory anyway. (He likes ignoring inconvenient truths.)"
  },
  {
    "objectID": "labs/lab-1-InternalValidityPO_sols.html#step-5-comparing-outcomes-between-groups",
    "href": "labs/lab-1-InternalValidityPO_sols.html#step-5-comparing-outcomes-between-groups",
    "title": "Lab 1 Solutions",
    "section": "Step 5: Comparing Outcomes Between Groups",
    "text": "Step 5: Comparing Outcomes Between Groups\nOne of the simplest ways to estimate the treatment effect is to look at the Simple Difference in Outcomes (SDO)—the difference in the observed mean outcome between those who got the treatment (ventilators) and those who did not.\n\n# Calculate the share of units treated with vents (pi)\npi = mean(df$vents)\n\n# Manually calculate the simple difference in mean health outcomes\ney1 = df[vents == TRUE, mean(y)]\ney0 = df[vents == FALSE, mean(y)]\nsdo = ey1 - ey0\n\ncat(sprintf(\n  \"Simple Difference-in-Outcomes = %.03f - %.03f = %.03f \n\", \n  ey1, ey0, sdo\n))\n\nSimple Difference-in-Outcomes = 11.082 - 10.574 = 0.509 \n\n\n\n\n\n\n\n\nExplanation:\n\n\n\n\n\npi = mean(df$vents)\n\ndf$vents extracts the vents column as a vector.\nmean(df$vents) computes the proportion of individuals who received ventilation (TRUE is treated as 1, FALSE as 0).\nThis provides π (pi), the treatment probability.\n\n\n\ndf[vents == TRUE, mean(y)]\n\nCalculates the mean observed outcome y (lifespan) for the treated group.\n\n\n\ndf[vents == FALSE, mean(y)]\n\nCalculates the mean observed outcome y for the untreated group.\n\n\n\nSimple Difference in Outcomes (SDO)\n\nsdo = ey1 - ey0 computes the naive difference in means.\nThis is an unadjusted estimate of the treatment effect, which may be biased if treatment selection was non-random.\n\n\n\n\n\nDr. P-Hacker would run around shouting: “Aha! This difference proves the intervention works!” or “Aha! It’s not significant!” depending on the p-value. Let’s see if we can do better."
  },
  {
    "objectID": "labs/lab-1-InternalValidityPO_sols.html#step-6-estimating-the-effect-with-regression",
    "href": "labs/lab-1-InternalValidityPO_sols.html#step-6-estimating-the-effect-with-regression",
    "title": "Lab 1 Solutions",
    "section": "Step 6: Estimating the Effect with Regression",
    "text": "Step 6: Estimating the Effect with Regression\nTo confirm our findings, let’s do an Ordinary Least Squares (OLS) regression of the realized outcome on the ventilator indicator:\n\n# Estimate the treatment effect using OLS\nreg = feols(\n  y ~ vents, data = df, \n  vcov = \"hc1\"\n)\n\ncat(\"\n\")\nprint(reg)\n\nOLS estimation, Dep. Var.: y\nObservations: 100,000\nStandard-errors: Heteroskedasticity-robust \n             Estimate Std. Error  t value  Pr(&gt;|t|)    \n(Intercept) 10.573703   0.017534 603.0511 &lt; 2.2e-16 ***\nventsTRUE    0.508554   0.024209  21.0072 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 3.82331   Adj. R2: 0.004388\n\n\n\n\n\n\n\n\nExplanation :\n\n\n\n\n\nfeols(y ~ vents, data = df, vcov = \"hc1\")\n\nfeols() from the fixest package estimates an Ordinary Least Squares (OLS) regression.\nThe formula y ~ vents models the observed outcome (y) as a function of treatment (vents).\nThis provides an estimate of the average treatment effect (ATE) under the assumption of unconfoundedness (i.e., no omitted variables affecting both treatment and outcome).\nvcov = \"hc1\" specifies robust standard errors (Huber-White correction), which adjust for heteroskedasticity in the residuals.\n\n\n\n\n\n\n\n\n\n\n\nAlternative Approach:\n\n\n\n\n\nUsing tidyverse with broom:\nlibrary(broom)\nreg %&gt;% tidy()\n\n\nIf clustering standard errors is needed:\nreg = feols(y ~ vents, data = df, vcov = \"cluster\")\n\n\n\n\nThis approach, while straightforward, is still subject to any biases from non-random assignment—like, say, Dr. Perfect or Dr. Bad being in charge."
  },
  {
    "objectID": "labs/lab-1-InternalValidityPO_sols.html#step-7-validating-the-decomposition",
    "href": "labs/lab-1-InternalValidityPO_sols.html#step-7-validating-the-decomposition",
    "title": "Lab 1 Solutions",
    "section": "Step 7: Validating the Decomposition",
    "text": "Step 7: Validating the Decomposition\nAt this point, we’d like to see if the Simple Difference in Outcomes (SDO) can be broken down into our measured parameters. You’ll fill in a table comparing Dr. Perfect to Dr. Bad, computing the ATE, ATT, ATU, selection bias, SDO, and so on.\nBelow is a quick consistency check to see if the SDO lines up with our theoretical decomposition:\n\n# Decomposition check\nsdo_check = ate + selection_bias + (1 - pi) * (att - atu)\n\ncat(sprintf(\"SDO Check = %.03f \n\", sdo_check))\n\nSDO Check = 0.509 \n\n\n\n\n\n\n\n\nDetailed Explanation of Commands:\n\n\n\n\n\nDecomposition Formula:\n\n\nThe simple difference in outcomes (SDO) can be decomposed into:\n = + + (1 - ) ( - )\n\nThis checks whether our earlier calculations are consistent with theoretical expectations.\n\n\n\nsdo_check = ate + selection_bias + (1 - pi) * (att - atu)\n\nate: The overall average treatment effect.\nselection_bias: The difference in untreated potential outcomes between treated and untreated individuals.\n(1 - pi) * (att - atu): Adjusts for differences in treatment assignment proportions.\n\n\n\n\n\nWhen you fill out the table, you should include:\n\n\nPerfect Doctor\nBad Doctor\nCausal Parameter\n\n\n\nATE\n\n\n\n\nATT\n\n\n\n\nATU\n\n\n\n\nSelection bias terms\n\n\n\n\nE[Y0 | D=1]\n\n\n\n\nE[Y0 | D=0]\n\n\n\n\nSelection bias\n\n\n\n\nCalculations\n\n\n\n\nPi (share on vents)\n\n\n\n\nSDO manual\n\n\n\n\nSDO OLS\n\n\n\n\nSDO Decomposition\n\n\n\n\nObs\n\n\n\n\n\nReflection\nFinally, reflect on these questions:\n\nIs the Simple Difference in Outcomes (SDO) enough to identify the ATE, ATT, or ATU?\nHow does selection bias play into interpreting the SDO?\nWhat lessons would Nurse Random want Dr. P-Hacker to learn about data and design?\n\n(Extra credit if you can imagine the dramatic showdown when Dr. Doub R. Obust finally unveils a doubly robust method that saves the day—but that’s a story for a future lab!)\nThat’s it! You’ve run the gauntlet of the Hospital of Uncertain Outcomes and lived to tell the tale. Now submit your work, and good luck diagnosing more of St. Null’s methodological maladies!"
  },
  {
    "objectID": "labs/lab-2-Power.html",
    "href": "labs/lab-2-Power.html",
    "title": "Lab 2",
    "section": "",
    "text": "In this lab, you will explore the concept of statistical conclusion validity by conducting power calculations via simulation. Specifically, you will:\n\nConduct a power simulation for a simple randomized experiment without clustering (i.e., ignoring the fact that participants may be grouped within clinics).\nExtend that simulation to account for clustering (i.e., clinics as the unit of randomization).\nExamine the impact of sample size, effect size, and clustering on statistical power, and visualize how minimum detectable effect sizes (MDE) change with sample size.\n\nBy the end of this lab, you will have a deeper understanding of:\n\nHow sources of uncertainty (sampling variation, variance in potential outcomes across participants, and measurement error) affect your study’s outcomes.\nWhy we must beware of Type I (false positive) and Type II (false negative) errors—especially if Dr. P-Hacker is anywhere near our data.\nHow p-values should (and should not!) be interpreted.\nHow to simulate data that include cluster-level effects and adjust for these effects in your analysis.",
    "crumbs": [
      "Unit 1: Internal Validity",
      "Lab 2: Power by Simulation"
    ]
  },
  {
    "objectID": "labs/lab-2-Power.html#overview-and-learning-objectives",
    "href": "labs/lab-2-Power.html#overview-and-learning-objectives",
    "title": "Lab 2",
    "section": "",
    "text": "In this lab, you will explore the concept of statistical conclusion validity by conducting power calculations via simulation. Specifically, you will:\n\nConduct a power simulation for a simple randomized experiment without clustering (i.e., ignoring the fact that participants may be grouped within clinics).\nExtend that simulation to account for clustering (i.e., clinics as the unit of randomization).\nExamine the impact of sample size, effect size, and clustering on statistical power, and visualize how minimum detectable effect sizes (MDE) change with sample size.\n\nBy the end of this lab, you will have a deeper understanding of:\n\nHow sources of uncertainty (sampling variation, variance in potential outcomes across participants, and measurement error) affect your study’s outcomes.\nWhy we must beware of Type I (false positive) and Type II (false negative) errors—especially if Dr. P-Hacker is anywhere near our data.\nHow p-values should (and should not!) be interpreted.\nHow to simulate data that include cluster-level effects and adjust for these effects in your analysis.",
    "crumbs": [
      "Unit 1: Internal Validity",
      "Lab 2: Power by Simulation"
    ]
  },
  {
    "objectID": "labs/lab-2-Power.html#the-hospital-drama-begins",
    "href": "labs/lab-2-Power.html#the-hospital-drama-begins",
    "title": "Lab 2",
    "section": "The Hospital Drama Begins",
    "text": "The Hospital Drama Begins\nWelcome to St. Null’s Memorial Hospital, where a brand-new quality improvement intervention is about to be tested. The hospital’s network of clinics, collectively called The Null Distribution, is famous for its dedication to meticulous data collection—and also for some questionable statistical practices performed by a rather infamous staff member.\n\n\nCEO Barnaby Beta has championed a new, cost-intensive intervention aimed at improving patient satisfaction.\n\nNurse Random insists on conducting a proper randomized control trial (RCT), but quickly realizes Dr. P-Hacker might have already meddled with the initial power calculations.\n\nDr. P-Hacker is known to declare victory (“Significant at the 5% level!”) before even cleaning the data, and is rumored to own a golden “p &lt; 0.05” sign that he waves in staff meetings.\n\nDr. Doub R. Obust is the voice of reason, reminding everyone of fundamental statistical principles.\n\nIn this lab, you (the consultants) have been summoned to salvage the situation. Let’s see how this plays out.",
    "crumbs": [
      "Unit 1: Internal Validity",
      "Lab 2: Power by Simulation"
    ]
  },
  {
    "objectID": "labs/lab-2-Power.html#the-plot-thickens-power-calculations-without-clustering",
    "href": "labs/lab-2-Power.html#the-plot-thickens-power-calculations-without-clustering",
    "title": "Lab 2",
    "section": "The Plot Thickens: Power Calculations Without Clustering",
    "text": "The Plot Thickens: Power Calculations Without Clustering\nScene 1: The Mysterious Spreadsheet\nNurse Random bursts into the conference room, clutching a color-coded spreadsheet.\n\nNurse Random: “Dr. P-Hacker, your power calculations look suspiciously high. Did you account for the fact that we’re randomizing by clinic, not by individual patient?”\n\n\nDr. P-Hacker: “Of course not, Nurse Random. Look, a random patient is a random patient—no matter which clinic they come from! Besides, I love high power. It makes me feel like my study can conquer the world!”\n\n\nDr. Doub R. Obust (rolling eyes): “We’re going to need to run a simulation that properly reflects how the intervention is assigned at the clinic level, not just among individual patients. Otherwise, your calculations will be as overconfident as your new P-value neon sign.”\n\nNonetheless, Dr. P-Hacker shares his “method” with you first. Let’s start by replicating his approach (ignoring clustering). This will be our baseline—what not to do if clinics are truly the unit of randomization.\nStep 1: Flawed Power Simulation (Ignoring Clustering)\nWe’ll simulate a dataset as if individual patients are randomly assigned to treatment or control. We’ll compute the statistical power for detecting a true treatment effect of a specified size, ignoring any clinic-level differences.\n\n\n\n\n\n\nTask 1: Flawed Simulation Setup\n\n\n\nFill in the code below to acheive a power of around 0.8 (may not be exact, but get as close as you can):\n\nSet a sample size for the total number of patients.\n\nSpecify a treatment effect (effect).\n\nDecide on the proportion of participants to receive treatment (prop).\n\nSelect a significance level (t_alpha).\n\nRun multiple simulations (sim.size).\n\nThen, run a linear regression on each simulated dataset, gather the p-values, and compute how often the null hypothesis is rejected (i.e., estimate power).\nNote: The code below is not executable. You need to change the eval: false to eval: true to make it work and fill in the blanks.\n\n\n\nlibrary(data.table)\nset.seed(123456)\n\n# Define parameters\nsample_size &lt;-     # e.g. 500\neffect &lt;-          # e.g. 0.2\nprop &lt;-            # e.g. 0.5\nt_alpha &lt;-         # e.g. 0.05\nsim.size &lt;-        # e.g. 2000\n\n# Initialize storage for results\nreject_t &lt;- numeric(sim.size)\n\n# Simulation loop\nfor (i in 1:sim.size) {\n  dt &lt;- data.table(id = 1:sample_size)\n  \n  # Assign treatment individually (incorrect for cluster randomization!)\n  dt[, treatment := rbinom(.N, 1, prop)]\n  \n  # Simulate outcome (10 is baseline, 'effect' is added if treatment=1)\n  dt[, outcome := 10 + effect * treatment + rnorm(.N, mean = 0, sd = 1)]\n  \n  # Estimate the effect\n  fit &lt;- lm(outcome ~ treatment, data = dt)\n  p_value &lt;- summary(fit)$coefficients[2,4]\n  \n  # Check if we reject H0: (p-value &lt; alpha)\n  reject_t[i] &lt;- ifelse(p_value &lt; t_alpha, 1, 0)\n}\n\n# Compute estimated power\npower_flawed &lt;- mean(reject_t)\ncat(\"Estimated Power (Ignoring Clustering):\", power_flawed, \"\\n\")\n\nDiscussion of Step 1\n\n\nNurse Random sighs: “We got an estimated power of 0.8 (yours might be slightly different). But do we trust this number?”\n\nDr. Doub R. Obust chimes in: “Nope. We’re ignoring that patients within the same clinic might be more similar to each other than to patients in other clinics. Our standard errors are artificially small.”\n\nAt this point, CEO Barnaby Beta perks up: “Artificially small standard errors? That means we’re basically claiming more precision than we actually have, right?”\nYes, indeed. If we disregard the clustering of patients, we risk making a Type I error more often than we realize. Dr. P-Hacker, in typical fashion, responds:\n\nDr. P-Hacker: “Type I error? Isn’t that just when we see something interesting that’s obviously there?!”Dr. Doub R. Obust (groaning): “No. A Type I error is a false positive—we conclude there is an effect even though, in truth, there isn’t. Like thinking you’ve discovered a rare golden banana flavor at the cafeteria soda machine when really it’s just seltzer water with a weird label!”",
    "crumbs": [
      "Unit 1: Internal Validity",
      "Lab 2: Power by Simulation"
    ]
  },
  {
    "objectID": "labs/lab-2-Power.html#understanding-the-sources-of-uncertainty",
    "href": "labs/lab-2-Power.html#understanding-the-sources-of-uncertainty",
    "title": "Lab 2",
    "section": "Understanding the Sources of Uncertainty",
    "text": "Understanding the Sources of Uncertainty\nBefore we fix our simulation, Dr. Doub R. Obust insists that you reflect on why ignoring clinic-level clustering is a problem. He ticks off sources of variability that a real experiment faces:\n\n\nSampling Variation: Even if you have a large population of patients, the sample you select is just one draw from a bigger population. Different samples might give different estimates.\n\n\nMeasurement Error: If your measurement tool for patient well-being is noisy (e.g., patients often mis-report how they feel, or staff record data incorrectly), it introduces extra variability that can muddy your effect estimates.\n\nVariance in Potential Outcomes: Not all patients respond to treatments in the same way. Some might be strongly affected, some not at all—leading to variation in the outcomes. This includes clustering: Patients in the same clinic share certain characteristics, environmental factors, or staff practices that can affect their potential outcomes. This correlation must be accounted for in the design and analysis.\n\n\nNurse Random: “So if we ignore that last point—clustering—our estimate of the variability (and thus the standard error) is off, and we might incorrectly claim significance. That’s basically p-hacking 101!”",
    "crumbs": [
      "Unit 1: Internal Validity",
      "Lab 2: Power by Simulation"
    ]
  },
  {
    "objectID": "labs/lab-2-Power.html#power-calculations-with-clustering",
    "href": "labs/lab-2-Power.html#power-calculations-with-clustering",
    "title": "Lab 2",
    "section": "Power Calculations With Clustering",
    "text": "Power Calculations With Clustering\nScene 2: A (Cluster-)Randomized Trial\nNow we come to the correct approach: our unit of randomization is the clinic, not the individual. That means each clinic either receives the intervention or does not, and all patients in a clinic share the same treatment assignment.\nStep 2: Incorporating Clustering\nWe’ll model:\n\n\nnum_clusters = number of clinics.\n\n\ncluster_size = number of patients per clinic.\n\n\nicc (Intraclass Correlation Coefficient): A measure of how strongly patients in the same clinic resemble each other. High ICC means patients within a clinic are more correlated.\n\nWe’ll generate clinic-level “random effects” and individual-level error terms to reflect both measurement error and the variability in potential outcomes across individuals.\n\n\n\n\n\n\n\nTask 2: Correcting the Simulation for Clustering\n\n\n\nFill in the code below to acheive a power of 0.8:\n\nDefine the number of clinics and the number of patients per clinic.\n\nAssign each entire clinic to treatment or control.\n\nIncorporate cluster-level random effects.\n\nFit the model but adjust standard errors for clustering.\n\n\n\n\n\nlibrary(multiwayvcov)  # for cluster-robust standard errors\n\n# Define cluster parameters\nnum_clusters &lt;-     # e.g. 50\ncluster_size &lt;-     # e.g. 10\ntotal_sample &lt;- num_clusters * cluster_size\nicc &lt;-              # e.g. 0.4\n\n# Variances\nind_err_var &lt;- 1\n# cluster_err_var is derived from the intraclass correlation coefficient\ncluster_err_var &lt;- (icc * ind_err_var) / (1 - icc)\n\nsim.size &lt;-         # e.g. 2000\neffect &lt;-           # e.g. 0.2\nprop &lt;-             # e.g. 0.5\nt_alpha &lt;-          # e.g. 0.05\n\nreject_t &lt;- numeric(sim.size)\n\nset.seed(654321)\nfor (i in 1:sim.size) {\n  \n  # Create a data table with one row per cluster, repeated for each patient\n  clusters &lt;- data.table(\n    cluster = rep(1:num_clusters, each = cluster_size)\n  )\n  \n  # Generate a cluster-level random effect\n  clusters[, cluster_error := rep(\n    rnorm(num_clusters, mean = 0, sd = sqrt(cluster_err_var)),\n    each = cluster_size\n  )]\n  \n  # Randomize at the clinic level\n  clusters[, treatment := rep(\n    rbinom(num_clusters, 1, prop),\n    each = cluster_size\n  )]\n  \n  # Add an individual-level error\n  clusters[, individual_error := rnorm(.N, mean = 0, sd = sqrt(ind_err_var))]\n  \n  # Final outcome for each individual\n  clusters[, outcome := 10 + effect * treatment + cluster_error + individual_error]\n  \n  # Fit a naive linear model (ignoring clustering in standard errors)\n  fit &lt;- lm(outcome ~ treatment, data = clusters)\n  \n  # Adjust standard errors for clustering\n  robust_SE &lt;- cluster.vcov(fit, clusters$cluster, df_correction = TRUE)\n  robust_coef &lt;- coeftest(fit, robust_SE)\n  \n  # Get the p-value for the treatment coefficient\n  p_value &lt;- robust_coef[2,4]\n  reject_t[i] &lt;- ifelse(p_value &lt; t_alpha, 1, 0)\n}\n\n# Compute estimated power with clustering\npower_clustered &lt;- mean(reject_t)\ncat(\"Estimated Power (Clustered):\", power_clustered, \"\\n\")\n\nDiscussion of Step 2\nDr. Doub R. Obust looks at the new power estimate and remarks:\n\n“As you can see, once we account for clustering, the power is (usually) lower than in the flawed approach. That’s because those cluster-level similarities effectively reduce the amount of independent information we have. Our standard errors are bigger, so it’s harder to find significance unless the effect size or sample size is larger.”\n\nCEO Barnaby Beta shakes his head: “But that means our study might be underpowered if we stick to our current budget!”\nNurse Random replies with a grin: “Don’t worry, we can plan more carefully. Otherwise, if we run a smaller study, we risk a Type II error—failing to reject the null hypothesis when there really is an effect. You know, like leaving the gold standard intervention on the shelf because we didn’t gather enough data to prove it works.”\nDr. P-Hacker interjects:\n\n“And there’s always p &lt; 0.10, right? We can move our threshold for significance to get more ‘positive’ results!”Nurse Random (scolding): “That’s literally p-hacking. Please step away from the analysis, Doctor.”",
    "crumbs": [
      "Unit 1: Internal Validity",
      "Lab 2: Power by Simulation"
    ]
  },
  {
    "objectID": "labs/lab-2-Power.html#the-truth-about-p-values",
    "href": "labs/lab-2-Power.html#the-truth-about-p-values",
    "title": "Lab 2",
    "section": "The Truth About p-Values",
    "text": "The Truth About p-Values\nA quick comedic break for a lesson on p-values:\n\n\nDr. P-Hacker keeps shouting that p &lt; 0.05 means there’s a 5% chance the null hypothesis is true.\n\n\nNurse Random corrects him: “No, no, no. A p-value is the probability of observing a result at least as extreme as ours if the null is true. It’s NOT the probability the null is true. You can’t interpret it that way.”\n\n\nDr. P-Hacker: “I was so sure it was the probability that I was right.”Dr. Doub R. Obust: “We live and learn, Doctor.”",
    "crumbs": [
      "Unit 1: Internal Validity",
      "Lab 2: Power by Simulation"
    ]
  },
  {
    "objectID": "labs/lab-2-Power.html#plotting-the-minimum-detectable-effect-mde",
    "href": "labs/lab-2-Power.html#plotting-the-minimum-detectable-effect-mde",
    "title": "Lab 2",
    "section": "Plotting the Minimum Detectable Effect (MDE)",
    "text": "Plotting the Minimum Detectable Effect (MDE)\nBecause CEO Barnaby Beta wants to know how large an effect must be to have a reasonable chance of detection, you might want to simulate across a range of effect sizes and/or sample sizes. You can then plot the resulting power curves to see what the Minimum Detectable Effect (MDE) is for a given power requirement.\n\n\n\n\n\n\nTask 3: Create an MDE Plot\n\n\n\n\nLoop over a grid of effect sizes (or sample sizes).\n\nFor each value, compute power using the cluster-based simulation approach.\n\nPlot the effect size on the x-axis and the corresponding power on the y-axis.\n\n\n\n\n# Example code snippet (feel free to modify)\nlibrary(ggplot2)\n\neffect_sizes &lt;- seq(0, 0.5, by = 0.05) \nresults &lt;- data.table(effect = effect_sizes, power = NA_real_)\n\nfor (e in seq_along(effect_sizes)) {\n  # Repeat your cluster simulation steps but with effect = effect_sizes[e]\n  # ...\n  # store the average of reject_t in results$power[e]\n  # ...\n}\n\nggplot(results, aes(x = effect, y = power)) +\n  geom_line() +\n  geom_point() +\n  labs(\n    title = \"Power vs. Effect Size\",\n    x = \"Effect Size\",\n    y = \"Power\"\n  ) +\n  theme_minimal()",
    "crumbs": [
      "Unit 1: Internal Validity",
      "Lab 2: Power by Simulation"
    ]
  },
  {
    "objectID": "labs/lab-2-Power.html#final-words-from-the-hospital-staff",
    "href": "labs/lab-2-Power.html#final-words-from-the-hospital-staff",
    "title": "Lab 2",
    "section": "Final Words from the Hospital Staff",
    "text": "Final Words from the Hospital Staff\nCEO Barnaby Beta: “Alright, so if we need to ensure we have enough clinics and participants to achieve our desired power, we may need a bigger budget than expected. Let’s not forget that ignoring clustering would have given us a false sense of security in our power. Now we know better.”\nNurse Random: “And no more Type I or Type II error confusion, Dr. P-Hacker. We must keep our definitions straight if we’re to have any credibility around here!”\nDr. P-Hacker (sighing): “Fine, fine. Guess I’ll tone down the p-value hype. But I’m keeping my neon sign.”\nDr. Doub R. Obust (with a grin): “We’ll allow the neon sign, as long as you promise to interpret it correctly.”",
    "crumbs": [
      "Unit 1: Internal Validity",
      "Lab 2: Power by Simulation"
    ]
  },
  {
    "objectID": "labs/lab-2-Power.html#submission-instructions",
    "href": "labs/lab-2-Power.html#submission-instructions",
    "title": "Lab 2",
    "section": "Submission Instructions",
    "text": "Submission Instructions\n\nMake sure your .qmd file knits successfully (to .pdf or .html).\n\nUpload your compiled document to Gradescope.\n\n\nInclude your discussion of the results, your code, and your responses to the callout sections.\n\nRemember, the moral of the story: Always check who (or what) is being randomized, and account for clustering when necessary! Otherwise, your study conclusions might be as random as Dr. P-Hacker’s next dinner choice.",
    "crumbs": [
      "Unit 1: Internal Validity",
      "Lab 2: Power by Simulation"
    ]
  },
  {
    "objectID": "course-communication.html",
    "href": "course-communication.html",
    "title": "Communication",
    "section": "",
    "text": "Slack will be our primary communication hub. An invitation to the course Slack will be shared on the first day of class. Use this place to ask questions, share resources, and engage with your peers, the instructor, and the TA. (For personal or sensitive matters, please use e-mail)\n\n\n\n\n\n\nImportant:\n\n\n\nSlack will not be continuously monitored by the instructor or TA outside of posted office hours. At other times, students are expected to help/engage with one another to foster a shared learning experience. If a question hasn’t been addressed, the TA or Sean will respond but not immediately or even the same day.\nYou may DM Sean or the TA in Slack (but, again, don’t expect a response immediately or even the same day.)\n\n\n\nThe #announcements channel will be used to share course announcements.\nUse the #help channel as a first stop for general questions about the class or for technical assistance.\nUse the #general channel to discuss lab assignments and discuss/share resources related to course topics.\nUse the #interesting-stuff channel to share relevant articles, podcasts, packages, data sources, and anything else relevant that you think your classmates will benefit from.\n\n\n\n\n\n\n\nDon’t post any sensitive or personal information in the class Slack\n\n\n\nSlack is only for general class discussions, announcements, and collaboration. In particular, please do not share or post any of the following:\n\nIndividual or group grades, GPA, or other academic evaluations or records.\nSocial Security Numbers\nPersonal information about tuition payments, financial aid, or scholarships.\nAny information on accommodations or health-related information.\nAny other sensitive information\n\nFor sensitive information, please use UNC email.",
    "crumbs": [
      "Course Information",
      "Communication"
    ]
  },
  {
    "objectID": "course-communication.html#slack",
    "href": "course-communication.html#slack",
    "title": "Communication",
    "section": "",
    "text": "Slack will be our primary communication hub. An invitation to the course Slack will be shared on the first day of class. Use this place to ask questions, share resources, and engage with your peers, the instructor, and the TA. (For personal or sensitive matters, please use e-mail)\n\n\n\n\n\n\nImportant:\n\n\n\nSlack will not be continuously monitored by the instructor or TA outside of posted office hours. At other times, students are expected to help/engage with one another to foster a shared learning experience. If a question hasn’t been addressed, the TA or Sean will respond but not immediately or even the same day.\nYou may DM Sean or the TA in Slack (but, again, don’t expect a response immediately or even the same day.)\n\n\n\nThe #announcements channel will be used to share course announcements.\nUse the #help channel as a first stop for general questions about the class or for technical assistance.\nUse the #general channel to discuss lab assignments and discuss/share resources related to course topics.\nUse the #interesting-stuff channel to share relevant articles, podcasts, packages, data sources, and anything else relevant that you think your classmates will benefit from.\n\n\n\n\n\n\n\nDon’t post any sensitive or personal information in the class Slack\n\n\n\nSlack is only for general class discussions, announcements, and collaboration. In particular, please do not share or post any of the following:\n\nIndividual or group grades, GPA, or other academic evaluations or records.\nSocial Security Numbers\nPersonal information about tuition payments, financial aid, or scholarships.\nAny information on accommodations or health-related information.\nAny other sensitive information\n\nFor sensitive information, please use UNC email.",
    "crumbs": [
      "Course Information",
      "Communication"
    ]
  },
  {
    "objectID": "course-communication.html#office-hours",
    "href": "course-communication.html#office-hours",
    "title": "Communication",
    "section": "Office Hours",
    "text": "Office Hours\nSean will hold office hours on Wednesdays from 1-2pm. Please book an appointment at least 24 hours ahead of time using this LINK.\nThe TA will hold office hours on…",
    "crumbs": [
      "Course Information",
      "Communication"
    ]
  },
  {
    "objectID": "course-communication.html#email",
    "href": "course-communication.html#email",
    "title": "Communication",
    "section": "Email",
    "text": "Email\nPlease use e-mail only for personal matters. (For anything related to the course material or coding questions, please use Slack.)",
    "crumbs": [
      "Course Information",
      "Communication"
    ]
  },
  {
    "objectID": "ex-simulate.html",
    "href": "ex-simulate.html",
    "title": "Simulating Experimental Data with Noncompliance",
    "section": "",
    "text": "In this example, we simulate data for a simple randomized experiment with one treatment and one control group. The randomization is at the individual level. Additionally, we introduce noncompliance, where some individuals assigned to the treatment group do not take the treatment.\n\n\nDefine the parameters for the experiment, such as the number of participants, compliance rate, and treatment effects.\n\n# Set seed for reproducibility\nset.seed(123)\n\n# Parameters\nn &lt;- 500  # Total number of participants\nprob_treatment &lt;- 0.5  # Probability of being assigned to treatment\ncompliance_rate &lt;- 0.8  # Proportion of treated participants who comply\n\n# Treatment effect\nbaseline_mean &lt;- 50  # Mean outcome for control group\ntreatment_effect &lt;- 10  # Additional effect for treated participants\n\n\nSimulate random assignment to treatment and compliance behavior.\n\n# Simulate random assignment\nassignment &lt;- rbinom(n, 1, prob_treatment)  # 1 = assigned to treatment, 0 = control\n\n# Simulate compliance behavior\n# If assigned to treatment, comply with probability `compliance_rate`\ncompliance &lt;- ifelse(assignment == 1, rbinom(n, 1, compliance_rate), 0)  # 1 = complied, 0 = did not comply\n\n\nSimulate the outcome variable based on compliance and treatment assignment.\n\n# Simulate outcomes\noutcome &lt;- baseline_mean + \n  (assignment * compliance * treatment_effect) +  # Effect for those who comply\n  rnorm(n, mean = 0, sd = 5)  # Add random noise\n\n# Combine data into a data frame\nexperiment_data &lt;- data.frame(\n  ID = 1:n,\n  Assignment = assignment,\n  Compliance = compliance,\n  Outcome = outcome\n)\n\n# View the first few rows of the data\nhead(experiment_data)\n\n  ID Assignment Compliance  Outcome\n1  1          0          0 46.99054\n2  2          1          1 55.03151\n3  3          0          0 55.13393\n4  4          1          1 63.75531\n5  5          1          1 52.45417\n6  6          0          0 49.52426\n\n\n\nExport the simulated data to a CSV file for further analysis.\n\n# Export the data to a CSV file\nwrite.csv(experiment_data, \"simulated_experiment_data.csv\", row.names = FALSE)\n\n# Confirm export\ncat(\"Data has been exported to 'simulated_experiment_data.csv'\")\n\nData has been exported to 'simulated_experiment_data.csv'",
    "crumbs": [
      "Semester Project",
      "Data Simulation Example"
    ]
  },
  {
    "objectID": "ex-simulate.html#step-1-setup-and-parameters",
    "href": "ex-simulate.html#step-1-setup-and-parameters",
    "title": "Simulating Experimental Data with Noncompliance",
    "section": "",
    "text": "Define the parameters for the experiment, such as the number of participants, compliance rate, and treatment effects.\n\n# Set seed for reproducibility\nset.seed(123)\n\n# Parameters\nn &lt;- 500  # Total number of participants\nprob_treatment &lt;- 0.5  # Probability of being assigned to treatment\ncompliance_rate &lt;- 0.8  # Proportion of treated participants who comply\n\n# Treatment effect\nbaseline_mean &lt;- 50  # Mean outcome for control group\ntreatment_effect &lt;- 10  # Additional effect for treated participants",
    "crumbs": [
      "Semester Project",
      "Data Simulation Example"
    ]
  },
  {
    "objectID": "ex-simulate.html#step-2-random-assignment-and-compliance",
    "href": "ex-simulate.html#step-2-random-assignment-and-compliance",
    "title": "Simulating Experimental Data with Noncompliance",
    "section": "",
    "text": "Simulate random assignment to treatment and compliance behavior.\n\n# Simulate random assignment\nassignment &lt;- rbinom(n, 1, prob_treatment)  # 1 = assigned to treatment, 0 = control\n\n# Simulate compliance behavior\n# If assigned to treatment, comply with probability `compliance_rate`\ncompliance &lt;- ifelse(assignment == 1, rbinom(n, 1, compliance_rate), 0)  # 1 = complied, 0 = did not comply",
    "crumbs": [
      "Semester Project",
      "Data Simulation Example"
    ]
  },
  {
    "objectID": "ex-simulate.html#step-3-simulate-outcomes",
    "href": "ex-simulate.html#step-3-simulate-outcomes",
    "title": "Simulating Experimental Data with Noncompliance",
    "section": "",
    "text": "Simulate the outcome variable based on compliance and treatment assignment.\n\n# Simulate outcomes\noutcome &lt;- baseline_mean + \n  (assignment * compliance * treatment_effect) +  # Effect for those who comply\n  rnorm(n, mean = 0, sd = 5)  # Add random noise\n\n# Combine data into a data frame\nexperiment_data &lt;- data.frame(\n  ID = 1:n,\n  Assignment = assignment,\n  Compliance = compliance,\n  Outcome = outcome\n)\n\n# View the first few rows of the data\nhead(experiment_data)\n\n  ID Assignment Compliance  Outcome\n1  1          0          0 46.99054\n2  2          1          1 55.03151\n3  3          0          0 55.13393\n4  4          1          1 63.75531\n5  5          1          1 52.45417\n6  6          0          0 49.52426",
    "crumbs": [
      "Semester Project",
      "Data Simulation Example"
    ]
  },
  {
    "objectID": "ex-simulate.html#step-4-export-data-to-csv",
    "href": "ex-simulate.html#step-4-export-data-to-csv",
    "title": "Simulating Experimental Data with Noncompliance",
    "section": "",
    "text": "Export the simulated data to a CSV file for further analysis.\n\n# Export the data to a CSV file\nwrite.csv(experiment_data, \"simulated_experiment_data.csv\", row.names = FALSE)\n\n# Confirm export\ncat(\"Data has been exported to 'simulated_experiment_data.csv'\")\n\nData has been exported to 'simulated_experiment_data.csv'",
    "crumbs": [
      "Semester Project",
      "Data Simulation Example"
    ]
  },
  {
    "objectID": "unit-0/prep-1.html",
    "href": "unit-0/prep-1.html",
    "title": "Class Preparation",
    "section": "",
    "text": "In preparation for our upcoming class on [Insert Topic Here], please complete the following activities. These materials will set the foundation for our in-class discussions and activities.\n\n\n\n\n\n[Insert Reading Title and Link Here]\n[Insert Additional Reading Title and Link Here]\n\n\n\n\n\n\n[Insert Video Title and Link Here]\n[Insert Additional Video Title and Link Here]\n\n\n\n\n\nComplete the pre-class quiz on Sakai: [Insert Link Here]\nDeadline: [Insert Date and Time]\n\n\n\n\nConsider the following questions as you review the materials: 1. [Insert Question Here] 2. [Insert Additional Question Here] 3. [Insert Additional Question Here]\n\n\n\n\nPrepare notes summarizing key concepts from the readings and videos. These will be helpful for in-class discussions.\n\nRemember: This course uses a flipped classroom approach. Completing these tasks prior to class is essential for your active participation and understanding during our sessions."
  },
  {
    "objectID": "unit-0/prep-1.html#overview",
    "href": "unit-0/prep-1.html#overview",
    "title": "Class Preparation",
    "section": "",
    "text": "In preparation for our upcoming class on [Insert Topic Here], please complete the following activities. These materials will set the foundation for our in-class discussions and activities."
  },
  {
    "objectID": "unit-0/prep-1.html#required-readings",
    "href": "unit-0/prep-1.html#required-readings",
    "title": "Class Preparation",
    "section": "",
    "text": "[Insert Reading Title and Link Here]\n[Insert Additional Reading Title and Link Here]"
  },
  {
    "objectID": "unit-0/prep-1.html#required-videos",
    "href": "unit-0/prep-1.html#required-videos",
    "title": "Class Preparation",
    "section": "",
    "text": "[Insert Video Title and Link Here]\n[Insert Additional Video Title and Link Here]"
  },
  {
    "objectID": "unit-0/prep-1.html#quiz",
    "href": "unit-0/prep-1.html#quiz",
    "title": "Class Preparation",
    "section": "",
    "text": "Complete the pre-class quiz on Sakai: [Insert Link Here]\nDeadline: [Insert Date and Time]"
  },
  {
    "objectID": "unit-0/prep-1.html#reflection-questions",
    "href": "unit-0/prep-1.html#reflection-questions",
    "title": "Class Preparation",
    "section": "",
    "text": "Consider the following questions as you review the materials: 1. [Insert Question Here] 2. [Insert Additional Question Here] 3. [Insert Additional Question Here]"
  },
  {
    "objectID": "unit-0/prep-1.html#notes",
    "href": "unit-0/prep-1.html#notes",
    "title": "Class Preparation",
    "section": "",
    "text": "Prepare notes summarizing key concepts from the readings and videos. These will be helpful for in-class discussions.\n\nRemember: This course uses a flipped classroom approach. Completing these tasks prior to class is essential for your active participation and understanding during our sessions."
  },
  {
    "objectID": "unit-0/unit-0-foundations-2.html",
    "href": "unit-0/unit-0-foundations-2.html",
    "title": "Unit 0: Foundations",
    "section": "",
    "text": "Guest Lecture by Dr. Dorien Emmers\n\n\n\n\n\n\nClass will be remote-only\n\n\n\nJoin from the comfort of your own home!\nZoom link: https://unc.zoom.us/j/7840211370?omn=97363476255\n\n\n\n\nSkim: Using Community Health Workers to Deliver a Scalable Integrated Parenting Program in Rural China: A Cluster Randomized Controlled Trial\n\nOptional\n\nDownload Replication Data and Code: \nDr. Emmers wrote this analysis code in Stata (Boo! ). Try converting it to R using the stata2r package.\n\n\n\n\n\n\n\n\nPerusall\n\n\n\nPerusall is a free online platform that allows you to collaboratively annotate content with your classmates. Here is a link to the Perusall page for this course: HPM 883. If needed, the class enrollment code is SYLVIA-ZXTWH.\n\nAlthough it is a good way to engage with your classmates around the material, it is not required that you comment on Perusall. If you wish, you can just download the readings directly.\n\n\n\nSubmit: Any questions you have for Dr. Emmers? Submit to the #guest-lecture-questions channel on Slack\n\n\n\n\n\nSlides\nZoom Recording\n\n\n\n\n\n\n\nIntroductory Survey Due by end of day on Jan 15\n\n\n\nIntroductory Class Survey",
    "crumbs": [
      "Unit 0: Foundations",
      "Unit 0.1: A Field Experiment in Health Services Research"
    ]
  },
  {
    "objectID": "unit-0/unit-0-foundations-2.html#overview",
    "href": "unit-0/unit-0-foundations-2.html#overview",
    "title": "Unit 0: Foundations",
    "section": "",
    "text": "Guest Lecture by Dr. Dorien Emmers\n\n\n\n\n\n\nClass will be remote-only\n\n\n\nJoin from the comfort of your own home!\nZoom link: https://unc.zoom.us/j/7840211370?omn=97363476255\n\n\n\n\nSkim: Using Community Health Workers to Deliver a Scalable Integrated Parenting Program in Rural China: A Cluster Randomized Controlled Trial\n\nOptional\n\nDownload Replication Data and Code: \nDr. Emmers wrote this analysis code in Stata (Boo! ). Try converting it to R using the stata2r package.\n\n\n\n\n\n\n\n\nPerusall\n\n\n\nPerusall is a free online platform that allows you to collaboratively annotate content with your classmates. Here is a link to the Perusall page for this course: HPM 883. If needed, the class enrollment code is SYLVIA-ZXTWH.\n\nAlthough it is a good way to engage with your classmates around the material, it is not required that you comment on Perusall. If you wish, you can just download the readings directly.\n\n\n\nSubmit: Any questions you have for Dr. Emmers? Submit to the #guest-lecture-questions channel on Slack\n\n\n\n\n\nSlides\nZoom Recording\n\n\n\n\n\n\n\nIntroductory Survey Due by end of day on Jan 15\n\n\n\nIntroductory Class Survey",
    "crumbs": [
      "Unit 0: Foundations",
      "Unit 0.1: A Field Experiment in Health Services Research"
    ]
  },
  {
    "objectID": "unit-0/lec-0.html#course-summary",
    "href": "unit-0/lec-0.html#course-summary",
    "title": "Course Introduction",
    "section": "Course Summary",
    "text": "Course Summary\nExperiments and Machine Learning for Health Services Research\nPrimary Goal: Equip you with tools to design, analyze, and interpret field experiments and apply machine learning to health services research.\n\nKey Focus Areas:\nField experiments for causal inference.\nMachine learning for prediction and causal analysis.\nApplications:\nHealth policy evaluations, resource allocation, and improving healthcare decision-making."
  },
  {
    "objectID": "unit-0/lec-0.html#about-me-dr.-sean-sylvia",
    "href": "unit-0/lec-0.html#about-me-dr.-sean-sylvia",
    "title": "Course Introduction",
    "section": "About Me: Dr. Sean Sylvia",
    "text": "About Me: Dr. Sean Sylvia\n\n\nResearch: Designing & evaluating innovative approaches to improve health service delivery, globally.\n\nExperimental methods, machine learning, economics of digital health\n\nUNC Health Policy & Management (7 years)\n\nCarolina Population Center\nSheps Center for Health Services Research\n\nPreviously: Renmin University of China, World Bank\nTraining: U Maryland (PhD), Stanford (pre-doc)"
  },
  {
    "objectID": "unit-0/lec-0.html#proud-randomista",
    "href": "unit-0/lec-0.html#proud-randomista",
    "title": "Course Introduction",
    "section": "Proud “Randomista”",
    "text": "Proud “Randomista”\n\n\n\nExtensive fieldwork using randomized controlled trials (RCTs)\nStarting in 2006, I’ve worked with colleagues on ~20 RCTs in China, Africa, and North Carolina.\n\n[Hot Tip: Collaborate. Collaborate. Collaborate.]\n\nKey projects include:\n\nField experiments in rural China on primary healthcare\nDesign and evaluation of community health worker interventions for early childhood health and development\nBehavioral nudges to recruit providers to MAT training for Opiod Use Disorder in North Carolina\n\n\n\nSCROLL DOWN TO READ Nobel Prize in Economics 2019 was awarded jointly to Abhijit Banerjee, Esther Duflo and Michael Kremer “for their experimental approach to alleviating global poverty”"
  },
  {
    "objectID": "unit-0/lec-0.html#teaching-assistant-yumeng-du",
    "href": "unit-0/lec-0.html#teaching-assistant-yumeng-du",
    "title": "Course Introduction",
    "section": "Teaching Assistant: Yumeng Du",
    "text": "Teaching Assistant: Yumeng Du\nPhD Student in Health Policy and Management (Economics Track)\nResearch Interests:\n- Evaluation of digital health programs for underserved populations\nBackground:\n- MSc in Public Health, London School of Hygiene and Tropical Medicine (LSHTM), UK\n- MBBS, Central South University, China\n- Former RA at UNC-China Project,implementing an RCT on rural telemedicine kiosk program\nRole:\n- Available for technical support and office hours\n- Assisting with labs and project feedback"
  },
  {
    "objectID": "unit-0/lec-0.html#experiments-and-machine-learning-for-hsr",
    "href": "unit-0/lec-0.html#experiments-and-machine-learning-for-hsr",
    "title": "Course Introduction",
    "section": "Experiments and Machine Learning for HSR",
    "text": "Experiments and Machine Learning for HSR\n\n\n\nRandomized experiments: the gold standard for testing mechanisms and interpretability.\nMachine Learning: Ideal for useful predictions when interpretability or theory is less critical.\nBridging the Divide:\n\nNew methods apply core ML principles to causal inference.\n\nAs we’ll see, parts of causal inference problems can be recast as prediction problems.\n\nPowerful tools for theory-building and expanding scope of feasible research.\n\n\n\n\n\n\n\n\n\nNote\n\n\nImportant!\nYou’ll hear the term “causal machine learning” in the literature. This is a misnomer!\nMachine Learning is about prediction. Period. It does not magically solve the causal inference problem.\nIt CAN:\n\nHelp strengthen plausibility of existing exclusion restriction in some cases\nAllow us to study treatment effect heterogeneity in new ways"
  },
  {
    "objectID": "unit-0/lec-0.html#reproducibility",
    "href": "unit-0/lec-0.html#reproducibility",
    "title": "Course Introduction",
    "section": "Reproducibility",
    "text": "Reproducibility\nReproducibility is a Fundamental Principle of Science\n\nGood experiments are necessarily reproducible.\n\nClear, well-documented protocols, reproducible data collection, and clear and reproducible analysis.\nThe “reproducibility crisis” in some fields (e.g. psychology). This is actually a good thing! Only possible to confirm results if they are reproducible.\n\nReproducibility and ML:\n\nExploratory analyses, when systematic, help build reliable theories.\nML provides tools for:\n\nConducting reproducible exploratory research.\nIdentifying robust treatment effect heterogeneity."
  },
  {
    "objectID": "unit-0/lec-0.html#causal-inference-as-a-missing-data-problem",
    "href": "unit-0/lec-0.html#causal-inference-as-a-missing-data-problem",
    "title": "Course Introduction",
    "section": "Causal Inference as a Missing Data Problem",
    "text": "Causal Inference as a Missing Data Problem\n\n\n\nThe Fundamental Problem of Causal Inference:\n\nWe cannot observe a unit in multiple states of the world simultaneously.\nRepresented as “missing data” in potential outcomes.\n\nExperimentation Solves under some key assumptions:\n\nSUTVA.\nObservability.\nComplete Compliance.\nStatistical Independence.\n\n\n\n\n\n\nUnit\nTreatment (W)\nY(0)\nY(1)\nObserved Outcome\n\n\n\n\n1\n1\n?\n5\n5\n\n\n2\n0\n3\n?\n3\n\n\n3\n1\n?\n6\n6\n\n\n4\n0\n4\n?\n4\n\n\n\n\nProblem: We can only observe either (Y(0)) or (Y(1)) but not both for any individual."
  },
  {
    "objectID": "unit-0/lec-0.html#randomization-recovers-the-ate-on-average.",
    "href": "unit-0/lec-0.html#randomization-recovers-the-ate-on-average.",
    "title": "Course Introduction",
    "section": "Randomization recovers the ATE on average.",
    "text": "Randomization recovers the ATE on average.\n\nPotential outcomes: \\(Y_i(1)\\) and \\(Y_i(0)\\)\nTreatment indicator: \\(W_i = 1\\) if treated, \\(W_i = 0\\) if control\nObserved outcome: \\(Y_i = W_iY_i(1) + (1 - W_i)Y_i(0)\\)\nAverage Treatment Effect (ATE): \\(\\tau = \\mathbb{E}[Y_i(1) - Y_i(0)]\\)\n\nRandomization ensures that treatment is independent of potential outcomes:\n\\[\nW_i \\perp \\!\\!\\! \\perp (Y_i(0), Y_i(1)),\n\\]\nHence, \\(\\mathbb{E}[Y | W_i = w] = \\mathbb{E}[Y_i(w) | W_i = w] = \\mathbb{E}[Y_i(w)]\\) for all \\(w \\in \\{0, 1\\}\\).\nTherefore, the difference between \\(Y_i(1)\\) and \\(Y_i(0)\\) is the ATE:\n\\[\n\\mathbb{E}[Y_i(1) - Y_i(0)] = \\mathbb{E}[Y_i(1)] - \\mathbb{E}[Y_i(0)] = \\tau.\n\\]"
  },
  {
    "objectID": "unit-0/lec-0.html#experimental-problems-ep1-ep2",
    "href": "unit-0/lec-0.html#experimental-problems-ep1-ep2",
    "title": "Course Introduction",
    "section": "Experimental Problems: EP1 & EP2",
    "text": "Experimental Problems: EP1 & EP2\nEP1: Internal Validity\n\nThe Effects of Causes (Internal Validity):\n\nDo assumptions in the potential outcomes framework hold?\n\nThe Causes of Effects (Mediators and Moderators):\n\nWhat drives the effects we see?\n\n\nEP2: External Validity\n\nGeneralizability: Would it work in different settings?\n\nPeople / populations\nContexts\n\nScalability: Can it scale to create meaningful impact?"
  },
  {
    "objectID": "unit-0/lec-0.html#data-generation-and-modeling-for-causal-inference",
    "href": "unit-0/lec-0.html#data-generation-and-modeling-for-causal-inference",
    "title": "Course Introduction",
    "section": "Data Generation and Modeling for Causal Inference",
    "text": "Data Generation and Modeling for Causal Inference\n\n\n\nControlled: researcher knows and controls the assignment mechanism\nUncontrolled: researcher assignment mechanism neither controls nor knows the assignment mechanism\n\n\n\n\nSource: List (2009)\n\n\n\n\nLab Experiments: Controlled settings, abstract framing, imposed rules.\nField Experiments:\n\nArtefactual Field Experiment (AFE): Non-standard subject pool.\nFramed Field Experiment (FFE): Adds field-specific context.\nNatural Field Experiment (NFE): Subjects unaware they are in an experiment.\n\nSurvey Experiments:\n\nEmbedded in survey designs."
  },
  {
    "objectID": "unit-0/lec-0.html#criteria-that-define-field-experiments",
    "href": "unit-0/lec-0.html#criteria-that-define-field-experiments",
    "title": "Course Introduction",
    "section": "7 Criteria that define field experiments",
    "text": "7 Criteria that define field experiments\n\nExperimental Subjects: Population and Selection\n\nThe nature of the subject pool\nThe nature of the information that subjects bring to the experimental task\nSelection into the experiment\n\nExperimental Environment\n\nThe nature of the commodity in the experiment\nThe nature of the experimental task\nThe nature of the stakes\nExperimental proclamation"
  },
  {
    "objectID": "unit-0/lec-0.html#choosing-the-right-experiment",
    "href": "unit-0/lec-0.html#choosing-the-right-experiment",
    "title": "Course Introduction",
    "section": "Choosing the Right Experiment",
    "text": "Choosing the Right Experiment\n\nWeigh Costs and Benefits:\n\nBenefits:\n\nEP1: Internal validity.\nEP2: Generalizability and scalability.\n\nCosts:\n\nMonetary\nLogistical\nOpportunity costs."
  },
  {
    "objectID": "unit-0/lec-0.html#units-and-topics",
    "href": "unit-0/lec-0.html#units-and-topics",
    "title": "Course Introduction",
    "section": "Units and Topics",
    "text": "Units and Topics\n\nFoundations of Causal Inference:\n\nMissing data problem and potential outcomes.\nRandomization and the ATE.\n\nExperimental Design:\n\nPower analysis and randomization strategies.\n\nMachine Learning for Causal Inference:\n\nLasso, random forests, and causal forests.\n\nViolations of Internal Validity:\n\nSUTVA, compliance issues, and observability.\n\nScaling and External Validity:\n\nGeneralizability and implementation challenges."
  },
  {
    "objectID": "unit-0/lec-0.html#weekly-ish-format",
    "href": "unit-0/lec-0.html#weekly-ish-format",
    "title": "Course Introduction",
    "section": "Weekly-ish Format",
    "text": "Weekly-ish Format\n\nTwo(-ish) sessions per topic:\n\nLecture: Introduces key concepts and theory.\nLab: Hands-on practice with coding and data analysis.\n\nFlipped(-ish) Classroom Approach\n\nPre-class readings and recorded materials.\nInteractive sessions focused on data exploration and applied learning.\nCollaborative problem-solving in small groups."
  },
  {
    "objectID": "unit-0/lec-0.html#assessments",
    "href": "unit-0/lec-0.html#assessments",
    "title": "Course Introduction",
    "section": "Assessments",
    "text": "Assessments\n\nPre-class Quizzes (10%):\n\nTest your understanding of key concepts\nRequest clarification on specific points\n\nLab Assignments (25%): Practical applications and analysis.\nExams (35%): Evaluate theoretical and practical understanding.\nSemester Project (30%): Research application.\n\nPre-analysis Plan (15%): Develop and document a structured analysis plan.\nFinal Paper (15%): Synthesize course material into a research application.\nDetails on class project to come."
  },
  {
    "objectID": "unit-0/lec-0.html#resources-and-support",
    "href": "unit-0/lec-0.html#resources-and-support",
    "title": "Course Introduction",
    "section": "Resources and Support",
    "text": "Resources and Support\n\nCourse website with materials and updates: Course Website\nWeekly office hours with Dr. Sylvia and Yumeng\nCommunication channels:\n\nSlack: Join the course Slack to ask questions, share resources, and engage with your peers, the instructor, and the TA. (For personal matters, please use e-mail)\nEmail: Please use e-mail only for personal matters. (For anything related to the course material or coding questions, please use Slack.)\n\nGradescope: Submit assignments."
  },
  {
    "objectID": "lab.html",
    "href": "lab.html",
    "title": "Labs",
    "section": "",
    "text": "More labs to be added as the semester progresses.\n\n\n\n\n\n\n\n\n\nNo.\n\n\nTitle\n\n\nDue date\n\n\n\n\n\n\nLab 1\n\n\nThe Hospital of Uncertain Outcomes\n\n\nInternal Validity, Potential Outcomes\n\n\n\n\nLab 1 Solutions\n\n\nThe Hospital of Uncertain Outcomes\n\n\nInternal Validity, Potential Outcomes\n\n\n\n\nLab 2\n\n\nPower by Simulation\n\n\nConducting power calculations by simulation, first without clusters and then with clusters.\n\n\n\n\nLab 2: Power by Simulation\n\n\n(Possible)Solutions\n\n\nConducting power calculations by simulation, first without clusters and then with clusters.\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "unit-1/unit-1-internal-1.html",
    "href": "unit-1/unit-1-internal-1.html",
    "title": "Unit 1.1: Internal Validity and Potential Outcomes",
    "section": "",
    "text": "Reviewws the Potential Outcomes Framework\nAddresses key aspects of experimental design, including:\n\nRandomization\nExclusion restrictions\n\nThese elements are essential for establishing causality.\nWe will also review the notation that will be used throughout the class.\n\n\n\n\nRead:\n\nApplied Causal Inference Powered by ML and AI, Chapter 2\nCausal Inference Mixtape, Chapter 4\n\nListen: Internal Validity Podcast\n\n\n\n\n\nGroup Assignments\n\n\n\n\nGroup Assignments\n\n\n\nSelected Additional Topics via Borda count:\n\n\n\n\nTopic Selection\n\n\n\nSemester Project Description: Semester Project\nLecture: Internal Validity\n\n\n\n\n\nLab 1: The Hospital of Uncertain Outcomes\n\nSolutions",
    "crumbs": [
      "Unit 1: Internal Validity",
      "Unit 1.0: Internal Validity"
    ]
  },
  {
    "objectID": "unit-1/unit-1-internal-1.html#unit-overview",
    "href": "unit-1/unit-1-internal-1.html#unit-overview",
    "title": "Unit 1.1: Internal Validity and Potential Outcomes",
    "section": "",
    "text": "Reviewws the Potential Outcomes Framework\nAddresses key aspects of experimental design, including:\n\nRandomization\nExclusion restrictions\n\nThese elements are essential for establishing causality.\nWe will also review the notation that will be used throughout the class.\n\n\n\n\nRead:\n\nApplied Causal Inference Powered by ML and AI, Chapter 2\nCausal Inference Mixtape, Chapter 4\n\nListen: Internal Validity Podcast\n\n\n\n\n\nGroup Assignments\n\n\n\n\nGroup Assignments\n\n\n\nSelected Additional Topics via Borda count:\n\n\n\n\nTopic Selection\n\n\n\nSemester Project Description: Semester Project\nLecture: Internal Validity\n\n\n\n\n\nLab 1: The Hospital of Uncertain Outcomes\n\nSolutions",
    "crumbs": [
      "Unit 1: Internal Validity",
      "Unit 1.0: Internal Validity"
    ]
  },
  {
    "objectID": "unit-1/unit-1-internal-2.html",
    "href": "unit-1/unit-1-internal-2.html",
    "title": "Unit 1.2: Statistical Conclusion Validity",
    "section": "",
    "text": "Introduce statistical conclusion validity\nExplore sources of uncertainty in experiments\nIntroduce hypothesis testing\nExamine the role of statistical power in determining sample size\nUse simulation techniques for power calculations and design assessments\nDiscuss multiple hypothesis testing and its implications\n\n\n\n\nRead:\n\nWager Chapter 1\nDeclare Design Book: 5.1, 8.2, 10\n\nListen:\n\nExperimental Design and Multiple Hypothesis Testing Podcast\n\n\n\n\n\n\nLecture: Statistical Conclusion Validity\n\n\n\n\n\nLab 2: Power Calculation by Simulation\n\nDue by 11:59pm on Monday, February 17\nSubmit on Gradescope\nSolutions: lab-2-Power-sols.qmd",
    "crumbs": [
      "Unit 1: Internal Validity",
      "Unit 1.1: Statistical Conclusion Validity"
    ]
  },
  {
    "objectID": "unit-1/unit-1-internal-2.html#unit-overview",
    "href": "unit-1/unit-1-internal-2.html#unit-overview",
    "title": "Unit 1.2: Statistical Conclusion Validity",
    "section": "",
    "text": "Introduce statistical conclusion validity\nExplore sources of uncertainty in experiments\nIntroduce hypothesis testing\nExamine the role of statistical power in determining sample size\nUse simulation techniques for power calculations and design assessments\nDiscuss multiple hypothesis testing and its implications\n\n\n\n\nRead:\n\nWager Chapter 1\nDeclare Design Book: 5.1, 8.2, 10\n\nListen:\n\nExperimental Design and Multiple Hypothesis Testing Podcast\n\n\n\n\n\n\nLecture: Statistical Conclusion Validity\n\n\n\n\n\nLab 2: Power Calculation by Simulation\n\nDue by 11:59pm on Monday, February 17\nSubmit on Gradescope\nSolutions: lab-2-Power-sols.qmd",
    "crumbs": [
      "Unit 1: Internal Validity",
      "Unit 1.1: Statistical Conclusion Validity"
    ]
  },
  {
    "objectID": "unit-1/lec-1-2.html",
    "href": "unit-1/lec-1-2.html",
    "title": "Unit 1-2: Statistical Conclusion Validity",
    "section": "",
    "text": "This lecture delves into statistical conclusion validity, exploring how we can assess the reliability and accuracy of estimated treatment effects in experimental research. As we have seen, due to the fundamental nature of potential outcomes, it is inherently impossible to recover individual treatment effects since each unit reveals only one potential outcome. However, under specific assumptions and with an appropriate assignment mechanism—such as random assignment—we can consistently estimate an average treatment effect (ATE). This lecture provides a brief, selective statistical background on how to estimate the ATE and quantify the uncertainty in these estimates, ensuring statistical conclusion validity. To streamline the presentation, we focus on settings with a binary treatment and largely ignore the role of covariates."
  },
  {
    "objectID": "unit-1/lec-1-2.html#what-is-uncertainty",
    "href": "unit-1/lec-1-2.html#what-is-uncertainty",
    "title": "Unit 1-2: Statistical Conclusion Validity",
    "section": "What is Uncertainty?",
    "text": "What is Uncertainty?\nUncertainty in empirical research refers to the inherent imprecision that arises when we attempt to infer quantities that cannot be directly observed. Whether we are engaged in descriptive, causal, or generalization inference, our estimates come with uncertainty that must be quantified and communicated.\nTwo primary frameworks exist for this purpose: the Bayesian and the frequentist approaches.1\nBayesian Approach\nThe Bayesian framework uses Bayes’ rule to combine prior beliefs with the observed data, resulting in a posterior probability distribution over the parameter of interest, \\(\\theta\\). Mathematically, this is expressed as:\n\\[\n\\Pr(\\theta = \\theta' \\mid d = d') = \\frac{\\Pr(d = d' \\mid \\theta = \\theta')\\, \\Pr(\\theta = \\theta')}{\\sum_{\\theta''} \\Pr(d = d' \\mid \\theta = \\theta'')\\, \\Pr(\\theta = \\theta'')},\n\\]\nwhere:\n\n\\(d\\) represents data, and \\(d'\\) represents the observed data (or an “observed realization of the data”)\n\\(\\theta'\\) and \\(\\theta''\\) represent particular values of the parameter \\(\\theta\\).\n\nFrom this posterior distribution, the posterior mean serves as our best estimate of \\(\\theta\\), and the posterior variance quantifies the uncertainty associated with that estimate.\nBy applying Bayes’ rule over different values of \\(\\theta\\), we construct a complete probability distribution that represents all possible answers. This posterior distribution simultaneously provides our best estimate—often summarized by the posterior mean—and quantifies our uncertainty via the posterior variance.\nWhile intuitive, a challenge is that specifying prior uncertainty (\\(\\Pr(\\theta = \\theta')\\)) is often a subjective choice, and the posterior distribution is often difficult to interpret and communicate.\nFrequentist Approach\nIn contrast, the frequentist approach avoids specifying prior beliefs and focuses on the likelihood function, \\(\\Pr(d = d' \\mid \\theta = \\theta')\\), which describes the probability of observing the data \\(d'\\) given a specific value of \\(\\theta\\).2 I.e. instead of thinking of the strength of beliefs, we consider that \\(\\theta\\) generates the actual probability distriubtion over possible data \\(d\\).\nThis approach yields useful quantities:\nP-value: The p-value for a null hypothesis, \\(\\theta = \\theta_0\\), is defined as the probability of observing data as extreme as \\(d_{m*}\\) under the null hypothesis, or\n\\[\n\\Pr(d = d_{m*} \\mid \\theta = \\theta_0).\n\\]\nwhere \\(d_{m*}\\) is the test statistic.\nConfidence Interval: A 95% confidence interval is constructed such that, if the experiment were repeated many times, 95% of the intervals would contain the true parameter value. This approach provides a framework to rule out parameter values that are inconsistent with the observed data, or \\(Pr(d = d' \\mid \\theta = \\theta') \\leq 0.05\\)."
  },
  {
    "objectID": "unit-1/lec-1-2.html#where-does-uncertainty-come-from-in-an-experimental-study",
    "href": "unit-1/lec-1-2.html#where-does-uncertainty-come-from-in-an-experimental-study",
    "title": "Unit 1-2: Statistical Conclusion Validity",
    "section": "Where Does Uncertainty Come From in an Experimental Study?",
    "text": "Where Does Uncertainty Come From in an Experimental Study?\nBefore getting into estimation and the uncertainty of the estimate, we need to be precise about the source of uncertainty. In experimental studies, uncertainty is an inherent part of the inference process, arising from several key sources. Recognizing these sources is critical to designing robust experiments and correctly interpreting the results. The main contributors to uncertainty include:\n\nSampling Variation:\nUncertainty due to sampling variation stems from the fact that any sample drawn from a population is just one of many possible samples. Consequently, the same treatment might yield different results if applied to a different sample, reflecting random fluctuations in the selection process.\nVariance in Potential Outcomes:\nThe natural variability in the potential outcomes (i.e., the outcomes that would be observed under different treatment conditions) can lead to uncertainty. High variance makes it more challenging to detect a true treatment effect because the noise in the data can obscure the signal, thereby reducing the study’s power to reject a false null hypothesis.\nMeasurement Error:\nMeasurement error occurs when there are inaccuracies in recording or assessing the potential outcomes. Such errors introduce additional variability and can bias the estimated treatment effect, further contributing to uncertainty in the experimental results.\n\nTo vizualize this, the diagram below shows a “directed acyclic graph” (DAG) representation of the “data strategy” framework discussed in Chapter 8 of Research Design in the Social Sciences.\n\n\nData Strategy DAG, Source: Research Design in the Social Sciences\n\n\nIn Figure 8.1, we illustrate these three elements of data strategies: sampling (S), treatment assignment (Z), and measurement (Q). These nodes are highlighted by blue boxes to emphasize that they are in the control of the researcher. No arrows go into the S, Z, or Q nodes; they are set by the researcher. In each case, the strategy selected by the researcher affects a corresponding endogenous variable. The sampling procedure causes changes in the endogenous response (R), which represents whether participants provide outcome data, for example responding to survey questions. R is not under the full control of the researchers: it is affected by S, the sampling procedure, but also by the idiosyncratic choices of participants who have higher and lower interest and ability to respond and participate in the study (U). Similarly, the endogenous variable treatment D represents whether participants actually receive the treatment, regardless of their assignment Z. D is affected by the treatment assignment procedure (Z) of course. But except in cases when Z fully determines D (no noncompliance), we are concerned that it will be affected by unobserved idiosyncratic features of individuals U. The third researcher node is Q, the measurement procedure. Q affects Y, the observed outcome, measured by the researcher. Y is also affected by a latent variable Y*, which cannot be directly observed. The measurement procedure provides an imperfect measurement of that latent variable, which is (potentially) affected by treatment D and unobserved heterogeneity U. In the robustness section at the end of the chapter, we explore further variations of this DAG that incorporate threats to inference from noncompliance, attrition, excludability violations, and interference.\n\n\n\n\n\n\n\nTip\n\n\n\nCan you draw four arrows representing the four exclusion restrictions?"
  },
  {
    "objectID": "unit-1/lec-1-2.html#statistical-conclusion-validity",
    "href": "unit-1/lec-1-2.html#statistical-conclusion-validity",
    "title": "Unit 1-2: Statistical Conclusion Validity",
    "section": "Statistical Conclusion Validity",
    "text": "Statistical Conclusion Validity\nGiven that our exclusion restrictions are satisfied, we can estimate the average treatment effect (ATE). The question is then: How can we ensure valid statistical conclusions from our estimate of the ATE?\nWe need to consider the uncertainty in our estimate, and whether we can reject the null hypothesis that the treatment has no effect."
  },
  {
    "objectID": "unit-1/lec-1-2.html#the-super-population-approach",
    "href": "unit-1/lec-1-2.html#the-super-population-approach",
    "title": "Unit 1-2: Statistical Conclusion Validity",
    "section": "The Super-Population Approach",
    "text": "The Super-Population Approach\nThe super-population approach assumes that the study sample is drawn from a larger, hypothetical infinite population represented by a probability distribution, \\(Q\\). This perspective views potential outcomes—both with and without treatment—as stochastic variables drawn from \\(Q\\). The primary goal in this framework is to estimate a feature of this distribution, typically the expected treatment effect:\n\\[\nE[Y(1) - Y(0)]\n\\]\nwhere \\(Y(1)\\) and \\(Y(0)\\) denote the potential outcomes under treatment and control conditions, respectively. Under this approach, each sample is considered an independent and identically distributed (i.i.d.) draw from the distribution \\(Q\\), meaning the researcher is interested in making generalizable inferences beyond the study sample.\nA key implication of this framework is that two sources of variance affect our estimation of treatment effects: 1. Sampling variance—arising from differences between one sample and another. 2. Assignment mechanism variance—introduced by the randomness in treatment assignment.\nThe super-population approach is useful when researchers aim to extend their findings to a broader population, such as in policy recommendations or clinical trials. However, it requires strong assumptions about how well the study sample represents the population."
  },
  {
    "objectID": "unit-1/lec-1-2.html#the-finite-population-approach",
    "href": "unit-1/lec-1-2.html#the-finite-population-approach",
    "title": "Unit 1-2: Statistical Conclusion Validity",
    "section": "The Finite-Population Approach",
    "text": "The Finite-Population Approach\nIn contrast, the finite-population approach considers the sample as a fixed, well-defined group rather than a subset of an infinite population. Here, the researcher is not making inferences beyond the observed sample but instead treating the units as the entire relevant population. This approach is common in evaluations of specific interventions where the focus is on estimating the finite-population average treatment effect (ATE) (also called the sample average treatemnt effect, or SATE):3\n\\[\n\\tau_{fp} = \\frac{1}{N} \\sum_{i=1}^{N} \\left[Y_i(1) - Y_i(0)\\right]\n\\]\nwhere \\(N\\) is the total number of units in the study. Unlike in the super-population approach, the potential outcomes in a finite population framework are fixed, not random. The treatment effect is then viewed as an empirical quantity to be estimated within the sample, rather than a parameter of an underlying distribution.\nA practical distinction between the two approaches is in their implications for statistical inference:\n\nIn the super-population approach, standard errors reflect both sampling variability and randomization-induced variation.\nIn the finite-population approach, standard errors are based only on the variation within the observed sample, without assuming a broader distribution.\n\nThis framework is particularly relevant when researchers are concerned with internal validity over generalizability, such as in program evaluations or field experiments."
  },
  {
    "objectID": "unit-1/lec-1-2.html#subpopulations-in-the-super-population-framework",
    "href": "unit-1/lec-1-2.html#subpopulations-in-the-super-population-framework",
    "title": "Unit 1-2: Statistical Conclusion Validity",
    "section": "Subpopulations in the Super-Population Framework",
    "text": "Subpopulations in the Super-Population Framework\nWithin the super-population framework, researchers often refine their analysis by considering subpopulations to account for heterogeneous treatment effects. One important example is the Conditional Average Treatment Effect (CATE):\n\\[\nE[Y(1, X) - Y(0, X) | X]\n\\]\nwhere \\(X\\) represents observed covariates that influence treatment effects. This approach allows for differentiated insights across groups, such as demographic segments in public health interventions."
  },
  {
    "objectID": "unit-1/lec-1-2.html#choosing-between-the-two-approaches",
    "href": "unit-1/lec-1-2.html#choosing-between-the-two-approaches",
    "title": "Unit 1-2: Statistical Conclusion Validity",
    "section": "Choosing Between the Two Approaches",
    "text": "Choosing Between the Two Approaches\nThe choice between these sampling frameworks depends on the research question:\n\nIf the goal is to make generalizable claims about a broader population, the super-population approach is preferred.\nIf the study focuses on a specific, finite group of units, the finite-population approach is more appropriate.\n\nBoth perspectives provide valuable insights, and many empirical studies incorporate elements of both frameworks, particularly when considering external validity and policy relevance."
  },
  {
    "objectID": "unit-1/lec-1-2.html#statistical-significance-and-the-t-statistic",
    "href": "unit-1/lec-1-2.html#statistical-significance-and-the-t-statistic",
    "title": "Unit 1-2: Statistical Conclusion Validity",
    "section": "Statistical Significance and the t-Statistic",
    "text": "Statistical Significance and the t-Statistic\nTo formally test the null hypothesis, we construct a t-statistic:\n\\[\nt = \\frac{\\hat{\\tau}}{\\text{SE}(\\hat{\\tau})}\n\\]\nwhere SE(\\(\\hat{\\tau}\\)) represents the standard error of the difference-in-means estimator. As the sample size grows, this t-statistic follows a standard normal distribution (or Student’s t-distribution for small samples). A large absolute value of $ t $ provides evidence against the null hypothesis.\nTo determine whether the result is statistically significant, we compare the t-statistic to a critical value determined by our chosen significance level (\\(\\alpha\\), commonly set at 0.05). If the absolute value of the t-statistic exceeds this threshold, we reject the null hypothesis."
  },
  {
    "objectID": "unit-1/lec-1-2.html#type-i-and-type-ii-errors",
    "href": "unit-1/lec-1-2.html#type-i-and-type-ii-errors",
    "title": "Unit 1-2: Statistical Conclusion Validity",
    "section": "Type I and Type II Errors",
    "text": "Type I and Type II Errors\nWhile hypothesis testing provides a structured approach to evaluating treatment effects, errors can still occur:\n\n\nType I Error (\\(\\alpha\\)): Rejecting the null hypothesis when it is actually true (false positive).\n\nControlled by setting the significance level (\\(\\alpha\\)), which determines the probability of mistakenly rejecting \\(H_0\\).\nLower \\(\\alpha\\) reduces false positives but increases the risk of missing real effects.\n\n\n\nType II Error (\\(\\beta\\)): Failing to reject the null hypothesis when it is actually false (false negative).\n\nRelated to statistical power, which is the probability of detecting an effect when it truly exists.\n\n\n\n\n\nType 1 and Type 2 Errors"
  },
  {
    "objectID": "unit-1/lec-1-2.html#power-calculation-ensuring-detectability",
    "href": "unit-1/lec-1-2.html#power-calculation-ensuring-detectability",
    "title": "Unit 1-2: Statistical Conclusion Validity",
    "section": "Power Calculation: Ensuring Detectability",
    "text": "Power Calculation: Ensuring Detectability\nStatistical power refers to the ability of a test to correctly reject the null hypothesis when a true effect exists. Mathematically:\n\\[\n\\text{Power} = 1 - \\beta\n\\]\nFactors influencing power:\n\n\nEffect size (\\(\\tau\\)): Larger effects are easier to detect.\n\nSample size (\\(N\\)): Larger samples reduce variability, increasing power.\n\nSignificance level (\\(\\alpha\\)): Lowering \\(\\alpha\\) increases the risk of missing true effects.\n\nStandard deviation of outcomes: Higher variability in outcomes reduces power.\n\nTo achieve a well-powered experiment, researchers conduct power calculations before data collection to determine the minimum sample size required to detect an effect with reasonable confidence.\nA Simple Example\nLet’s consider a simple example of a power simulation using a simple random assignment to a treatment and control group, where the estimate is the difference-in-means estimator.\n\nlibrary(data.table)\n\nStep 1: Define Simulation Parameters\n\nLet’s assume we have a total sample size of \\(N = 200\\) individuals.\nThe treatment group receives an intervention, while the control group does not.\nThe true treatment effect is set to \\(\\tau = 2\\).\nThe outcome variable follows a normal distribution with mean 10 and standard deviation 4.\n\n\nset.seed(072111)  # Ensures reproducibility\n\n# Define parameters\nN &lt;- 200  # Total sample size\np &lt;- 0.5  # Probability of assignment to treatment\ntrue_tau &lt;- 2  # True treatment effect\nsigma &lt;- 4  # Standard deviation of outcome\n\nStep 2: Simulate Data\nNow we can simulate the data using data.table:\n\n# Simulate data using data.table\ndt &lt;- data.table(id = 1:N)\ndt[, treatment := rbinom(.N, 1, p)]\ndt[, outcome := 10 + true_tau * treatment + rnorm(.N, mean = 0, sd = sigma)]\n\n#display first few rows\nhead(dt)\n\n      id treatment  outcome\n   &lt;int&gt;     &lt;int&gt;    &lt;num&gt;\n1:     1         0 10.39758\n2:     2         1 17.78058\n3:     3         0 11.46925\n4:     4         0 14.56752\n5:     5         1 10.55347\n6:     6         1 14.99933\n\n\nStep 3: Estimate Treatment Effect\n\ndiff_means &lt;- dt[treatment == 1, mean(outcome)] - dt[treatment == 0, mean(outcome)]\nSE &lt;- sqrt(dt[treatment == 1, var(outcome)] / dt[treatment == 1, .N] +\n           dt[treatment == 0, var(outcome)] / dt[treatment == 0, .N])\n\nt_stat &lt;- diff_means / SE  # Compute t-statistic\np_value &lt;- 2 * (1 - pt(abs(t_stat), df = N - 2))  # Two-tailed test\n\n# Display results\ncat(\"Estimated Treatment Effect:\", diff_means, \"\\n\")\n\nEstimated Treatment Effect: 2.453868 \n\ncat(\"p-value:\", p_value, \"\\n\")\n\np-value: 1.088986e-05 \n\n\nStep 4: Power Simulation\nFirst, let’s consider how we will interpret the results:\n\nIf the p-value is less than 0.05, we reject the null hypothesis and conclude that the treatment has a significant effect.\nIf the p-value is greater than 0.05, we fail to reject the null, meaning we do not have enough evidence to confirm a treatment effect.\n\nNow, let’s simulate a power simulation:\nFirst, define the power simulation function:\n\nsimulate_power &lt;- function(N, true_tau, sigma, p, alpha, reps = 1000) {\n  rejections &lt;- 0\n  \n  for (i in 1:reps) {\n    dt &lt;- data.table(id = 1:N)\n    dt[, treatment := rbinom(.N, 1, p)]\n    dt[, outcome := 10 + true_tau * treatment + rnorm(.N, mean = 0, sd = sigma)]\n    diff_means &lt;- dt[treatment == 1, mean(outcome)] - dt[treatment == 0, mean(outcome)]\n    SE &lt;- sqrt(dt[treatment == 1, var(outcome)] / dt[treatment == 1, .N] +\n               dt[treatment == 0, var(outcome)] / dt[treatment == 0, .N])\n    t_stat &lt;- diff_means / SE\n    p_value &lt;- 2 * (1 - pt(abs(t_stat), df = N - 2))\n    \n    if (p_value &lt; alpha) {\n      rejections &lt;- rejections + 1\n    }\n  }\n  return(rejections / reps)\n}\n\nNow, simulate this experiment 1,000 times:\n\n# Run power simulation\npower &lt;- simulate_power(N = 200, true_tau = 2, sigma = 4, p = 0.5, alpha = 0.05, reps = 1000)\ncat(\"Estimated Power:\", power, \"\\n\")\n\nEstimated Power: 0.938 \n\n\nStep 5: Interpreting Power Calculation\n\nThe power of the test is the proportion of simulations in which we correctly reject the null hypothesis when the treatment effect is truly \\(\\tau = 2\\).\nA power value close to 0.80 or higher indicates that the experiment is well-powered.\n\n\n\n\n\n\n\nTip\n\n\n\nTry changing the sample size or effect size, to see how the power changes."
  },
  {
    "objectID": "unit-1/lec-1-2.html#the-mida-framework-for-simulation",
    "href": "unit-1/lec-1-2.html#the-mida-framework-for-simulation",
    "title": "Unit 1-2: Statistical Conclusion Validity",
    "section": "The MIDA Framework for Simulation",
    "text": "The MIDA Framework for Simulation\nA structured way to think about research design and simulation is the MIDA Framework,4(https://book.declaredesign.org/declaration-diagnosis-redesign/research-design.html)] which consists of:\n\n\nM: The Model - The underlying data-generating process that defines the inquiry.\n\nI: The Inquiry - The specific research question we are trying to answer.\n\nD: The Data Strategy - The way we collect and structure data, including sampling and treatment assignment.\n\nA: The Answer Strategy - The statistical method we use to estimate the effect.\n\n\n\nMIDA Framework, Source: Declaration Design\n\n\n\nElements of Research Design, Source: Declaration Design\n\nIn a real-world study, we can only observe a single realization of the design and generate one empirical answer \\(a_d\\). However, through simulation, we can consider many possible data realizations by repeatedly drawing from different models \\(m_1, m_2, ..., m_k\\) within the model space \\(M\\). This allows us to assess how our research design performs across different scenarios."
  },
  {
    "objectID": "unit-1/lec-1-2.html#diagnosing-research-designs-with-simulation",
    "href": "unit-1/lec-1-2.html#diagnosing-research-designs-with-simulation",
    "title": "Unit 1-2: Statistical Conclusion Validity",
    "section": "Diagnosing Research Designs with Simulation",
    "text": "Diagnosing Research Designs with Simulation\nWhen we simulate a research design, we evaluate its performance by considering:\n\n\nBias: How close is the empirical answer \\(a_d\\) to the true answer \\(a_m\\)?\n\nVariance: How much do the empirical answers fluctuate across different realizations?\n\nCoverage: How often do confidence intervals include the true effect?\n\nPower: How frequently does the study correctly reject the null hypothesis when a true effect exists?\n\nThe bottom half of the figure below illustrates how simulation allows us to examine the research design across multiple models (\\(m_1, ..., m_k\\)), generating different answers (\\(a_{m_1}, a_{m_2}, ..., a_{m_k}\\)) and associated datasets (\\(d_1, d_2, ..., d_k\\)). The simulated research design does not have direct access to the true answer but can assess performance across the models under consideration.\n\n\nSimulations in the MIDA Framework"
  },
  {
    "objectID": "unit-1/lec-1-2.html#the-challenge-of-multiple-hypothesis-testing",
    "href": "unit-1/lec-1-2.html#the-challenge-of-multiple-hypothesis-testing",
    "title": "Unit 1-2: Statistical Conclusion Validity",
    "section": "The Challenge of Multiple Hypothesis Testing",
    "text": "The Challenge of Multiple Hypothesis Testing\nIn many empirical settings, researchers conduct multiple hypothesis tests rather than a single test. This introduces the risk of false positives, or mistakenly finding significant effects simply due to chance. Multiple hypothesis testing arises naturally in at least three key scenarios:\n\n\nMultiple Outcomes: When we examine several outcomes (\\(Y_i\\)) to determine whether any are affected by the treatment.\n\nHeterogeneous Treatment Effects (CATEs): When treatment effects vary across subgroups, and we want to assess which subgroups exhibit an effect.\n\nMultiple Treatments: When we compare multiple interventions (\\(D_i\\)) and want to test their effects relative to a control group or to each other.\n\nTo properly interpret results, we need statistical techniques that adjust for multiple comparisons and control the probability of false discoveries."
  },
  {
    "objectID": "unit-1/lec-1-2.html#types-of-multiple-hypothesis-tests",
    "href": "unit-1/lec-1-2.html#types-of-multiple-hypothesis-tests",
    "title": "Unit 1-2: Statistical Conclusion Validity",
    "section": "Types of Multiple Hypothesis Tests",
    "text": "Types of Multiple Hypothesis Tests\nThere are two broad types of statistical hypothesis testing frameworks when dealing with multiple comparisons:\n\n\nJoint Tests: These assess whether at least one hypothesis is true (e.g., “Is at least one treatment effective?”).\n\nSimultaneous Tests: These examine whether multiple hypotheses are true at the same time (e.g., “Are both Treatment A and Treatment B effective?”).\n\nWhen conducting multiple tests, researchers need to control for an increased family-wise error rate (FWER), which is the probability of making at least one Type I error (false positive)."
  },
  {
    "objectID": "unit-1/lec-1-2.html#controlling-the-family-wise-error-rate-fwer",
    "href": "unit-1/lec-1-2.html#controlling-the-family-wise-error-rate-fwer",
    "title": "Unit 1-2: Statistical Conclusion Validity",
    "section": "Controlling the Family-Wise Error Rate (FWER)",
    "text": "Controlling the Family-Wise Error Rate (FWER)\nA single hypothesis test controls the probability of a Type I error at a given significance level (\\(\\alpha\\)). However, with multiple tests, the probability of making at least one false rejection increases:\n\\[\n\\text{FWER} = 1 - (1 - \\alpha)^k\n\\]\nwhere \\(k\\) is the number of tests. For example, if we conduct 5 tests at \\(\\alpha = 0.05\\), the probability of making no Type I errors across all tests is:\n\\[\n(1 - 0.05)^5 = 0.7738\n\\]\nThus, the probability of making at least one Type I error is:\n\\[\nFWER = 1 - 0.7738 = 0.2262\n\\]\nThis means there is a 22.62% chance of mistakenly rejecting at least one null hypothesis across the five tests. If we perform 20 tests, the FWER increases to 64%."
  },
  {
    "objectID": "unit-1/lec-1-2.html#approaches-to-controlling-the-fwer",
    "href": "unit-1/lec-1-2.html#approaches-to-controlling-the-fwer",
    "title": "Unit 1-2: Statistical Conclusion Validity",
    "section": "Approaches to Controlling the FWER",
    "text": "Approaches to Controlling the FWER\nTo mitigate this issue, researchers employ various multiple testing corrections, such as:\n\n\nBonferroni Correction: Adjusts the significance level by dividing \\(\\alpha\\) by the number of tests: \\(\\alpha^* = \\alpha / k\\). This is simple but conservative.\n\nHolm Method: A stepwise procedure that ranks p-values and adjusts them sequentially to control the FWER more efficiently.\n\nModern Approaches (e.g., Westfall-Young, Benjamini-Hochberg FDR control): These methods control for false discovery rates and are widely used in large-scale testing."
  },
  {
    "objectID": "unit-1/lec-1-2.html#simulating-multiple-hypothesis-testing-in-r",
    "href": "unit-1/lec-1-2.html#simulating-multiple-hypothesis-testing-in-r",
    "title": "Unit 1-2: Statistical Conclusion Validity",
    "section": "Simulating Multiple Hypothesis Testing in R",
    "text": "Simulating Multiple Hypothesis Testing in R\nTo illustrate the impact of multiple testing and FWER correction, let’s do a simulation in R:\n\nlibrary(data.table)\nset.seed(072111)  # Ensures reproducibility\n\n# Define parameters\nN &lt;- 200  # Sample size\nk &lt;- 10  # Number of hypothesis tests\nalpha &lt;- 0.05  # Significance level\n\n# Simulate k independent hypothesis tests\ndt &lt;- data.table(test_id = 1:k)\ndt[, p_value := runif(.N, min = 0, max = 1)]  # Generate uniform random p-values\n\ndt[, bonferroni := p_value &lt; (alpha / k)]  # Bonferroni correction\ndt[, holm := p.adjust(p_value, method = \"holm\") &lt; alpha]  # Holm correction\n\ndt[, naive_reject := p_value &lt; alpha]  # Standard test (without correction)\n\n# Count false positives\nfalse_discoveries &lt;- dt[, sum(naive_reject)]\nadjusted_false_discoveries &lt;- dt[, sum(bonferroni)]\nholm_false_discoveries &lt;- dt[, sum(holm)]\n\ncat(\"False positives (no correction):\", false_discoveries, \"\\n\")\n\nFalse positives (no correction): 2 \n\ncat(\"False positives (Bonferroni correction):\", adjusted_false_discoveries, \"\\n\")\n\nFalse positives (Bonferroni correction): 0 \n\ncat(\"False positives (Holm correction):\", holm_false_discoveries, \"\\n\")\n\nFalse positives (Holm correction): 0 \n\n\nInterpreting the Simulation\n\n\nWithout correction, we expect around \\(\\alpha \\times k\\) false discoveries.\n\nBonferroni correction sharply reduces false positives but may be overly conservative.\n\nHolm correction provides a better balance, controlling FWER while maintaining statistical power.\n\n\n\n\n\n\n\nTip\n\n\n\nNote: In the lab, we’ll cover some modern approaches that are more powerful but a bit more complex."
  },
  {
    "objectID": "unit-1/lec-1-2.html#footnotes",
    "href": "unit-1/lec-1-2.html#footnotes",
    "title": "Unit 1-2: Statistical Conclusion Validity",
    "section": "Footnotes",
    "text": "Footnotes\n\nReference: Research Design in the Social Sciences↩︎\nCan you explain in words how this differs from the Bayesian probability above?↩︎\nWager Chapter 1 refers to this as SATE.↩︎\nReference: Declaration Design↩︎"
  },
  {
    "objectID": "template-pap.html",
    "href": "template-pap.html",
    "title": "Pre-Analysis Plan Template",
    "section": "",
    "text": "This is a template for a Pre-Analysis Plan (PAP), roughly following guidelines from the American Economic Association (AEA) and the World Bank’s Development Impact Evaluation (DIME) group."
  },
  {
    "objectID": "template-pap.html#helpful-resources",
    "href": "template-pap.html#helpful-resources",
    "title": "Pre-Analysis Plan Template",
    "section": "Helpful Resources:",
    "text": "Helpful Resources:\n\nFor guidance on pre-analysis plans, refer to the World Bank’s DIME Wiki: Pre-Analysis Plan - DIME Wiki\nFor examples of pre-analysis plans, explore the AEA’s RCT Registry: AEA RCT Registry"
  },
  {
    "objectID": "project.html",
    "href": "project.html",
    "title": "Semester Project Description",
    "section": "",
    "text": "With the semester project, you’ll dive into the full experimental research lifecycle and apply the tools and techniques you’re learning in this course. Working in your assigned group, you’ll design an experiment to answer a meaningful research question of your choosing, develop a pre-analysis plan, and analyze data (we’ll simulate the data given time constraints). By the end of the semester, you’ll have a manuscript and replication package that showcase your ability to design, execute, and report on a rigorous experimental research project. This is your opportunity to apply what we learn in class to a topic of interest.\n\n\n\n\n\n\nThe Pre-Analysis Plan (PAP) ensures your experiment is well-conceived, transparent, and analytically rigorous. This document lays out your research design and specifies how you will analyze the data.\n\n\n\nIntroduction and Background: Explain your research question and its significance.\n\nHypotheses: Clearly state primary and secondary hypotheses.\n\nExperimental Design:\n\nType of experiment (e.g., lab, online A/B test, field).\n\nRandom assignment procedure.\n\nTreatment and control conditions.\n\nParticipant details (e.g., recruitment, sample size, inclusion criteria).\n\n\nKey Outcomes: Define primary and secondary outcomes and describe how they will be measured.\n\nAnalytical Strategy:\n\nSpecify statistical methods and models.\n\nIncorporate analysis related to compliance or heterogeneous treatment effects.\n\nUse a machine learning approach (e.g., predicting treatment heterogeneity or analyzing secondary outcomes).\n\nAddress how you will handle multiple hypotheses testing, if applicable.\n\n\nLimitations: Discuss potential weaknesses and their impact on findings.\n\n\n\n\n\nClarity and Completeness (40%)\n\nRigor of Design and Analysis (40%)\n\nFeasibility and Practicality (20%)\n\n\n\n\n\nPre-Analysis Plan Template\nFor guidance on pre-analysis plans: World Bank DIME Wiki\nFor examples of pre-analysis plans: AEA RCT Registry\n\n\n\n\n\n\nThis deliverable includes a journal-style manuscript and a replication package to showcase your research findings and ensure reproducibility.\n\n\n\nFinal Manuscript: (10–15 pages)\n\nAbstract: Concise summary of your study and findings.\nIntroduction: Background and motivation for your research.\nMethods: Detailed description of your experimental design, treatments, and analytical approach.\nResults: Clear presentation of findings, including tables and figures.\nDiscussion: Interpretation, limitations, and implications for future research or policy.\nReferences: Properly formatted citations.\n\nReplication Package:\n\nSimulated Data: Dataset formatted for analysis.\nCode: Well-documented scripts (R or Python recommended) for data cleaning and analysis, incorporating machine learning where appropriate.\nREADME File: Detailed instructions for replicating your analysis.\n\n\n\n\n\n\nManuscript:\n\nWriting Quality and Organization (20%)\nClarity and Depth of Analysis (30%)\n\nReplication Package:\n\nReproducibility (30%)\nCompleteness and Documentation (20%)\n\n\n\n\n\n\n\n\n\nNow - April 9: Work with team on pre-analysis plan.\nApril 9: Submit Pre-Analysis Plan for feedback.\nApril 9 - April 28: Simulate and analyze data; draft your final deliverable.\nApril 28: Submit Final Manuscript and Replication Package.\n\n\n\n\n\n\nPre-Analysis Plan: 50%\nFinal Manuscript and Replication Package: 50%",
    "crumbs": [
      "Semester Project",
      "Project Description"
    ]
  },
  {
    "objectID": "project.html#overview",
    "href": "project.html#overview",
    "title": "Semester Project Description",
    "section": "",
    "text": "With the semester project, you’ll dive into the full experimental research lifecycle and apply the tools and techniques you’re learning in this course. Working in your assigned group, you’ll design an experiment to answer a meaningful research question of your choosing, develop a pre-analysis plan, and analyze data (we’ll simulate the data given time constraints). By the end of the semester, you’ll have a manuscript and replication package that showcase your ability to design, execute, and report on a rigorous experimental research project. This is your opportunity to apply what we learn in class to a topic of interest.",
    "crumbs": [
      "Semester Project",
      "Project Description"
    ]
  },
  {
    "objectID": "project.html#deliverables",
    "href": "project.html#deliverables",
    "title": "Semester Project Description",
    "section": "",
    "text": "The Pre-Analysis Plan (PAP) ensures your experiment is well-conceived, transparent, and analytically rigorous. This document lays out your research design and specifies how you will analyze the data.\n\n\n\nIntroduction and Background: Explain your research question and its significance.\n\nHypotheses: Clearly state primary and secondary hypotheses.\n\nExperimental Design:\n\nType of experiment (e.g., lab, online A/B test, field).\n\nRandom assignment procedure.\n\nTreatment and control conditions.\n\nParticipant details (e.g., recruitment, sample size, inclusion criteria).\n\n\nKey Outcomes: Define primary and secondary outcomes and describe how they will be measured.\n\nAnalytical Strategy:\n\nSpecify statistical methods and models.\n\nIncorporate analysis related to compliance or heterogeneous treatment effects.\n\nUse a machine learning approach (e.g., predicting treatment heterogeneity or analyzing secondary outcomes).\n\nAddress how you will handle multiple hypotheses testing, if applicable.\n\n\nLimitations: Discuss potential weaknesses and their impact on findings.\n\n\n\n\n\nClarity and Completeness (40%)\n\nRigor of Design and Analysis (40%)\n\nFeasibility and Practicality (20%)\n\n\n\n\n\nPre-Analysis Plan Template\nFor guidance on pre-analysis plans: World Bank DIME Wiki\nFor examples of pre-analysis plans: AEA RCT Registry\n\n\n\n\n\n\nThis deliverable includes a journal-style manuscript and a replication package to showcase your research findings and ensure reproducibility.\n\n\n\nFinal Manuscript: (10–15 pages)\n\nAbstract: Concise summary of your study and findings.\nIntroduction: Background and motivation for your research.\nMethods: Detailed description of your experimental design, treatments, and analytical approach.\nResults: Clear presentation of findings, including tables and figures.\nDiscussion: Interpretation, limitations, and implications for future research or policy.\nReferences: Properly formatted citations.\n\nReplication Package:\n\nSimulated Data: Dataset formatted for analysis.\nCode: Well-documented scripts (R or Python recommended) for data cleaning and analysis, incorporating machine learning where appropriate.\nREADME File: Detailed instructions for replicating your analysis.\n\n\n\n\n\n\nManuscript:\n\nWriting Quality and Organization (20%)\nClarity and Depth of Analysis (30%)\n\nReplication Package:\n\nReproducibility (30%)\nCompleteness and Documentation (20%)",
    "crumbs": [
      "Semester Project",
      "Project Description"
    ]
  },
  {
    "objectID": "project.html#timeline",
    "href": "project.html#timeline",
    "title": "Semester Project Description",
    "section": "",
    "text": "Now - April 9: Work with team on pre-analysis plan.\nApril 9: Submit Pre-Analysis Plan for feedback.\nApril 9 - April 28: Simulate and analyze data; draft your final deliverable.\nApril 28: Submit Final Manuscript and Replication Package.",
    "crumbs": [
      "Semester Project",
      "Project Description"
    ]
  },
  {
    "objectID": "project.html#grading-breakdown",
    "href": "project.html#grading-breakdown",
    "title": "Semester Project Description",
    "section": "",
    "text": "Pre-Analysis Plan: 50%\nFinal Manuscript and Replication Package: 50%",
    "crumbs": [
      "Semester Project",
      "Project Description"
    ]
  },
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Resources",
    "section": "",
    "text": "List, John. Experimental Economics: Theory and Practice, 2025. (Link available soon)\nChernozhukov, Victor, Christian Hansen, Nathan Kallus, Martin Spindler, and Vasilis Syrgkanis. Applied Causal Inference Powered by ML and AI, 2024.\nWager, Stefan. Causal Inference: A Statistical Learning Approach, 2024.\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. An Introduction to Statistical Learning: With Applications in R. Springer Texts in Statistics. New York, NY: Springer US, 2021.",
    "crumbs": [
      "Resources"
    ]
  },
  {
    "objectID": "resources.html#key-texts",
    "href": "resources.html#key-texts",
    "title": "Resources",
    "section": "",
    "text": "List, John. Experimental Economics: Theory and Practice, 2025. (Link available soon)\nChernozhukov, Victor, Christian Hansen, Nathan Kallus, Martin Spindler, and Vasilis Syrgkanis. Applied Causal Inference Powered by ML and AI, 2024.\nWager, Stefan. Causal Inference: A Statistical Learning Approach, 2024.\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. An Introduction to Statistical Learning: With Applications in R. Springer Texts in Statistics. New York, NY: Springer US, 2021.",
    "crumbs": [
      "Resources"
    ]
  },
  {
    "objectID": "resources.html#r-tutorials",
    "href": "resources.html#r-tutorials",
    "title": "Resources",
    "section": "R Tutorials",
    "text": "R Tutorials\nParadis, Emmanuel. R for Beginners, 2005.\nWickham, Hadley, and Mine Çetinkaya-Rundel. R for Data Science, 2nd ed. 2023.\nR “Cheat Sheet” Compilation.\nThe R Graph Gallery",
    "crumbs": [
      "Resources"
    ]
  },
  {
    "objectID": "resources.html#websites",
    "href": "resources.html#websites",
    "title": "Resources",
    "section": "Websites",
    "text": "Websites\nField Experiments Website\nPapers in economic field experiments.\nAbdul Latif Jameel Poverty Action Lab (JPAL)\nA global research center based at MIT’s Economics Department that works to reduce poverty through rigorous scientific evidence and policy implementation\n\nJPAL maintains an excellent site with resources for randomized evaluations and more\n\nStanford Social Impact Lab\nMelissa Dell’s EconDL Website\n\nEconDL is a comprehensive resource detailing applications of Deep Learning in Economics. This is a companion website to the paper Deep Learning for Economists and aims to be a go-to resource for economists and other social scientists for applying tools provided by deep learning in their research.",
    "crumbs": [
      "Resources"
    ]
  },
  {
    "objectID": "resources.html#data",
    "href": "resources.html#data",
    "title": "Resources",
    "section": "Data",
    "text": "Data\nStanford GSB Social Impact Lab Experiment Data",
    "crumbs": [
      "Resources"
    ]
  },
  {
    "objectID": "resources.html#templates",
    "href": "resources.html#templates",
    "title": "Resources",
    "section": "Templates",
    "text": "Templates\nPre-Analysis Plan Template\nRCT Analysis Template Repository (GitHub)\nJ-Pal Sample Size and Power Repository",
    "crumbs": [
      "Resources"
    ]
  },
  {
    "objectID": "resources.html#interactive-tools",
    "href": "resources.html#interactive-tools",
    "title": "Resources",
    "section": "Interactive Tools",
    "text": "Interactive Tools\nOptimal Experimental Design Calculator\nAn interactive tool to help you determine optimal sample sizes and allocation ratios for experiments. Features include:\n\nCalculation of Minimum Detectable Effect (MDE)\nOptimal allocation under cost constraints\nSupport for both binary and continuous outcomes\nStep-by-step calculation explanations",
    "crumbs": [
      "Resources"
    ]
  },
  {
    "objectID": "unit-2/lec-2-1-slides.html#experimental-design-an-econ-101-approach",
    "href": "unit-2/lec-2-1-slides.html#experimental-design-an-econ-101-approach",
    "title": "Unit 2.1: Optimal Experimental Design",
    "section": "Experimental Design: an Econ 101 Approach",
    "text": "Experimental Design: an Econ 101 Approach\n\nPreviously: statistical conclusion validity, power calculations, simulation\nNow we’re in control: designing experiments to maximize learning (i.e. statistical power)\n\nKey Question:\nHow to design an experiment to maximize power subject to constraints (e.g. budget, logistical)?\n\nAnswer:\nLearn some economics!\n\n\nWe’ve covered statistical conclusion validity, power calculations, and simulation. Now we’re shifting to designing experiments."
  },
  {
    "objectID": "unit-2/lec-2-1-slides.html#back-to-econ-101",
    "href": "unit-2/lec-2-1-slides.html#back-to-econ-101",
    "title": "Unit 2.1: Optimal Experimental Design",
    "section": "(Back) to Econ 101",
    "text": "(Back) to Econ 101\nOptimize: Weighing costs and benefits\n\nObjective:\n\nMaximize statistical power\nOr minimize the smallest effect we can see (the Minimum Detectable Effect)\n\nConstraints:\n\nResearch Budget\nlogistical limitations"
  },
  {
    "objectID": "unit-2/lec-2-1-slides.html#key-elements-in-sample-size-calculation",
    "href": "unit-2/lec-2-1-slides.html#key-elements-in-sample-size-calculation",
    "title": "Unit 2.1: Optimal Experimental Design",
    "section": "Key Elements in Sample Size Calculation",
    "text": "Key Elements in Sample Size Calculation\n\n\nSignificance level (\\(\\alpha\\)): The probability of a false positive (rejecting the null when it’s actually true).\nMinimum Detectable Effect (MDE): The smallest true effect size you want to be able to detect with high probability.\nPower (\\(1 - \\beta\\)): The probability of detecting a true effect (i.e., rejecting the null when it’s false).\n\n\n\n\n\n\n\n\n\nWarning\n\n\nMDE ≠ Expected effect size!!"
  },
  {
    "objectID": "unit-2/lec-2-1-slides.html#a-simple-example",
    "href": "unit-2/lec-2-1-slides.html#a-simple-example",
    "title": "Unit 2.1: Optimal Experimental Design",
    "section": "A Simple Example",
    "text": "A Simple Example\n\nYou’ve received a research grant for a two-arm study:\n\nControl group (no intervention)\nTreatment group (new health intervention)\n\nOutcome measure: \\[Y_i = \\text{Health and Happiness Index}\\]\nKey questions:\n\nHow many participants do you need?\nHow to split the sample between treatment and control?"
  },
  {
    "objectID": "unit-2/lec-2-1-slides.html#outcome-model",
    "href": "unit-2/lec-2-1-slides.html#outcome-model",
    "title": "Unit 2.1: Optimal Experimental Design",
    "section": "Outcome Model",
    "text": "Outcome Model\n\n\\(Y_i\\): Outcome for subject \\(i\\)\n\\(\\mathbf{X}_i\\): Observable variables\n\\(\\alpha_i\\): Unobserved effect (“innate personal quirks”)\n\\(\\tau_i\\): Person-specific treatment effect (\\(\\mathbb{E}[\\tau_i] = 0\\))\n\\(\\varepsilon_i\\): i.i.d. error term\n\n\\[Y_{i} \\;=\\; \\alpha_i \\;+\\; \\mathbf{X}_i \\,\\beta \\;+\\; \\bar{\\tau} \\,D_i \\;+\\; \\tau_i\\,D_i \\;+\\; \\varepsilon_i.\\tag{1}\\]\n\n\\(\\bar{\\tau}\\): Average treatment effect\n\\(\\tau_i\\): Idiosyncratic difference around \\(\\bar{\\tau}\\)"
  },
  {
    "objectID": "unit-2/lec-2-1-slides.html#unbiased-ate-estimator",
    "href": "unit-2/lec-2-1-slides.html#unbiased-ate-estimator",
    "title": "Unit 2.1: Optimal Experimental Design",
    "section": "Unbiased ATE Estimator",
    "text": "Unbiased ATE Estimator\n\\[\\hat{\\tau}\\;=\\; \\mathbb{E}[Y_i \\mid D_i=1]\\;-\\;\\mathbb{E}[Y_i \\mid D_i=0]\\]\n\nUnbiased due to randomization\n\\(D_i\\) independent of \\(\\alpha_i, \\tau_i,\\) and \\(\\varepsilon_i\\)\n\nThen we have the Variance of Estimated ATE\n\\[\\mathrm{Var}(\\hat{\\tau})\\;=\\;\\frac{\\sigma^2}{N}\\;=\\;\\frac{\\mathrm{Var}(\\varepsilon_i)}{\\;N \\,\\times\\, \\mathrm{Var}(D_i)\\,}.\\tag{2}\\]\n\n\\(\\mathrm{Var}(\\varepsilon_i)\\): Variance of unobserved “noise”\n\\(N\\): Total number of units\n\\(\\mathrm{Var}(D_i)\\): Variance of treatment assignment indicator"
  },
  {
    "objectID": "unit-2/lec-2-1-slides.html#what-influences-mathrmvarhattau",
    "href": "unit-2/lec-2-1-slides.html#what-influences-mathrmvarhattau",
    "title": "Unit 2.1: Optimal Experimental Design",
    "section": "What influences \\(\\mathrm{Var}(\\hat{\\tau})\\)?",
    "text": "What influences \\(\\mathrm{Var}(\\hat{\\tau})\\)?\n\\[\\mathrm{Var}(\\hat{\\tau})\\;=\\;\\frac{\\sigma^2}{N}\\;=\\;\\frac{\\mathrm{Var}(\\varepsilon_i)}{\\;N \\,\\times\\, \\mathrm{Var}(D_i)\\,}.\\tag{2}\\]\n\n\nIncreases with \\(\\mathrm{Var}(\\varepsilon_i)\\)\n\n\n\n\nDecreases with \\(N\\)\n\n\n\n\nDecreases with \\(\\mathrm{Var}(D_i)\\)"
  },
  {
    "objectID": "unit-2/lec-2-1-slides.html#minimum-detectable-effect-mde---the-star-of-the-show",
    "href": "unit-2/lec-2-1-slides.html#minimum-detectable-effect-mde---the-star-of-the-show",
    "title": "Unit 2.1: Optimal Experimental Design",
    "section": "Minimum Detectable Effect (MDE) - the star of the show!",
    "text": "Minimum Detectable Effect (MDE) - the star of the show!\nNow let’s assume that potential outcomes are conditional potential outcomes y0 and y1 that are normally distributed.\n\\[\nY_{i0} | X_i \\sim \\mathbb{N}(\\mu_0, \\sigma^2) \\mid D_i=0\n\\]\n\\[\nY_{i1} | X_i \\sim \\mathbb{N}(\\mu_1, \\sigma^2) \\mid D_i=1.\n\\]\n\n\nMDE: Smallest effect \\(\\mu_1 - \\mu_0\\) detectable with specified power\nHypotheses: \\(H_0: \\mu_0 = 0 \\quad \\text{vs} \\quad H_1: \\mu_1 \\neq 0\\)\nEstimator: \\(\\bar{Y}_1 - \\bar{Y}_0\\) for \\(\\mu_1 - \\mu_0\\) (Assuming independent observations)"
  },
  {
    "objectID": "unit-2/lec-2-1-slides.html#mde-formula---the-star-of-the-show",
    "href": "unit-2/lec-2-1-slides.html#mde-formula---the-star-of-the-show",
    "title": "Unit 2.1: Optimal Experimental Design",
    "section": "MDE formula - the star of the show!",
    "text": "MDE formula - the star of the show!\n\\[\n\\text{MDE} = (z_{1-\\alpha/2} + z_{1-\\beta}) \\sqrt{\\frac{\\sigma^2}{n_1} + \\frac{\\sigma^2}{n_0}}\n\\]\n\n\\(n_1\\): Treatment sample size\n\\(n_0\\): Control sample size\n\\(\\sigma^2\\): Common outcome variance"
  },
  {
    "objectID": "unit-2/lec-2-1-slides.html#mde-is-effect-size-tau-s.t.-power-has-the-same-cutoff-as-the-significance-level",
    "href": "unit-2/lec-2-1-slides.html#mde-is-effect-size-tau-s.t.-power-has-the-same-cutoff-as-the-significance-level",
    "title": "Unit 2.1: Optimal Experimental Design",
    "section": "MDE is effect size \\(\\tau\\) s.t. power has the same cutoff as the significance level",
    "text": "MDE is effect size \\(\\tau\\) s.t. power has the same cutoff as the significance level"
  },
  {
    "objectID": "unit-2/lec-2-1-slides.html#how-many-participants-should-i-recruit-in-each-arm-of-our-study",
    "href": "unit-2/lec-2-1-slides.html#how-many-participants-should-i-recruit-in-each-arm-of-our-study",
    "title": "Unit 2.1: Optimal Experimental Design",
    "section": "How many participants should I recruit in each arm of our study?",
    "text": "How many participants should I recruit in each arm of our study?\nTo find the optimal sample allocation, we need to minimize the MDE:\n\\[\n\\min_{n_0, n_1} \\text{MDE}\n\\]\nsubject to \\(n_0 + n_1 = N\\)."
  },
  {
    "objectID": "unit-2/lec-2-1-slides.html#the-equal-variance-case",
    "href": "unit-2/lec-2-1-slides.html#the-equal-variance-case",
    "title": "Unit 2.1: Optimal Experimental Design",
    "section": "The Equal Variance Case",
    "text": "The Equal Variance Case\n\nAssume equal variances in treatment and control (and no cost difference)\n\n\\[\\text{MDE} = (z_{1-\\alpha/2} + z_{1-\\beta})\\sqrt{\\frac{\\sigma^2}{n_0} + \\frac{\\sigma^2}{n_1}}\\]\n\n\\(n_1\\): Treatment sample size\n\\(n_0\\): Control sample size\n\\(\\sigma^2\\): Common variance\nResult: \\(n_0^* = n_1^* = \\frac{N}{2}\\)\nOptimal allocation is a 50-50 split"
  },
  {
    "objectID": "unit-2/lec-2-1-slides.html#unequal-variances",
    "href": "unit-2/lec-2-1-slides.html#unequal-variances",
    "title": "Unit 2.1: Optimal Experimental Design",
    "section": "Unequal Variances",
    "text": "Unequal Variances\nBut how often are variances likely to be equal?\n\nWith unequal variances, the MDE formula changes:\n\n\\[\\text{MDE} = (z_{1-\\alpha/2} + z_{1-\\beta})\\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_0^2}{n_0}}\\]\n\nStrategy: Oversample group with higher variance\nChallenges: Estimating \\(\\sigma_0^2\\) and \\(\\sigma_1^2\\) in advance"
  },
  {
    "objectID": "unit-2/lec-2-1-slides.html#so.-much.-math.-lets-make-a-calculator",
    "href": "unit-2/lec-2-1-slides.html#so.-much.-math.-lets-make-a-calculator",
    "title": "Unit 2.1: Optimal Experimental Design",
    "section": "So. Much. Math. Let’s make a calculator",
    "text": "So. Much. Math. Let’s make a calculator\nOptimal Experimental Design Calculator"
  },
  {
    "objectID": "unit-2/lec-2-1-slides.html#real-world-cost-considerations",
    "href": "unit-2/lec-2-1-slides.html#real-world-cost-considerations",
    "title": "Unit 2.1: Optimal Experimental Design",
    "section": "Real-world cost considerations",
    "text": "Real-world cost considerations\n\nTreatment group: Higher cost (\\(c_1\\))\n\nIntervention funding\nStaff hiring\nSupplies\nParticipant incentives\n\nControl group: Lower but non-zero cost (\\(c_0\\))\n\nSurvey administration\nLab visits"
  },
  {
    "objectID": "unit-2/lec-2-1-slides.html#budget-constrained-optimization",
    "href": "unit-2/lec-2-1-slides.html#budget-constrained-optimization",
    "title": "Unit 2.1: Optimal Experimental Design",
    "section": "Budget-constrained optimization",
    "text": "Budget-constrained optimization\n\\[\n\\min_{n_0,\\,n_1} \\text{MDE}\n\\quad \\text{subject to} \\quad\nc_0\\,n_0 + c_1\\,n_1 \\;\\le\\; M\n\\]\n\n\\(c_0\\): Cost per control participant\n\\(c_1\\): Cost per treatment participant\n\\(M\\): Total budget\n\nOptimal allocation\n\\[\n\\frac{n_1}{n_0} \\;=\\; \\sqrt{\\frac{\\sigma_1^2\\,c_0}{\\sigma_0^2\\,c_1}}\n\\]"
  },
  {
    "objectID": "unit-2/lec-2-1-slides.html#what-is-the-implication",
    "href": "unit-2/lec-2-1-slides.html#what-is-the-implication",
    "title": "Unit 2.1: Optimal Experimental Design",
    "section": "What is the implication?",
    "text": "What is the implication?\n\\[\n\\frac{n_1}{n_0} \\;=\\; \\sqrt{\\frac{\\sigma_1^2\\,c_0}{\\sigma_0^2\\,c_1}}\n\\]\nNo Free Lunch: Cost differences can shift your allocation away from the standard 50–50.\n\nIf \\(c_1 \\gg c_0\\): Fewer treatment participants\nIf \\(c_1 \\ll c_0\\): More treatment participants"
  },
  {
    "objectID": "unit-2/lec-2-1-slides.html#design-extensions",
    "href": "unit-2/lec-2-1-slides.html#design-extensions",
    "title": "Unit 2.1: Optimal Experimental Design",
    "section": "Design Extensions",
    "text": "Design Extensions\nSee lecture notes for discussion of some extensions (The premise is the same)\n\nDichotomous treatment with binomial outcome\nMultiple treatment arms\nClustered designs"
  },
  {
    "objectID": "unit-2/lec-2-1-slides.html#strategies-to-increase-power",
    "href": "unit-2/lec-2-1-slides.html#strategies-to-increase-power",
    "title": "Unit 2.1: Optimal Experimental Design",
    "section": "Strategies to Increase Power",
    "text": "Strategies to Increase Power\nAlso in notes, some strategies for increasing power with fixed n:\n\nMaximize compliance\nChoose less noisy outcome measures\nMultiple waves of baseline and endline data\nInclude covariates in estimation\nImplement stratified randomization"
  },
  {
    "objectID": "unit-2/lec-2-1-slides.html#conclusion",
    "href": "unit-2/lec-2-1-slides.html#conclusion",
    "title": "Unit 2.1: Optimal Experimental Design",
    "section": "Conclusion",
    "text": "Conclusion\n\nOne magical thing about experimentation: allows control over design, you’re not just taking what the world gives you\nConsider all factors: sample sizes, costs, clustering, compliance\nPlan thoroughly to maximize power within constraints\n\nNext time: look at more advanced randomization strategies that can further improve your power"
  },
  {
    "objectID": "unit-2/shiny-oed-app.html",
    "href": "unit-2/shiny-oed-app.html",
    "title": "Optimal Experiment Design App",
    "section": "",
    "text": "This interactive tool helps you explore optimal experimental design for randomized controlled trials (RCTs) with one treatment and one control group. The app allows you to:\n\nCalculate optimal sample sizes for different effect sizes\nCompare different allocation ratios between treatment and control groups\nExplore cost-constrained designs"
  },
  {
    "objectID": "unit-2/shiny-oed-app.html#overview",
    "href": "unit-2/shiny-oed-app.html#overview",
    "title": "Optimal Experiment Design App",
    "section": "",
    "text": "This interactive tool helps you explore optimal experimental design for randomized controlled trials (RCTs) with one treatment and one control group. The app allows you to:\n\nCalculate optimal sample sizes for different effect sizes\nCompare different allocation ratios between treatment and control groups\nExplore cost-constrained designs"
  },
  {
    "objectID": "unit-2/shiny-oed-app.html#key-statistical-concepts",
    "href": "unit-2/shiny-oed-app.html#key-statistical-concepts",
    "title": "Optimal Experiment Design App",
    "section": "Key Statistical Concepts",
    "text": "Key Statistical Concepts\nEffect Size\nThe effect size represents the magnitude of the difference between treatment and control groups. Understanding effect size is crucial for: 1. Determining practical significance (not just statistical significance) 2. Planning sample sizes 3. Comparing results across different studies\nFor Continuous Outcomes\nMeasured as Cohen’s \\(d\\) (standardized mean difference):\n\\[d = \\frac{\\mu_T - \\mu_C}{\\sigma_{\\text{pooled}}}\\]\nTypical interpretations:\n\n\nSmall effect: ~0.2 (e.g., a small improvement in test scores)\n\nMedium effect: ~0.5 (e.g., noticeable improvement in patient outcomes)\n\nLarge effect: ~0.8 (e.g., dramatic improvement in treatment response)\n\nThe standardization by pooled standard deviation (\\(\\sigma_{\\text{pooled}}\\)) allows comparison across different scales and measures.\nFor Binary Outcomes\nMeasured as the difference in proportions (\\(p_T - p_C\\)):\n\n\n\\(p_T\\) is the success rate in the treatment group\n\n\\(p_C\\) is the success rate in the control group\n\nExample interpretations:\n\nA difference of 0.05 (5 percentage points) might be meaningful for rare events\nA difference of 0.20 (20 percentage points) might be expected for effective interventions\n\nThe variance for binary outcomes follows the binomial distribution:\n\n\\(\\text{Var}(p) = \\frac{p(1-p)}{n}\\)\nLarger variance near \\(p = 0.5\\)\n\nSmaller variance near \\(p = 0\\) or \\(p = 1\\)\n\nStatistical Power and Sample Size\nThe interplay between power, sample size, and effect size forms the foundation of experimental design. These concepts are connected through a fundamental trade-off:\nPower \\((1-\\beta)\\)\n\nThe probability of detecting a true effect when it exists:\n\nDefinition: Power \\(= P(\\text{Reject }H_0 \\mid H_1 \\text{ is true})\\)\nInterpretation: The probability of a study finding a real effect\n\nTypical value: \\(0.80\\) (80%)\n\nLower values (e.g., 70%) increase risk of missing real effects\nHigher values (e.g., 90%) require substantially larger sample sizes\n\n\nType II Error \\((\\beta)\\)\n\n\n\nDefinition: \\(\\beta = P(\\text{Fail to reject }H_0 \\mid H_1 \\text{ is true})\\)\n\n\nInterpretation: The probability of missing a real effect\n\nRelationship: Power \\(= 1 - \\beta\\)\n\n\nExample: With 80% power, β = 20% chance of missing a real effect\nSignificance Level \\((\\alpha)\\)\n\nThe probability of falsely claiming an effect exists:\n\nDefinition: \\(\\alpha = P(\\text{Reject }H_0 \\mid H_0 \\text{ is true})\\)\nInterpretation: Risk of false positive findings\n\nCommon values:\n\n0.05 (5%): Standard for most research\n0.01 (1%): More stringent, used for critical decisions\n0.10 (10%): Sometimes used in pilot studies\n\n\nTrade-offs\n\n\nSample Size vs. Power:\n\nLarger samples → Higher power\nDoubling power often requires more than doubling sample size\n\n\n\nEffect Size vs. Sample Size:\n\nSmaller effects require larger samples\nRelationship is quadratic \\((n \\propto 1/d^2)\\)\n\n\n\n\nType I vs. Type II Errors:\n\nReducing one type of error often increases the other\nMust balance based on consequences of each error type\n\n\nMinimum Detectable Effect (MDE)\nThe MDE is the smallest true effect size that can be detected with the specified power and significance level. For a two-sided test:\n\\[\\text{MDE} = (z_{1-\\alpha/2} + z_{1-\\beta}) \\sqrt{\\frac{\\text{Var}_T}{n_T} + \\frac{\\text{Var}_C}{n_C}}\\]\nwhere:\n\n\n\\(z_{1-\\alpha/2}\\) is the critical value for significance level \\(\\alpha\\)\n\n\n\\(z_{1-\\beta}\\) is the critical value for power \\(1-\\beta\\)\n\n\n\\(\\text{Var}_T\\), \\(\\text{Var}_C\\) are the variances in treatment and control groups\n\n\\(n_T\\), \\(n_C\\) are the sample sizes in treatment and control groups\nOptimal Allocation\nThe optimal allocation ratio between treatment and control groups depends on:\n\nEqual Sample Sizes: When costs are equal and variances are similar, a 1:1 ratio is optimal\nUnequal Variances: The optimal ratio is proportional to the standard deviations: \\[\\frac{n_T}{n_C} \\propto \\frac{\\sigma_T}{\\sigma_C}\\]\nUnequal Costs: When costs differ, the optimal ratio is: \\[\\frac{n_T}{n_C} \\propto \\frac{\\sigma_T}{\\sigma_C} \\sqrt{\\frac{c_C}{c_T}}\\] where \\(c_T\\), \\(c_C\\) are the per-unit costs"
  },
  {
    "objectID": "unit-2/shiny-oed-app.html#using-the-app",
    "href": "unit-2/shiny-oed-app.html#using-the-app",
    "title": "Optimal Experiment Design App",
    "section": "Using the App",
    "text": "Using the App\nLocal RStudio Version\nTo run this app locally in RStudio:\n\nshiny::runApp(\"unit-2/oed-app.R\")\n\nBrowser Version\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 1500\n\nlibrary(shiny)\nlibrary(ggplot2)\n\nui &lt;- fluidPage(\n  titlePanel(\"Optimal Experimental Design for One Treatment vs. One Control\"),\n  \n  # Add help text at the top\n  fluidRow(\n    column(12,\n      h4(\"Understanding Power Analysis and Optimal Design\"),\n      p(\"This app helps you determine optimal sample sizes and allocation ratios for experiments.\",\n        \"The calculations consider both statistical power and practical constraints.\"),\n      p(\"Key concepts:\"),\n      tags$ul(\n        tags$li(strong(\"Minimum Detectable Effect (MDE):\"), \n               \"The smallest true effect size that your study can reliably detect with the specified power.\"),\n        tags$li(strong(\"Optimal Allocation:\"), \n               \"The best way to divide participants between treatment and control groups, considering:\",\n               tags$ul(\n                 tags$li(\"Statistical efficiency (minimizing MDE)\"),\n                 tags$li(\"Cost constraints\"),\n                 tags$li(\"Practical implementation\"))\n               ),\n        tags$li(strong(\"Power vs. Sample Size:\"), \n               \"There's always a trade-off - higher power requires larger samples.\")\n      ),\n      p(\"Adjust the parameters below to explore how different choices affect your study design.\"),\n      tags$hr()\n    )\n  ),\n  \n  sidebarLayout(\n    sidebarPanel(\n      # --- Choose outcome type (binary or continuous)\n      radioButtons(\n        inputId = \"outcomeType\", \n        label = \"Outcome Type\",\n        choices = c(\"Binary\" = \"binary\", \"Continuous\" = \"continuous\"),\n        selected = \"continuous\"\n      ),\n      div(class = \"well\",\n        h5(\"Outcome Types:\"),\n        p(strong(\"Binary Outcomes:\"),\n          \"Measured as proportions or percentages (e.g., success rates, mortality rates).\",\n          \"The variance is determined by the proportion itself - highest at p=0.5.\"),\n        p(strong(\"Continuous Outcomes:\"),\n          \"Measured on a continuous scale (e.g., blood pressure, test scores).\",\n          \"Requires estimates of population variance in each group.\")\n      ),\n      \n      # --- Significance level (alpha)\n      sliderInput(\n        inputId = \"alpha\", \n        label = \"Significance level (alpha):\",\n        min = 0.001, \n        max = 0.10, \n        step = 0.001,\n        value = 0.05\n      ),\n      div(class = \"well\",\n        h5(\"Significance Level (α):\"),\n        p(\"The probability of falsely concluding there is an effect when there isn't one (Type I error).\"),\n        tags$ul(\n          tags$li(strong(\"0.05 (5%)\"), \": Standard choice - 1 in 20 chance of false positive\"),\n          tags$li(strong(\"0.01 (1%)\"), \": More conservative - use for critical decisions\"),\n          tags$li(strong(\"0.10 (10%)\"), \": More liberal - might use in pilot studies\")\n        )\n      ),\n      \n      # --- Power (1 - beta)\n      sliderInput(\n        inputId = \"power\", \n        label = \"Statistical Power (1 - beta):\",\n        min = 0.50, \n        max = 0.99, \n        step = 0.01,\n        value = 0.80\n      ),\n      div(class = \"well\",\n        h5(\"Statistical Power (1-β):\"),\n        p(\"The probability of detecting a true effect when it exists.\"),\n        tags$ul(\n          tags$li(strong(\"0.80 (80%)\"), \": Standard choice - accepts 20% chance of missing real effects\"),\n          tags$li(strong(\"0.90 (90%)\"), \": Higher power - use for critical studies, but requires larger samples\"),\n          tags$li(strong(\"0.70 (70%)\"), \": Lower power - might use in pilot studies or with resource constraints\")\n        ),\n        p(\"Remember: Increasing power requires larger sample sizes, often substantially.\")\n      ),\n      \n      # --- Parameters for continuous outcome\n      conditionalPanel(\n        condition = \"input.outcomeType == 'continuous'\",\n        numericInput(\"sigmaT\", \"Treatment Variance (σ²_T):\", 1),\n        numericInput(\"sigmaC\", \"Control Variance (σ²_C):\", 1),\n        helpText(\"The expected variances in each group. If unknown, assume equal variances.\")\n      ),\n      \n      # --- Parameters for binary outcome\n      conditionalPanel(\n        condition = \"input.outcomeType == 'binary'\",\n        sliderInput(\"pT\", \"Treatment proportion (p_T):\",\n                    min = 0, max = 1, step = 0.01, value = 0.3),\n        sliderInput(\"pC\", \"Control proportion (p_C):\",\n                    min = 0, max = 1, step = 0.01, value = 0.3),\n        helpText(\"Set the difference in these values to determine your minimum detectable effect (MDE).\",\n                 \"p_C might be derived from historical data.\")\n      ),\n      \n      # --- Allocation: either fix total sample size or total cost\n      radioButtons(\n        inputId = \"constraintType\",\n        label = \"Constraint Type:\",\n        choices = c(\"Fixed Total Sample Size\" = \"sample\",\n                    \"Fixed Total Cost\"        = \"cost\")\n      ),\n      div(class = \"well\",\n        h5(\"Design Constraints:\"),\n        p(strong(\"Fixed Total Sample Size:\"),\n          \"Use when you have a specific total number of participants available.\",\n          \"The app will help you determine the optimal split between groups.\"),\n        p(strong(\"Fixed Total Cost:\"),\n          \"Use when you have a fixed budget and different costs per group.\",\n          \"Common when treatment is more expensive than control, or when recruitment costs differ.\")\n      ),\n      \n      # --- If total sample size is the constraint\n      conditionalPanel(\n        condition = \"input.constraintType == 'sample'\",\n        numericInput(\"N\", \"Total Sample Size (N):\", 200, min = 2, step = 1)\n      ),\n      \n      # --- If total cost is the constraint\n      conditionalPanel(\n        condition = \"input.constraintType == 'cost'\",\n        numericInput(\"costT\", \"Cost per Treatment Unit:\", 2, min = 1, step = 1),\n        numericInput(\"costC\", \"Cost per Control Unit:\", 1, min = 1, step = 1),\n        numericInput(\"budget\", \"Total Budget:\", 300, min = 1, step = 1)\n      ),\n      \n      # --- Option to let the user pick ratio or find optimum\n      radioButtons(\n        inputId = \"ratioChoice\",\n        label = \"Allocation Choice:\",\n        choices = c(\"Manually Pick Ratio\" = \"manual\",\n                    \"Find Optimal Ratio\"  = \"optimal\")\n      ),\n      \n      sliderInput(\n        inputId = \"ratio\",\n        label = \"Treatment:Control Ratio (r = n_T / n_C):\",\n        min = 0.1,\n        max = 10,\n        step = 0.1,\n        value = 1\n      )\n    ),\n    \n    mainPanel(\n      tabsetPanel(\n        tabPanel(\"MDE vs. Ratio\", \n                 plotOutput(\"plotMDE\"),\n                 br(),\n                 verbatimTextOutput(\"textResult\"),\n                 br(),\n                 h4(\"Step-by-Step Calculations\"),\n                 uiOutput(\"mdeCalcSteps\")\n        ),\n        tabPanel(\"Cost Allocation Plot\",\n                 plotOutput(\"plotCost\", height = \"500px\"),\n                 br(),\n                 h4(\"Step-by-Step Calculations\"),\n                 uiOutput(\"costCalcSteps\"),\n                 br(),\n                 helpText(\"This plot shows the budget constraint line \",\n                          \"and iso-MDE curves for different cost allocations \",\n                          \"(c_C on the x-axis, c_T on the y-axis). \",\n                          \"The red point indicates the optimal allocation \",\n                          \"where MDE is minimized, if available.\")\n        )\n      )\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n  \n  # Helper: z-value for alpha/2 (two-sided) and for power\n  zAlpha &lt;- reactive({\n    qnorm(1 - input$alpha / 2)\n  })\n  zBeta &lt;- reactive({\n    qnorm(input$power)\n  })\n  \n  # Function to compute MDE given n_C, n_T (the sample sizes)\n  computeMDE_from_n &lt;- function(nC, nT) {\n    if (input$outcomeType == \"continuous\") {\n      sigmaT &lt;- input$sigmaT\n      sigmaC &lt;- input$sigmaC\n      varTerm &lt;- sigmaT^2 / nT + sigmaC^2 / nC\n    } else {\n      pT &lt;- input$pT\n      pC &lt;- input$pC\n      varTerm &lt;- pT*(1 - pT)/nT + pC*(1 - pC)/nC\n    }\n    (zAlpha() + zBeta()) * sqrt(varTerm)\n  }\n  \n  # Function to compute MDE given a ratio (for either sample or cost constraint)\n  computeMDE_ratio &lt;- function(r) {\n    # 1) Determine n_T and n_C based on constraints and ratio\n    if (input$constraintType == \"sample\") {\n      # n_T + n_C = N, and n_T / n_C = r\n      n_C &lt;- input$N / (r + 1)\n      n_T &lt;- r * n_C\n    } else {\n      # costT * n_T + costC * n_C = budget, n_T / n_C = r\n      n_C &lt;- input$budget / (r * input$costT + input$costC)\n      n_T &lt;- r * n_C\n    }\n    \n    # 2) Compute MDE\n    computeMDE_from_n(n_C, n_T)\n  }\n  \n  # Plot: MDE vs. ratio\n  output$plotMDE &lt;- renderPlot({\n    # We'll vary r from 0.1 to 10 in increments of 0.1\n    rVals &lt;- seq(0.1, 10, by = 0.1)\n    mdeVals &lt;- sapply(rVals, computeMDE_ratio)\n    \n    plot(rVals, mdeVals, type = \"l\", lwd = 2, \n         xlab = \"Ratio (n_T / n_C)\", ylab = \"MDE\",\n         main = \"Minimum Detectable Effect vs. Allocation Ratio\")\n    \n    # If in manual mode, highlight user's chosen ratio\n    if (input$ratioChoice == \"manual\") {\n      rUser &lt;- input$ratio\n      userMde &lt;- computeMDE_ratio(rUser)\n      points(rUser, userMde, col = \"red\", pch = 19, cex = 1.5)\n    }\n  })\n  \n  # Helper function to format numbers nicely\n  format_num &lt;- function(x) format(round(x, 4), nsmall = 4)\n  \n  # Text output: either user-chosen ratio or optimal ratio\n  output$textResult &lt;- renderPrint({\n    if (input$ratioChoice == \"optimal\") {\n      # Find ratio that yields smallest MDE\n      rTest &lt;- seq(0.1, 10, 0.01)\n      mdeTest &lt;- sapply(rTest, computeMDE_ratio)\n      idxMin &lt;- which.min(mdeTest)\n      rOpt &lt;- rTest[idxMin]\n      mdeOpt &lt;- mdeTest[idxMin]\n      \n      if (input$constraintType == \"sample\") {\n        n_C_opt &lt;- input$N / (rOpt + 1)\n        n_T_opt &lt;- rOpt * n_C_opt\n      } else {\n        n_C_opt &lt;- input$budget / (rOpt * input$costT + input$costC)\n        n_T_opt &lt;- rOpt * n_C_opt\n      }\n      \n      cat(\"Optimal Allocation:\\n\")\n      cat(\"Optimal ratio (n_T / n_C):\", round(rOpt, 3), \"\\n\")\n      cat(\"Control group size (n_C):\", round(n_C_opt), \"\\n\")\n      cat(\"Treatment group size (n_T):\", round(n_T_opt), \"\\n\")\n      cat(\"Minimum Detectable Effect:\", round(mdeOpt, 3), \"\\n\")\n      \n    } else {\n      # User-chosen ratio\n      r &lt;- input$ratio\n      mde &lt;- computeMDE_ratio(r)\n      \n      if (input$constraintType == \"sample\") {\n        n_C &lt;- input$N / (r + 1)\n        n_T &lt;- r * n_C\n      } else {\n        n_C &lt;- input$budget / (r * input$costT + input$costC)\n        n_T &lt;- r * n_C\n      }\n      \n      cat(\"Current Allocation:\\n\")\n      cat(\"Ratio (n_T / n_C):\", round(r, 3), \"\\n\")\n      cat(\"Control group size (n_C):\", round(n_C), \"\\n\")\n      cat(\"Treatment group size (n_T):\", round(n_T), \"\\n\")\n      cat(\"Minimum Detectable Effect:\", round(mde, 3), \"\\n\")\n    }\n  })\n  \n  # Plot: Cost allocation\n  output$plotCost &lt;- renderPlot({\n    # Only show this plot if user is in \"cost\" constraint\n    if (input$constraintType == \"cost\") {\n      # Generate grid of n_C, n_T values across full plot range\n      max_n_C &lt;- input$budget/input$costC * 1.2  # Extend 20% beyond budget line\n      max_n_T &lt;- input$budget/input$costT * 1.2\n      n_C &lt;- seq(1, max_n_C, length.out = 100)\n      n_T &lt;- seq(1, max_n_T, length.out = 100)\n      \n      # Create matrix of MDE values\n      mde_matrix &lt;- matrix(NA, nrow = length(n_C), ncol = length(n_T))\n      for (i in 1:length(n_C)) {\n        for (j in 1:length(n_T)) {\n          mde_matrix[i,j] &lt;- computeMDE_from_n(n_C[i], n_T[j])\n        }\n      }\n      \n      # Find optimal allocation\n      rTest &lt;- seq(0.1, 10, 0.01)\n      mdeTest &lt;- sapply(rTest, computeMDE_ratio)\n      rOpt &lt;- rTest[which.min(mdeTest)]\n      n_C_opt &lt;- input$budget / (rOpt * input$costT + input$costC)\n      n_T_opt &lt;- rOpt * n_C_opt\n      mde_opt &lt;- computeMDE_from_n(n_C_opt, n_T_opt)\n      \n      # Create the plot\n      par(mar = c(5, 5, 4, 2))  # Increase margins for labels\n      \n      # Calculate reasonable MDE range for contours\n      min_mde &lt;- max(min(mde_matrix, na.rm = TRUE), mde_opt * 0.8)  # Don't go too far below optimal\n      max_mde &lt;- min(max(mde_matrix, na.rm = TRUE), mde_opt * 2.0)  # Don't show extremely large MDEs\n      \n      # Create evenly spaced levels, including the optimal MDE\n      contour_levels &lt;- sort(unique(c(\n        seq(min_mde, max_mde, length.out = 8),  # 8 regular levels\n        mde_opt  # Include the optimal MDE level\n      )))\n      \n      # Plot contours\n      contour(n_C, n_T, mde_matrix, \n              xlab = \"Control Group Size (n_C)\",\n              ylab = \"Treatment Group Size (n_T)\",\n              main = \"Cost-constrained Allocation\",\n              levels = contour_levels,\n              labcex = 0.8,  # Slightly smaller contour labels\n              drawlabels = TRUE)\n      \n      # Add budget constraint line\n      budget_line_n_C &lt;- seq(0, max_n_C, length.out = 100)\n      budget_line_n_T &lt;- (input$budget - input$costC * budget_line_n_C) / input$costT\n      lines(budget_line_n_C, budget_line_n_T, col = \"red\", lwd = 2)\n      \n      # Add optimal MDE curve (the contour that passes through optimal point)\n      n_C_curve &lt;- seq(1, max_n_C, length.out = 200)\n      n_T_curve &lt;- numeric(length(n_C_curve))\n      for(i in 1:length(n_C_curve)) {\n        # Find n_T that gives the optimal MDE for this n_C\n        n_T_test &lt;- seq(1, max_n_T, length.out = 200)\n        mde_test &lt;- sapply(n_T_test, function(nt) computeMDE_from_n(n_C_curve[i], nt))\n        n_T_curve[i] &lt;- n_T_test[which.min(abs(mde_test - mde_opt))]\n      }\n      lines(n_C_curve, n_T_curve, col = \"blue\", lwd = 2, lty = 2)\n      \n      # Add optimal point\n      points(n_C_opt, n_T_opt, col = \"red\", pch = 19, cex = 1.5)\n      \n      # Add legend for optimal MDE curve\n      legend(\"topright\", \n             legend = sprintf(\"Optimal MDE = %.3f\", mde_opt),\n             col = \"blue\", lwd = 2, lty = 2)\n    }\n  })\n  \n  # Render step-by-step MDE calculations\n  output$mdeCalcSteps &lt;- renderUI({\n    # Get current values\n    if (input$ratioChoice == \"optimal\") {\n      rTest &lt;- seq(0.1, 10, 0.01)\n      mdeTest &lt;- sapply(rTest, computeMDE_ratio)\n      r &lt;- rTest[which.min(mdeTest)]\n    } else {\n      r &lt;- input$ratio\n    }\n    \n    # Calculate sample sizes\n    if (input$constraintType == \"sample\") {\n      n_C &lt;- input$N / (r + 1)\n      n_T &lt;- r * n_C\n    } else {\n      n_C &lt;- input$budget / (r * input$costT + input$costC)\n      n_T &lt;- r * n_C\n    }\n    \n    # Calculate variances\n    if (input$outcomeType == \"continuous\") {\n      varT &lt;- input$sigmaT^2\n      varC &lt;- input$sigmaC^2\n      var_term &lt;- varT/n_T + varC/n_C\n    } else {\n      varT &lt;- input$pT * (1 - input$pT)\n      varC &lt;- input$pC * (1 - input$pC)\n      var_term &lt;- varT/n_T + varC/n_C\n    }\n    \n    # Critical values\n    z_alpha &lt;- qnorm(1 - input$alpha/2)\n    z_beta &lt;- qnorm(input$power)\n    \n    # Final MDE\n    mde &lt;- (z_alpha + z_beta) * sqrt(var_term)\n    \n    # Create step-by-step explanation with LaTeX\n    tagList(\n      withMathJax(),\n      \n      h5(\"1. Sample Size Calculation:\"),\n      if (input$constraintType == \"sample\") {\n        tagList(\n          p(sprintf(\"With total N = %d and ratio r = %.3f:\", input$N, r)),\n          withMathJax(sprintf(\"$$n_C = \\\\frac{N}{r + 1} = \\\\frac{%d}{%.3f + 1} = %.1f$$\", input$N, r, n_C)),\n          withMathJax(sprintf(\"$$n_T = r \\\\cdot n_C = %.3f \\\\cdot %.1f = %.1f$$\", r, n_C, n_T))\n        )\n      } else {\n        tagList(\n          p(sprintf(\"With budget = %d, c_T = %d, c_C = %d, and ratio r = %.3f:\", \n                    input$budget, input$costT, input$costC, r)),\n          withMathJax(sprintf(\"$$n_C = \\\\frac{\\\\text{budget}}{rc_T + c_C} = \\\\frac{%d}{%.3f \\\\cdot %d + %d} = %.1f$$\", \n                               input$budget, r, input$costT, input$costC, n_C)),\n          withMathJax(sprintf(\"$$n_T = r \\\\cdot n_C = %.3f \\\\cdot %.1f = %.1f$$\", r, n_C, n_T))\n        )\n      },\n      \n      h5(\"2. Variance Terms:\"),\n      if (input$outcomeType == \"continuous\") {\n        withMathJax(sprintf(\"$$\\\\sigma^2_T = %.3f, \\\\quad \\\\sigma^2_C = %.3f$$\", varT, varC))\n      } else {\n        tagList(\n          withMathJax(sprintf(\"$$\\\\text{Var}_T = p_T(1-p_T) = %.3f(1-%.3f) = %.3f$$\", input$pT, input$pT, varT)),\n          withMathJax(sprintf(\"$$\\\\text{Var}_C = p_C(1-p_C) = %.3f(1-%.3f) = %.3f$$\", input$pC, input$pC, varC))\n        )\n      },\n      withMathJax(sprintf(\"$$\\\\text{Combined variance} = \\\\frac{\\\\text{Var}_T}{n_T} + \\\\frac{\\\\text{Var}_C}{n_C} = \\\\frac{%.3f}{%.1f} + \\\\frac{%.3f}{%.1f} = %.4f$$\",\n                          varT, n_T, varC, n_C, var_term)),\n      \n      h5(\"3. Critical Values:\"),\n      withMathJax(sprintf(\"$$z_{1-\\\\alpha/2} = %.4f \\\\quad (\\\\text{for } \\\\alpha = %.3f)$$\", z_alpha, input$alpha)),\n      withMathJax(sprintf(\"$$z_{1-\\\\beta} = %.4f \\\\quad (\\\\text{for power } = %.3f)$$\", z_beta, input$power)),\n      \n      h5(\"4. Final MDE Calculation:\"),\n      withMathJax(\"$$\\\\text{MDE} = (z_{1-\\\\alpha/2} + z_{1-\\\\beta}) \\\\sqrt{\\\\frac{\\\\text{Var}_T}{n_T} + \\\\frac{\\\\text{Var}_C}{n_C}}$$\"),\n      withMathJax(sprintf(\"$$\\\\text{MDE} = (%.4f + %.4f) \\\\sqrt{%.4f} = %.4f$$\", \n                          z_alpha, z_beta, var_term, mde))\n    )\n  })\n  \n  # Render step-by-step cost allocation calculations\n  output$costCalcSteps &lt;- renderUI({\n    if (input$constraintType != \"cost\") {\n      return(p(\"Step-by-step calculations are shown when using cost constraints.\"))\n    }\n    \n    # Get current optimal values if in optimal mode\n    if (input$ratioChoice == \"optimal\") {\n      rTest &lt;- seq(0.1, 10, 0.01)\n      mdeTest &lt;- sapply(rTest, computeMDE_ratio)\n      r &lt;- rTest[which.min(mdeTest)]\n      n_C &lt;- input$budget / (r * input$costT + input$costC)\n      n_T &lt;- r * n_C\n      total_cost &lt;- input$costT * n_T + input$costC * n_C\n      \n      tagList(\n        withMathJax(),\n        h4(\"Optimal Cost Allocation:\"),\n        \n        h5(\"1. Budget Constraint:\"),\n        withMathJax(sprintf(\"$$%d = %d n_T + %d n_C$$\", \n                           input$budget, input$costT, input$costC)),\n        \n        h5(\"2. Optimal Allocation Ratio:\"),\n        withMathJax(sprintf(\"$$r = \\\\frac{n_T}{n_C} = %.3f$$\", r)),\n        \n        h5(\"3. Sample Sizes:\"),\n        withMathJax(sprintf(\"$$n_C = \\\\frac{\\\\text{budget}}{rc_T + c_C} = \\\\frac{%d}{%.3f \\\\cdot %d + %d} = %.1f$$\",\n                           input$budget, r, input$costT, input$costC, n_C)),\n        withMathJax(sprintf(\"$$n_T = r \\\\cdot n_C = %.3f \\\\cdot %.1f = %.1f$$\", \n                           r, n_C, n_T)),\n        \n        h5(\"4. Cost Verification:\"),\n        withMathJax(sprintf(\"$$\\\\text{Total cost} = c_T n_T + c_C n_C = %d \\\\cdot %.1f + %d \\\\cdot %.1f = %d$$\",\n                           input$costT, n_T, input$costC, n_C, total_cost))\n      )\n    } else {\n      n_C &lt;- input$budget / (input$ratio * input$costT + input$costC)\n      n_T &lt;- input$ratio * n_C\n      total_cost &lt;- input$costT * n_T + input$costC * n_C\n      \n      tagList(\n        withMathJax(),\n        h4(\"Manual Cost Allocation:\"),\n        \n        h5(\"1. Budget Constraint:\"),\n        withMathJax(sprintf(\"$$%d = %d n_T + %d n_C$$\", \n                           input$budget, input$costT, input$costC)),\n        \n        h5(\"2. Fixed Allocation Ratio:\"),\n        withMathJax(sprintf(\"$$r = \\\\frac{n_T}{n_C} = %.3f$$\", input$ratio)),\n        \n        h5(\"3. Sample Sizes:\"),\n        withMathJax(sprintf(\"$$n_C = \\\\frac{\\\\text{budget}}{rc_T + c_C} = \\\\frac{%d}{%.3f \\\\cdot %d + %d} = %.1f$$\",\n                           input$budget, input$ratio, input$costT, input$costC, n_C)),\n        withMathJax(sprintf(\"$$n_T = r \\\\cdot n_C = %.3f \\\\cdot %.1f = %.1f$$\", \n                           input$ratio, n_C, n_T)),\n        \n        h5(\"4. Cost Verification:\"),\n        withMathJax(sprintf(\"$$\\\\text{Total cost} = c_T n_T + c_C n_C = %d \\\\cdot %.1f + %d \\\\cdot %.1f = %d$$\",\n                           input$costT, n_T, input$costC, n_C, total_cost))\n      )\n    }\n  })\n}\n\nshinyApp(ui = ui, server = server)"
  },
  {
    "objectID": "unit-2/lec-2-2-slides.html#randomization-the-foundation-of-causal-inference",
    "href": "unit-2/lec-2-2-slides.html#randomization-the-foundation-of-causal-inference",
    "title": "Unit 2.2: Advanced Randomization Techniques",
    "section": "Randomization: The Foundation of Causal Inference",
    "text": "Randomization: The Foundation of Causal Inference\n\n\nLast session: Maximizing power through optimal experimental design\nToday: The art and science of randomization\nRandomization enables causal claims by balancing all factors:\n\nObservable characteristics\nUnobservable characteristics\nPotential outcomes\n\n\n\n\nEmphasize that randomization is what allows us to make causal claims - it’s the key distinction between experimental and observational studies."
  },
  {
    "objectID": "unit-2/lec-2-2-slides.html#why-does-randomization-work",
    "href": "unit-2/lec-2-2-slides.html#why-does-randomization-work",
    "title": "Unit 2.2: Advanced Randomization Techniques",
    "section": "Why Does Randomization Work?",
    "text": "Why Does Randomization Work?\nBecause it ensures:\n\n\nNon-zero probability condition: Everyone has a chance of treatment\nIndividualism: Independence across units\nUnconfoundedness: Balance on observed/unobserved covariates\n\n\n\\[E[Y_i(0)|D_i=1] = E[Y_i(0)|D_i=0]\\]\n\nThe untreated potential outcomes are the same in both groups!"
  },
  {
    "objectID": "unit-2/lec-2-2-slides.html#what-you-need-for-randomization",
    "href": "unit-2/lec-2-2-slides.html#what-you-need-for-randomization",
    "title": "Unit 2.2: Advanced Randomization Techniques",
    "section": "What You Need for Randomization",
    "text": "What You Need for Randomization\n\n\nSample of units: Who or what will be randomized\nAllocation ratio: How many units to each condition\nRandomization device: Physical or computational\nBaseline covariates: (for some approaches)"
  },
  {
    "objectID": "unit-2/lec-2-2-slides.html#choosing-the-unit-of-randomization",
    "href": "unit-2/lec-2-2-slides.html#choosing-the-unit-of-randomization",
    "title": "Unit 2.2: Advanced Randomization Techniques",
    "section": "Choosing the Unit of Randomization",
    "text": "Choosing the Unit of Randomization\n\n\nKey Considerations\n\nShould match observational unit when possible\nMust align with treatment delivery\nNeed to minimize spillovers\nConsider statistical power\n\n\nCommon Units\n\nIndividual: Patients, students\nCluster: Villages, clinics, schools\nTime periods: Days, weeks, shifts\nNetworks: Households, peer groups\n\n\n\nCritical trade-off: Statistical power vs. internal validity"
  },
  {
    "objectID": "unit-2/lec-2-2-slides.html#the-spillover-problem",
    "href": "unit-2/lec-2-2-slides.html#the-spillover-problem",
    "title": "Unit 2.2: Advanced Randomization Techniques",
    "section": "The Spillover Problem",
    "text": "The Spillover Problem\n\n\nSpillovers occur when treatment affects untreated units\nTypes of spillovers:\n\nDirect interaction between units\nGeneral equilibrium effects\nInformation diffusion\nResource competition\n\n\n\n\nRandomization at a higher level (clustering) can minimize unwanted spillovers."
  },
  {
    "objectID": "unit-2/lec-2-2-slides.html#measuring-spillovers-two-stage-randomization",
    "href": "unit-2/lec-2-2-slides.html#measuring-spillovers-two-stage-randomization",
    "title": "Unit 2.2: Advanced Randomization Techniques",
    "section": "Measuring Spillovers: Two-Stage Randomization",
    "text": "Measuring Spillovers: Two-Stage Randomization\n\nFirst stage: Randomize clusters to high or low treatment intensity\nSecond stage: Randomize individuals within clusters to treatment or control\nAllows measurement of:\n\nDirect treatment effects\nWithin-cluster spillovers\nBetween-cluster spillovers\n\nProvides estimates of:\n\nTotal treatment effect (direct + spillover)\nIsolated direct effect\nSpillover effect on untreated individuals"
  },
  {
    "objectID": "unit-2/lec-2-2-slides.html#classical-assignment-mechanisms",
    "href": "unit-2/lec-2-2-slides.html#classical-assignment-mechanisms",
    "title": "Unit 2.2: Advanced Randomization Techniques",
    "section": "Classical Assignment Mechanisms",
    "text": "Classical Assignment Mechanisms\n\nBernoulli Trials\nComplete Randomization\nRe-randomization\nStratified Randomization\nMatched-Pair Designs"
  },
  {
    "objectID": "unit-2/lec-2-2-slides.html#bernoulli-trials",
    "href": "unit-2/lec-2-2-slides.html#bernoulli-trials",
    "title": "Unit 2.2: Advanced Randomization Techniques",
    "section": "Bernoulli Trials",
    "text": "Bernoulli Trials\n\nSimplest approach: independent coin flips\n\\(P(Z_i = 1) = p\\) for all units \\(i\\)\n\n\n\n\nAdvantages:\n\nSimple to implement\nCan randomize as participants arrive\nNo baseline data needed\n\n\n\n\nDisadvantages:\n\nRandom group sizes\nPotential imbalance on key covariates\nImplementation vulnerability"
  },
  {
    "objectID": "unit-2/lec-2-2-slides.html#case-study-the-canadian-national-breast-screening-study",
    "href": "unit-2/lec-2-2-slides.html#case-study-the-canadian-national-breast-screening-study",
    "title": "Unit 2.2: Advanced Randomization Techniques",
    "section": "Case Study: The Canadian National Breast Screening Study",
    "text": "Case Study: The Canadian National Breast Screening Study\n\n\nMajor randomized trial evaluating mammography screening effectiveness\nUsed alternating assignment (first to treatment, second to control)\nDesign flaw: clinical breast exams conducted before randomization\nNurses and physicians could (and did) influence group assignments"
  },
  {
    "objectID": "unit-2/lec-2-2-slides.html#cnbss-randomization-failures",
    "href": "unit-2/lec-2-2-slides.html#cnbss-randomization-failures",
    "title": "Unit 2.2: Advanced Randomization Techniques",
    "section": "CNBSS: Randomization Failures",
    "text": "CNBSS: Randomization Failures\n\n\nPre-randomization examination: Detected suspicious lumps before group assignment\nSelection bias: Women with palpable lumps disproportionately assigned to mammography group\nInadequate concealment: Study staff could influence group assignments\nEvidence of manipulation: Names overwritten, identities reversed, lines skipped\n\n\n\nResult: Mammography group had 68% higher incidence of advanced cancers at baseline!"
  },
  {
    "objectID": "unit-2/lec-2-2-slides.html#cnbss-impact-on-study-validity",
    "href": "unit-2/lec-2-2-slides.html#cnbss-impact-on-study-validity",
    "title": "Unit 2.2: Advanced Randomization Techniques",
    "section": "CNBSS: Impact on Study Validity",
    "text": "CNBSS: Impact on Study Validity\n\n\nStudy reported no mortality benefit from mammography screening\nHowever: The randomization bias likely masked true benefits\nHigher-risk patients concentrated in treatment group\nControl group contamination: ~25% received mammograms outside the study\n\n\n\nBroader lesson: Compromise in randomization can fundamentally undermine study validity and lead to decades of scientific controversy"
  },
  {
    "objectID": "unit-2/lec-2-2-slides.html#complete-randomization",
    "href": "unit-2/lec-2-2-slides.html#complete-randomization",
    "title": "Unit 2.2: Advanced Randomization Techniques",
    "section": "Complete Randomization",
    "text": "Complete Randomization\n\nset.seed(072311)  # Set seed for reproducibility\n\n# Parameters\nN &lt;- 100          # Total number of units\np &lt;- 0.5          # Proportion to assign to treatment\n\n# Generate random numbers and sort\nunits &lt;- data.frame(\n  id = 1:N,\n  random_num = runif(N)\n)\nunits &lt;- units[order(units$random_num),]\n\n# Assign first p% to treatment\nunits$treatment &lt;- 0\nunits$treatment[1:(N*p)] &lt;- 1\n\n# Check resulting allocation\ntable(units$treatment)\n\n\n 0  1 \n50 50 \n\n\nEach participant has fixed probability of assignment, with total group sizes fixed in advance."
  },
  {
    "objectID": "unit-2/lec-2-2-slides.html#chance-imbalance-with-complete-randomization",
    "href": "unit-2/lec-2-2-slides.html#chance-imbalance-with-complete-randomization",
    "title": "Unit 2.2: Advanced Randomization Techniques",
    "section": "Chance Imbalance with Complete Randomization",
    "text": "Chance Imbalance with Complete Randomization\n\n\nEven with perfect implementation, covariates may be imbalanced\nExample: In a study of 722 people (NLSY data):\n\n~45% of randomizations had all covariates balanced\n~30% had one imbalanced covariate\nRemaining had multiple imbalanced covariates\n\n\n\n\nThis raises two critical questions: 1. How can we ensure better balance in design? 2. What do we do if imbalance occurs?"
  },
  {
    "objectID": "unit-2/lec-2-2-slides.html#re-randomization",
    "href": "unit-2/lec-2-2-slides.html#re-randomization",
    "title": "Unit 2.2: Advanced Randomization Techniques",
    "section": "Re-randomization",
    "text": "Re-randomization\n\n\nGenerate multiple randomizations\nKeep only those with good balance\nApproach 1: All p-values &gt; threshold (e.g., 0.05)\nApproach 2: Choose iteration with best overall balance"
  },
  {
    "objectID": "unit-2/lec-2-2-slides.html#drawbacks-of-re-randomization",
    "href": "unit-2/lec-2-2-slides.html#drawbacks-of-re-randomization",
    "title": "Unit 2.2: Advanced Randomization Techniques",
    "section": "Drawbacks of Re-randomization",
    "text": "Drawbacks of Re-randomization\n\n\nOpaque constraints: “Black box” process\nUnusual handling of outliers\nComputationally expensive\nCould run forever if criteria too strict\nStatistical inference must account for the procedure\nCannot balance on unobserved covariates"
  },
  {
    "objectID": "unit-2/lec-2-2-slides.html#stratified-block-randomization",
    "href": "unit-2/lec-2-2-slides.html#stratified-block-randomization",
    "title": "Unit 2.2: Advanced Randomization Techniques",
    "section": "Stratified (Block) Randomization",
    "text": "Stratified (Block) Randomization\n\n\n\n\n\nStratified Randomization\n\n\n\n\n\nDivide sample into strata based on covariates\nRandomize separately within each stratum\nPerfect balance on stratification variables"
  },
  {
    "objectID": "unit-2/lec-2-2-slides.html#implementing-stratified-randomization",
    "href": "unit-2/lec-2-2-slides.html#implementing-stratified-randomization",
    "title": "Unit 2.2: Advanced Randomization Techniques",
    "section": "Implementing Stratified Randomization",
    "text": "Implementing Stratified Randomization\n\nset.seed(072311)\n\n# Create example data with categorical covariates\ndata &lt;- data.frame(\n  id = 1:100,\n  gender = sample(c(\"Male\", \"Female\"), 100, replace = TRUE),\n  age_group = sample(c(\"Under 30\", \"30-50\", \"Over 50\"), 100, replace = TRUE),\n  stringsAsFactors = FALSE\n)\n\n# Create strata based on combinations of covariates\ndata$stratum &lt;- paste(data$gender, data$age_group, sep = \"_\")\n\n# Function for stratified randomization\nstratified_randomize &lt;- function(data, strata_var, p = 0.5) {\n  # Initialize assignment vector\n  assignment &lt;- rep(NA, nrow(data))\n  \n  # Get unique strata\n  strata &lt;- unique(data[[strata_var]])\n  \n  # Randomize within each stratum\n  for (s in strata) {\n    # Get indices for this stratum\n    indices &lt;- which(data[[strata_var]] == s)\n    n_stratum &lt;- length(indices)\n    \n    # Calculate number to assign to treatment\n    n_treat &lt;- round(n_stratum * p)\n    \n    # Ensure at least one in each group if possible\n    if (n_stratum &gt; 1) {\n      n_treat &lt;- min(max(n_treat, 1), n_stratum - 1)\n    } else {\n      n_treat &lt;- sample(0:1, 1)  # Random for singletons\n    }\n    \n    # Perform randomization within stratum\n    treat_indices &lt;- sample(indices, n_treat)\n    assignment[indices] &lt;- 0\n    assignment[treat_indices] &lt;- 1\n  }\n  \n  return(assignment)\n}\n\n# Apply stratified randomization\ndata$treatment &lt;- stratified_randomize(data, \"stratum\")\n\n# Check balance by strata\ntable(data$stratum, data$treatment)\n\n                 \n                   0  1\n  Female_30-50    11 10\n  Female_Over 50   8  8\n  Female_Under 30  7  7\n  Male_30-50       9  9\n  Male_Over 50     7  8\n  Male_Under 30    8  8\n\n\nThis code creates strata from combinations of gender and age group, then randomizes within each stratum."
  },
  {
    "objectID": "unit-2/lec-2-2-slides.html#selecting-stratification-variables",
    "href": "unit-2/lec-2-2-slides.html#selecting-stratification-variables",
    "title": "Unit 2.2: Advanced Randomization Techniques",
    "section": "Selecting Stratification Variables",
    "text": "Selecting Stratification Variables\n\n\nDiscrete variables are easier to implement\nPrioritize variables that strongly predict outcomes\nInclude variables where heterogeneous effects are expected\nBe careful of too many strata - causes “small cell” problems\n\n\n\nHandling “misfits” (when strata size not divisible by treatments):\n\nRemove units randomly\nCreate separate strata for misfits\nUse different randomization approach for misfits"
  },
  {
    "objectID": "unit-2/lec-2-2-slides.html#matched-pairs-randomization",
    "href": "unit-2/lec-2-2-slides.html#matched-pairs-randomization",
    "title": "Unit 2.2: Advanced Randomization Techniques",
    "section": "Matched Pairs Randomization",
    "text": "Matched Pairs Randomization\n\n\nCreate pairs of similar units\nRandomize one to treatment within each pair\nLike stratification taken to the extreme\nPerfect for continuous covariates"
  },
  {
    "objectID": "unit-2/lec-2-2-slides.html#implementing-matched-pairs",
    "href": "unit-2/lec-2-2-slides.html#implementing-matched-pairs",
    "title": "Unit 2.2: Advanced Randomization Techniques",
    "section": "Implementing Matched Pairs",
    "text": "Implementing Matched Pairs\n\nlibrary(optmatch)  # For optimal matching\n\n# Generate example data\nset.seed(072311)\ndata &lt;- data.frame(\n  id = 1:100,\n  age = rnorm(100, 45, 10),\n  income = rnorm(100, 50000, 15000),\n  health_score = rnorm(100, 70, 15)\n)\n\n# Create distance matrix based on covariates\nX &lt;- as.matrix(data[, c(\"age\", \"income\", \"health_score\")])\nX_scaled &lt;- scale(X)  # Standardize to have equal importance\ndist_matrix &lt;- dist(X_scaled)\n\n# Create optimal matches\nmatches &lt;- pairmatch(dist_matrix, data = data)\ndata$pair_id &lt;- matches\n\n# Randomize within pairs\npair_randomize &lt;- function(data, pair_var) {\n  # Initialize assignment vector\n  assignment &lt;- rep(NA, nrow(data))\n  \n  # Get unique pairs\n  pairs &lt;- unique(data[[pair_var]])\n  pairs &lt;- pairs[!is.na(pairs)]  # Remove NA pairs\n  \n  # Randomize within each pair\n  for (p in pairs) {\n    # Get indices for this pair\n    indices &lt;- which(data[[pair_var]] == p)\n    \n    # Skip if not exactly 2 units\n    if (length(indices) != 2) next\n    \n    # Randomly assign one to treatment\n    treat_index &lt;- sample(indices, 1)\n    assignment[indices] &lt;- 0\n    assignment[treat_index] &lt;- 1\n  }\n  \n  return(assignment)\n}\n\n# Apply pair randomization\ndata$treatment &lt;- pair_randomize(data, \"pair_id\")\n\nThis code matches participants based on age, income, and health score, then randomizes one member of each pair to treatment."
  },
  {
    "objectID": "unit-2/lec-2-2-slides.html#balance-tests-approaches-for-verification",
    "href": "unit-2/lec-2-2-slides.html#balance-tests-approaches-for-verification",
    "title": "Unit 2.2: Advanced Randomization Techniques",
    "section": "Balance Tests: Approaches for Verification",
    "text": "Balance Tests: Approaches for Verification\n\n\nIndividual covariate tests: t-tests, Chi-square tests\nJoint omnibus tests: F-tests testing multiple covariates simultaneously\nRegression-based tests: Regress treatment on covariates\nStandardized differences: Sample size-independent assessment"
  },
  {
    "objectID": "unit-2/lec-2-2-slides.html#standardized-differences-for-large-samples",
    "href": "unit-2/lec-2-2-slides.html#standardized-differences-for-large-samples",
    "title": "Unit 2.2: Advanced Randomization Techniques",
    "section": "Standardized Differences for Large Samples",
    "text": "Standardized Differences for Large Samples\n\nProblem with p-values: With large samples, tiny imbalances become “significant”\nSolution: Standardized mean difference (SMD)\n\n\\[SMD = \\frac{\\bar{X}_{treatment} - \\bar{X}_{control}}{\\sqrt{\\frac{s^2_{treatment} + s^2_{control}}{2}}}\\]\n\n\nScale-free measure of imbalance\nIndependent of sample size\nRule of thumb: |SMD| &lt; 0.1 is negligible imbalance"
  },
  {
    "objectID": "unit-2/lec-2-2-slides.html#the-table-1-debate",
    "href": "unit-2/lec-2-2-slides.html#the-table-1-debate",
    "title": "Unit 2.2: Advanced Randomization Techniques",
    "section": "The “Table 1” Debate",
    "text": "The “Table 1” Debate\n\n\nFor Including\n\nTransparency\nDemonstrates randomization effectiveness\nProvides context for readers\nShows extent of imbalance\n\n\nAgainst Including\n\nRandomization guarantees balance in expectation\nOveremphasis on statistically significant differences\nCan lead to inappropriate adjustments\nJournal space limitations\n\n\n\nCompromise: Report balance without p-values, use standardized differences"
  },
  {
    "objectID": "unit-2/lec-2-2-slides.html#design-based-inference",
    "href": "unit-2/lec-2-2-slides.html#design-based-inference",
    "title": "Unit 2.2: Advanced Randomization Techniques",
    "section": "Design-Based Inference",
    "text": "Design-Based Inference\n\n\nRandomization creates the foundation for inference\nFisher’s approach: Test sharp null that treatment has no effect on any unit\nRandomization distribution: Generate distribution of test statistics under all possible randomizations\nPermutation tests: Compare observed statistic to randomization distribution"
  },
  {
    "objectID": "unit-2/lec-2-2-slides.html#complex-experimental-designs",
    "href": "unit-2/lec-2-2-slides.html#complex-experimental-designs",
    "title": "Unit 2.2: Advanced Randomization Techniques",
    "section": "Complex Experimental Designs",
    "text": "Complex Experimental Designs\n\nUnits of Randomization & Spillovers\nCluster Randomization\nFactorial Designs\nFractional Factorial Designs\n\nWithin-Subject vs. Between-Subject\nRandomized Phase-in\nAdaptive Designs\nVariable Treatment Probabilities"
  },
  {
    "objectID": "unit-2/lec-2-2-slides.html#factorial-designs-testing-multiple-treatments",
    "href": "unit-2/lec-2-2-slides.html#factorial-designs-testing-multiple-treatments",
    "title": "Unit 2.2: Advanced Randomization Techniques",
    "section": "Factorial Designs: Testing Multiple Treatments",
    "text": "Factorial Designs: Testing Multiple Treatments\n\n\n\n\n\nIntervention B\nNo Intervention A\nIntervention A\n\n\n\n\nNo\nControl\nA only\n\n\nYes\nB only\nA and B\n\n\n\n\n\n\nTest multiple treatments simultaneously\nEstimate main effects AND interactions\nEfficient use of resources\nExample: Two interventions with four groups\n\nNo intervention (control)\nIntervention A only\nIntervention B only\nBoth A and B"
  },
  {
    "objectID": "unit-2/lec-2-2-slides.html#randomized-phase-in-designs",
    "href": "unit-2/lec-2-2-slides.html#randomized-phase-in-designs",
    "title": "Unit 2.2: Advanced Randomization Techniques",
    "section": "Randomized Phase-in Designs",
    "text": "Randomized Phase-in Designs\n\n\nAll units eventually receive treatment\nRandomize the timing of treatment\nAdvantages:\n\nBetter compliance (everyone gets treatment eventually)\nClear for partners with resource constraints\nPolitically appealing compromise\n\n\n\n\nBut watch out for:\n\nAnticipation effects\nLimited long-term impact measurement"
  },
  {
    "objectID": "unit-2/lec-2-2-slides.html#adaptive-randomization",
    "href": "unit-2/lec-2-2-slides.html#adaptive-randomization",
    "title": "Unit 2.2: Advanced Randomization Techniques",
    "section": "Adaptive Randomization",
    "text": "Adaptive Randomization\n\n\nUpdates assignment probabilities based on accumulated data\nBalances:\n\nExploration: Learning which treatment works best\nExploitation: Assigning more units to better treatments\n\nKey ethical advantage: Fewer participants receive inferior treatments\n\n\n\nNote: We’ll cover adaptive designs in more detail in a future lecture"
  },
  {
    "objectID": "unit-2/lec-2-2-slides.html#variable-treatment-probabilities",
    "href": "unit-2/lec-2-2-slides.html#variable-treatment-probabilities",
    "title": "Unit 2.2: Advanced Randomization Techniques",
    "section": "Variable Treatment Probabilities",
    "text": "Variable Treatment Probabilities\n\n\nUnits can have different probabilities of assignment\nRequirements:\n\nProbabilities must be between 0 and 1\nProbabilities must be known\n\nAnalysis using inverse probability weighting:\n\n\n\\[\\hat{\\tau}_{IPW} = \\frac{1}{N}\\sum_{i=1}^N \\frac{D_i Y_i}{p_i} - \\frac{(1-D_i)Y_i}{1-p_i}\\]"
  },
  {
    "objectID": "unit-2/lec-2-2-slides.html#practical-implementation",
    "href": "unit-2/lec-2-2-slides.html#practical-implementation",
    "title": "Unit 2.2: Advanced Randomization Techniques",
    "section": "Practical Implementation",
    "text": "Practical Implementation\n\n\nCreate single entry per randomization unit\nSort file in reproducible way\nSet and preserve random seed\nAssign treatments\nSave assignments securely\nTest balance extensively\n\n\nAlways document your randomization procedure thoroughly!"
  },
  {
    "objectID": "unit-2/lec-2-2-slides.html#ethical-considerations",
    "href": "unit-2/lec-2-2-slides.html#ethical-considerations",
    "title": "Unit 2.2: Advanced Randomization Techniques",
    "section": "Ethical Considerations",
    "text": "Ethical Considerations\n\n\nLong-term benefits vs. short-term resource distribution\nEquity in who receives potentially beneficial treatments\nTransparency with participants about randomization\nMinimize harm from potentially ineffective interventions"
  },
  {
    "objectID": "unit-2/lec-2-2-slides.html#combining-randomization-approaches",
    "href": "unit-2/lec-2-2-slides.html#combining-randomization-approaches",
    "title": "Unit 2.2: Advanced Randomization Techniques",
    "section": "Combining Randomization Approaches",
    "text": "Combining Randomization Approaches\n\n\nStratify on discrete variables\nRe-randomize on continuous variables within strata\nUse matched pairs for important continuous variables\nAdapt to your specific context and constraints\n\n\n\nThere is no one-size-fits-all approach!"
  },
  {
    "objectID": "unit-2/lec-2-2-slides.html#key-takeaways",
    "href": "unit-2/lec-2-2-slides.html#key-takeaways",
    "title": "Unit 2.2: Advanced Randomization Techniques",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\n\nChoose randomization unit carefully (individual vs. cluster)\nBalance variables that strongly predict outcomes\nUse re-randomization, stratification, or matching when feasible\nDocument your randomization procedure completely\nTest balance appropriately for your sample size\nAccount for your randomization procedure in analysis"
  },
  {
    "objectID": "unit-2/lec-2-2-slides.html#which-method-when",
    "href": "unit-2/lec-2-2-slides.html#which-method-when",
    "title": "Unit 2.2: Advanced Randomization Techniques",
    "section": "Which Method When?",
    "text": "Which Method When?\n\n\n\nApproach\nWhen to Use\nKey Consideration\n\n\n\n\nSimple\nLarge samples\nSimplicity\n\n\nStratified\nStrong predictors known\nNumber of strata\n\n\nMatched-Pair\nSmall samples\nFinding good matches\n\n\nRe-randomization\nBalance is critical\nComplexity of inference\n\n\nCluster\nGroup-level intervention\nICC and number of clusters"
  },
  {
    "objectID": "unit-2/lec-2-2-slides.html#next-time-machine-learning-for-causal-inference",
    "href": "unit-2/lec-2-2-slides.html#next-time-machine-learning-for-causal-inference",
    "title": "Unit 2.2: Advanced Randomization Techniques",
    "section": "Next Time: Machine Learning for Causal Inference",
    "text": "Next Time: Machine Learning for Causal Inference\nWe’ll explore how modern prediction algorithms can enhance our ability to:\n\n\nEstimate average treatment effects more precisely\nDiscover heterogeneous treatment effects\nSelect optimal treatments for individuals"
  },
  {
    "objectID": "unit-2/lec-2-2-slides.html#questions",
    "href": "unit-2/lec-2-2-slides.html#questions",
    "title": "Unit 2.2: Advanced Randomization Techniques",
    "section": "Questions?",
    "text": "Questions?"
  },
  {
    "objectID": "unit-2/lec-2-2-slides.html#classical-assignment-mechanisms-framework",
    "href": "unit-2/lec-2-2-slides.html#classical-assignment-mechanisms-framework",
    "title": "Unit 2.2: Advanced Randomization Techniques",
    "section": "Classical Assignment Mechanisms Framework",
    "text": "Classical Assignment Mechanisms Framework\n\n\n\n\n\nflowchart TD\n    %% Top nodes - conditions\n    A[\"Non-zero probability condition\"]:::gold\n    B[\"Individualism condition\"]:::gold\n    C[\"Unconfoundedness condition\"]:::gold\n    \n    %% Middle node - mechanisms\n    D{{\"Classical Random Assignment Mechanisms\"}}:::tarHeelRed\n    \n    %% Assignment types\n    E[\"Bernoulli Trial\"]:::carolinaBlue\n    F[\"Complete Randomized\\nExperiment (CRE)\"]:::carolinaBlue\n    G[\"Stratified Randomization\"]:::carolinaBlue\n    H[\"Rerandomization\"]:::carolinaBlue\n    I[\"Matched Pairs\"]:::carolinaBlue\n    \n    %% Bottom node - inference\n    J{{\"Design-conscious Inference\"}}:::uncGreen\n    \n    %% Connections\n    A --&gt; D\n    B --&gt; D\n    C --&gt; D\n    \n    D --&gt; E\n    D --&gt; F\n    D --&gt; G\n    D --&gt; H\n    D --&gt; I\n    \n    E --&gt; J\n    F --&gt; J\n    G --&gt; J\n    H --&gt; J\n    I --&gt; J\n    \n    %% UNC Brand Colors\n    classDef gold fill:#FFD100,stroke:#13294B,stroke-width:1px,color:#13294B\n    classDef tarHeelRed fill:#DC143C,stroke:#13294B,stroke-width:1px,color:#FFFFFF\n    classDef carolinaBlue fill:#4B9CD3,stroke:#13294B,stroke-width:1px,color:#FFFFFF\n    classDef uncGreen fill:#8DB434,stroke:#13294B,stroke-width:1px,color:#FFFFFF"
  },
  {
    "objectID": "unit-2/lec-2-2-slides.html#cluster-randomization",
    "href": "unit-2/lec-2-2-slides.html#cluster-randomization",
    "title": "Unit 2.2: Advanced Randomization Techniques",
    "section": "Cluster Randomization",
    "text": "Cluster Randomization\n\n\nDefinition: Randomize groups (clusters) rather than individuals\nCommon clusters: Schools, clinics, villages, neighborhoods\nKey parameter: Intraclass correlation coefficient (ICC)\nDesign effect: \\(DE = 1 + (m-1) \\times ICC\\)\n\n\\(m\\) = average cluster size\nIncreases required sample size\n\n\n\n\nAnalysis must account for clustering! * Cluster-robust standard errors * Mixed-effects models * GEE approaches"
  },
  {
    "objectID": "unit-2/lec-2-2-slides.html#random-sampling-vs.-random-assignment",
    "href": "unit-2/lec-2-2-slides.html#random-sampling-vs.-random-assignment",
    "title": "Unit 2.2: Advanced Randomization Techniques",
    "section": "Random Sampling vs. Random Assignment",
    "text": "Random Sampling vs. Random Assignment\n\nRandom Sampling vs. Random Assignment"
  },
  {
    "objectID": "unit-2/lec-2-2-slides.html#re-randomization-code",
    "href": "unit-2/lec-2-2-slides.html#re-randomization-code",
    "title": "Unit 2.2: Advanced Randomization Techniques",
    "section": "Re-randomization Code",
    "text": "Re-randomization Code\n\nbalance_check &lt;- function(data, treatment_var, balance_vars) {\n  # Initialize minimum p-value\n  min_pval &lt;- 1\n  \n  # Check each covariate\n  for (var in balance_vars) {\n    # T-test for continuous variables\n    t_result &lt;- t.test(data[[var]] ~ data[[treatment_var]])\n    min_pval &lt;- min(min_pval, t_result$p.value)\n  }\n  \n  return(min_pval)\n}\n\n# Re-randomization function\nrerandomize &lt;- function(data, p_threshold = 0.1, max_attempts = 1000) {\n  n &lt;- nrow(data)\n  n_treat &lt;- floor(n * 0.5)  # 50% to treatment\n  attempts &lt;- 0\n  \n  repeat {\n    # Generate a new randomization\n    treatment &lt;- rep(0, n)\n    treatment[sample(1:n, n_treat)] &lt;- 1\n    data$treatment &lt;- treatment\n    \n    # Check balance\n    pval &lt;- balance_check(data, \"treatment\", c(\"age\", \"income\", \"education\"))\n    \n    attempts &lt;- attempts + 1\n    \n    # Accept if p-value threshold met or max attempts reached\n    if (pval &gt; p_threshold || attempts &gt;= max_attempts) {\n      break\n    }\n  }\n  \n  return(list(treatment = data$treatment, attempts = attempts, min_pval = pval))\n}\n\nThis function repeats randomization until finding one where all p-values exceed our threshold."
  },
  {
    "objectID": "unit-2/lec-2-2-slides.html#coming-up-machine-learning-for-causal-inference",
    "href": "unit-2/lec-2-2-slides.html#coming-up-machine-learning-for-causal-inference",
    "title": "Unit 2.2: Advanced Randomization Techniques",
    "section": "Coming Up: Machine Learning for Causal Inference",
    "text": "Coming Up: Machine Learning for Causal Inference\nWe’ll explore how modern prediction algorithms can enhance our ability to:\n\n\nEstimate average treatment effects more precisely\nDiscover heterogeneous treatment effects\nSelect optimal treatments for individuals"
  }
]