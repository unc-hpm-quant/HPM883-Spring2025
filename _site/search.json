[
  {
    "objectID": "peer-review.html",
    "href": "peer-review.html",
    "title": "Peer Review Guide",
    "section": "",
    "text": "Peer review is a cornerstone of academic publishing and the scientific process. As a PhD student or early-career researcher, being asked to review a manuscript is a sign of your growing expertise and an important responsibility. It’s also a fantastic learning opportunity. This guide provides practical steps and tips for writing thoughtful, constructive, and high-quality peer reviews.\n\nThe Purpose and Principles of Good Peer Review\nBefore diving in, remember the goals:\n\nImprove the Manuscript: Your primary role is to provide constructive feedback to help the authors strengthen their work.\nAdvise the Editor: Your review helps the journal editor decide whether the manuscript is suitable for publication (after potential revisions).\nUphold Standards: You act as a gatekeeper for scientific rigor, clarity, and significance in your field.\n\nCore Principles:\n\nBe Constructive: Focus on how the paper can be improved.\nBe Prompt: Respond to invitations quickly and submit your review on time.\nBe Fair and Unbiased: Evaluate the work objectively based on its merits, not on the authors’ reputation or institution. Acknowledge your own potential biases.\nBe Specific: Ground your comments in specific examples from the text (use page/line numbers).\nBe Respectful: Maintain a professional and courteous tone, even when critical. Address the work, not the authors personally.\nMaintain Confidentiality: Treat the manuscript and your review as confidential documents.\n\n\n\nStep 1: Before You Accept the Invitation\n\nExpertise: Do you have sufficient knowledge of the topic and methods to evaluate the manuscript critically? It’s okay if you aren’t an expert in every aspect, but you should be comfortable with the core subject matter. Don’t be afraid to decline if it’s truly outside your area.\nTime: Can you dedicate sufficient time (typically 3-6 hours) to provide a thorough review by the deadline? Rushing leads to poor reviews.\nConflicts of Interest (COI): Do you have any potential COIs? (e.g., collaborating with the authors recently, working on a very similar project, direct professional or personal conflicts). If unsure, declare it to the editor.\n\n\n\nStep 2: Reading the Manuscript – A Multi-Pass Approach\nDon’t just read it once. Try this:\n\nFirst Pass (Big Picture): Read the abstract, introduction, and conclusion. Skim the rest. Ask yourself:\n\nWhat is the main research question or objective?\nWhat are the key findings?\nIs the study potentially significant and original?\nDoes the overall structure make sense? Is it easy to follow the argument?\n\nSecond Pass (Deep Dive): Read the entire manuscript carefully, section by section. Take detailed notes. Pay attention to:\n\nIntroduction: Is the background adequate? Is the research question/hypothesis clearly stated and well-motivated? Is the gap in the literature clearly identified?\nMethods: Are the methods appropriate for the research question? Are they described in enough detail for replication (consider data sources, variable definitions, statistical/analytical techniques, ethical approvals)? Are there obvious flaws in the design or analysis plan? For quantitative papers like those in health economics or using ML: Check model specification, assumptions, robustness checks, potential endogeneity, etc.\nResults: Are the findings presented clearly and logically (using text, tables, figures)? Do the results directly address the research question? Are the tables and figures well-designed and easy to understand? Are statistical results reported appropriately?\nDiscussion: Do the authors interpret the results correctly? Do they link back to the research question and literature? Are limitations acknowledged adequately? Are the conclusions justified by the results? Is the significance/contribution clearly articulated?\nOverall: Is the writing clear, concise, and professional? Are there major issues with grammar or style that impede understanding? Are references appropriate and reasonably comprehensive?\n\n\n\n\nStep 3: Structuring Your Review\nOrganize your feedback clearly for both the editor and the authors. A standard structure works well:\n\nBrief Summary: Start with 2-3 sentences summarizing the paper’s topic, main approach, and key findings. This shows you understood the core message.\nOverall Impression & Recommendation: State your overall assessment (e.g., interesting study with potential, significant flaws, solid contribution). Then, clearly state your recommendation (Accept, Minor Revision, Major Revision, Reject – see below) and briefly justify it based on your main points.\nMajor Concerns: List the most significant issues that must be addressed for the paper to be potentially publishable. These often relate to:\n\nSerious methodological flaws (design, analysis, data)\nLack of originality or significance\nConclusions not supported by the data\nMajor gaps in the literature review or discussion\nFundamental lack of clarity\nNumber these points for easy reference. Be specific and explain why it’s a concern and, if possible, suggest how it might be addressed (without being overly prescriptive).\n\nMinor Concerns: List less critical issues that would improve the paper but aren’t fundamental flaws. Examples include:\n\nNeed for clarification in specific sections\nSuggestions for additional, non-essential analyses or discussion points\nMinor errors in tables/figures\nTypos or grammatical errors (mention patterns rather than listing every single one, unless they significantly hinder understanding).\nNumber these points too.\n\n(Optional) Confidential Comments to the Editor: Use this section for comments you don’t want the authors to see (e.g., concerns about potential plagiarism or ethical issues, frank assessment of significance/novelty if you feel it differs from your comments to the authors).\n\n\n\nStep 4: Writing Constructive Feedback\n\nBalance: Acknowledge strengths as well as weaknesses. Start with a positive comment if possible.\nSpecificity: Instead of “The methods are unclear,” say “The description of the sample selection process on page 5 lacks detail regarding exclusion criteria.”\nEvidence: Refer to specific page numbers, paragraphs, figures, or tables.\nTone: Be professional, polite, and objective. Avoid sarcasm, dismissive language, or personal attacks. Frame critiques as suggestions for improvement.\n\nInstead of: “The authors clearly don’t understand X.”\nTry: “The discussion of X could be strengthened by considering…” or “It was unclear how the authors accounted for Y; clarification is needed.”\n\nActionability: Suggest ways forward where possible, but respect the authors’ autonomy. “The authors might consider…” or “Perhaps clarifying X would help…”\n\n\n\nStep 5: Making Your Recommendation\nAlign your recommendation with the severity of the issues raised:\n\nAccept: Rare for an initial submission. Means the paper is publishable as is, perhaps with only very minor copyediting.\nMinor Revision: The paper is fundamentally sound but needs small improvements (clarifications, minor analyses, better figures). You are confident the authors can address these easily.\nMajor Revision: The paper has potential but requires substantial changes (significant re-analysis, major rewriting of sections, addressing key methodological concerns). The revisions might not be successful, requiring re-review.\nReject: The paper has fundamental flaws (poor methodology, lack of novelty/significance, conclusions unsupported) that cannot be realistically fixed through revision, or the topic is inappropriate for the journal.\n\n\n\nWhy Bother? Benefits for You\nEngaging in peer review:\n\nSharpens Your Critical Thinking: Evaluating others’ work improves your own research and writing skills.\nKeeps You Current: You see cutting-edge research (often before publication).\nBuilds Your Network & Reputation: Editors notice thoughtful reviewers.\nContributes to Your Field: It’s a vital part of academic citizenship.\nImproves Your Own Submissions: Understanding the review process from the other side helps you anticipate reviewer comments on your own papers.\n\n\n\nFinal Tips for New Reviewers\n\nCheck Journal Guidelines: Many journals have specific instructions or criteria for reviewers.\nDon’t Be Afraid to Ask: If you’re unsure about something, ask a mentor or senior colleague for advice (without breaking confidentiality about the manuscript’s specifics).\nIt’s Okay to Be Critical: Your job is to identify weaknesses. Just ensure your criticism is constructive and fair.\nProofread Your Review: Ensure your comments are clear, concise, and professional."
  },
  {
    "objectID": "unit-2/lec-2-2.html",
    "href": "unit-2/lec-2-2.html",
    "title": "Unit 2.2: Randomization Techniques",
    "section": "",
    "text": "Building on our previous discussion of optimal experimental design where we focused on maximizing statistical power under various constraints, today we turn our attention to the art and science of randomization itself. Randomization is the cornerstone of causal inference in experimental research, enabling us to make causal claims by balancing both observable and unobservable characteristics between treatment and control groups. Whereas observational studies must rely on often-questionable assumptions about selection mechanisms, properly randomized experiments provide a foundation for causal inference that is far more credible.\nThe power of randomization comes from its ability to create groups that are, in expectation, identical on all characteristics—not just those we can observe and measure, but also on unobservable factors that might influence outcomes. This property allows us to attribute any differences in outcomes between treatment and control groups to the treatment itself, rather than to pre-existing differences between groups.\n\nRandomization works because it satisfies three essential conditions:\n\n\nNon-zero probability condition: Each unit has a positive probability of receiving any treatment assignment.\n\nIndividualism: The assignment of one unit doesn’t depend on the assignments of other units.\n\nUnconfoundedness: The treatment assignment is independent of potential outcomes.\n\nWhen these conditions are met, we can write:\n\\[E[Y_i(0)|D_i=1] = E[Y_i(0)|D_i=0]\\]\nThis equation states that the expected untreated potential outcome for those in the treatment group equals the expected untreated potential outcome for those in the control group. In other words, the groups are balanced on the counterfactual outcome we never get to observe for the treatment group. This balance on unobservables is the key to establishing causality.\n\nBefore discussing specific randomization methods, let’s identify what’s generally required to implement randomization:\n\n\nSample of units: The individuals, clusters, or entities to be randomized\n\nAllocation ratio: The proportion of units to assign to each treatment condition\n\nRandomization device: A physical or computational mechanism to generate random assignments\n\nBaseline covariates: (For some approaches) Information on characteristics to balance across groups\n\n\n\nRandom Sampling vs. Random Assignment\n\nIt’s important to distinguish between two distinct concepts that are sometimes confused:\n\n\nRandom sampling: The process of selecting units from a population so that each unit has a known probability of selection\n\nRandom assignment: The process of allocating units to treatment conditions through a random process\n\nRandom sampling helps with external validity (generalizability), while random assignment helps with internal validity (causal inference). In many experiments, we don’t have a random sample from the population, but we still randomize treatment assignment within our convenience sample.\n\n\n\n\n\n\nflowchart TD\n    %% Top nodes - conditions\n    A[\"Non-zero probability condition\"]:::gold\n    B[\"Individualism condition\"]:::gold\n    C[\"Unconfoundedness condition\"]:::gold\n    \n    %% Middle node - mechanisms\n    subgraph D[Classical Random Assignment Mechanisms]\n        F[\"Bernoulli Trial\"]:::carolinaBlue\n        G[\"Complete Randomized\\nExperiment (CRE)\"]:::carolinaBlue\n        H[\"Stratified Randomization\"]:::carolinaBlue\n        I[\"Rerandomization\"]:::carolinaBlue\n        J[\"Matched Pairs\"]:::carolinaBlue\n    end\n    subgraph E[Complex Experimental Designs]\n        K[\"Blocking\"]:::carolinaBlue\n        L[\"Covariate-adaptive Randomization\"]:::carolinaBlue\n        M[\"Minimization\"]:::carolinaBlue\n    end\n    \n    %% Bottom node - inference\n    N{{\"Design-conscious Inference\"}}:::uncGreen\n    \n    %% Connections\n    A --&gt; D\n    B --&gt; D\n    C --&gt; D\n    A --&gt; E\n    B --&gt; E\n    C --&gt; E\n    D --&gt; N\n    E --&gt; N\n\n    %% UNC Brand Colors\n    classDef gold fill:#FFD100,stroke:#13294B,stroke-width:1px,color:#13294B\n    classDef lightGrey fill:#F7F7F7,stroke:#13294B,stroke-width:1px,color:#13294B\n    classDef carolinaBlue fill:#4B9CD3,stroke:#13294B,stroke-width:1px,color:#FFFFFF\n    classDef uncGreen fill:#8DB434,stroke:#13294B,stroke-width:1px,color:#13294B\n    \n    %% Apply lightGrey style to subgraphs\n    style D fill:#F7F7F7,stroke:#13294B,stroke-width:1px,color:#13294B\n    style E fill:#F7F7F7,stroke:#13294B,stroke-width:1px,color:#13294B\n\n\n\n\n\n\n\nThere are five primary approaches to random assignment, each with distinct advantages and disadvantages:\n\nBernoulli trials\nComplete randomization\nRe-randomization\nStratified randomization\nMatched-pair designs\n\nLet’s examine each approach in detail."
  },
  {
    "objectID": "unit-2/lec-2-2.html#introduction",
    "href": "unit-2/lec-2-2.html#introduction",
    "title": "Unit 2.2: Randomization Techniques",
    "section": "",
    "text": "Building on our previous discussion of optimal experimental design where we focused on maximizing statistical power under various constraints, today we turn our attention to the art and science of randomization itself. Randomization is the cornerstone of causal inference in experimental research, enabling us to make causal claims by balancing both observable and unobservable characteristics between treatment and control groups. Whereas observational studies must rely on often-questionable assumptions about selection mechanisms, properly randomized experiments provide a foundation for causal inference that is far more credible.\nThe power of randomization comes from its ability to create groups that are, in expectation, identical on all characteristics—not just those we can observe and measure, but also on unobservable factors that might influence outcomes. This property allows us to attribute any differences in outcomes between treatment and control groups to the treatment itself, rather than to pre-existing differences between groups.\n\nRandomization works because it satisfies three essential conditions:\n\n\nNon-zero probability condition: Each unit has a positive probability of receiving any treatment assignment.\n\nIndividualism: The assignment of one unit doesn’t depend on the assignments of other units.\n\nUnconfoundedness: The treatment assignment is independent of potential outcomes.\n\nWhen these conditions are met, we can write:\n\\[E[Y_i(0)|D_i=1] = E[Y_i(0)|D_i=0]\\]\nThis equation states that the expected untreated potential outcome for those in the treatment group equals the expected untreated potential outcome for those in the control group. In other words, the groups are balanced on the counterfactual outcome we never get to observe for the treatment group. This balance on unobservables is the key to establishing causality.\n\nBefore discussing specific randomization methods, let’s identify what’s generally required to implement randomization:\n\n\nSample of units: The individuals, clusters, or entities to be randomized\n\nAllocation ratio: The proportion of units to assign to each treatment condition\n\nRandomization device: A physical or computational mechanism to generate random assignments\n\nBaseline covariates: (For some approaches) Information on characteristics to balance across groups\n\n\n\nRandom Sampling vs. Random Assignment\n\nIt’s important to distinguish between two distinct concepts that are sometimes confused:\n\n\nRandom sampling: The process of selecting units from a population so that each unit has a known probability of selection\n\nRandom assignment: The process of allocating units to treatment conditions through a random process\n\nRandom sampling helps with external validity (generalizability), while random assignment helps with internal validity (causal inference). In many experiments, we don’t have a random sample from the population, but we still randomize treatment assignment within our convenience sample.\n\n\n\n\n\n\nflowchart TD\n    %% Top nodes - conditions\n    A[\"Non-zero probability condition\"]:::gold\n    B[\"Individualism condition\"]:::gold\n    C[\"Unconfoundedness condition\"]:::gold\n    \n    %% Middle node - mechanisms\n    subgraph D[Classical Random Assignment Mechanisms]\n        F[\"Bernoulli Trial\"]:::carolinaBlue\n        G[\"Complete Randomized\\nExperiment (CRE)\"]:::carolinaBlue\n        H[\"Stratified Randomization\"]:::carolinaBlue\n        I[\"Rerandomization\"]:::carolinaBlue\n        J[\"Matched Pairs\"]:::carolinaBlue\n    end\n    subgraph E[Complex Experimental Designs]\n        K[\"Blocking\"]:::carolinaBlue\n        L[\"Covariate-adaptive Randomization\"]:::carolinaBlue\n        M[\"Minimization\"]:::carolinaBlue\n    end\n    \n    %% Bottom node - inference\n    N{{\"Design-conscious Inference\"}}:::uncGreen\n    \n    %% Connections\n    A --&gt; D\n    B --&gt; D\n    C --&gt; D\n    A --&gt; E\n    B --&gt; E\n    C --&gt; E\n    D --&gt; N\n    E --&gt; N\n\n    %% UNC Brand Colors\n    classDef gold fill:#FFD100,stroke:#13294B,stroke-width:1px,color:#13294B\n    classDef lightGrey fill:#F7F7F7,stroke:#13294B,stroke-width:1px,color:#13294B\n    classDef carolinaBlue fill:#4B9CD3,stroke:#13294B,stroke-width:1px,color:#FFFFFF\n    classDef uncGreen fill:#8DB434,stroke:#13294B,stroke-width:1px,color:#13294B\n    \n    %% Apply lightGrey style to subgraphs\n    style D fill:#F7F7F7,stroke:#13294B,stroke-width:1px,color:#13294B\n    style E fill:#F7F7F7,stroke:#13294B,stroke-width:1px,color:#13294B\n\n\n\n\n\n\n\nThere are five primary approaches to random assignment, each with distinct advantages and disadvantages:\n\nBernoulli trials\nComplete randomization\nRe-randomization\nStratified randomization\nMatched-pair designs\n\nLet’s examine each approach in detail."
  },
  {
    "objectID": "unit-2/lec-2-2.html#bernoulli-trials",
    "href": "unit-2/lec-2-2.html#bernoulli-trials",
    "title": "Unit 2.2: Randomization Techniques",
    "section": "Bernoulli Trials",
    "text": "Bernoulli Trials\nBernoulli trials represent the simplest approach to randomization, where each unit is assigned to treatment independently with a fixed probability. This is conceptually equivalent to flipping a coin for each participant, with heads resulting in treatment assignment and tails resulting in control assignment.\nImplementation\nLet’s first create a dataset to work with:\n\nlibrary(data.table)\n\n# Generate data with 1000 participants\nn &lt;- 1000\n\n# Create baseline covariates\ndt &lt;- data.table(\n  # ID variable\n  id = 1:n,\n  \n  # Covariates that will be used for stratification\n  age = sample(18:80, n, replace = TRUE),                   # Continuous \n  education = sample(c(\"None\", \"Primary\", \"Secondary\", \"Higher\"), n, replace = TRUE, \n                    prob = c(0.1, 0.3, 0.4, 0.2)),          # Categorical\n  \n  # Additional covariates not used for stratification\n  female = rbinom(n, 1, 0.55),                              # Binary\n  income = round(rlnorm(n, meanlog = 10, sdlog = 1), 2),    # Continuous, right-skewed\n  rural = rbinom(n, 1, 0.4),                                # Binary\n  chronic_disease = rbinom(n, 1, 0.3),                      # Binary\n  satisfaction = sample(1:5, n, replace = TRUE,             # Ordinal 1-5 scale\n                       prob = c(0.1, 0.2, 0.4, 0.2, 0.1))\n)\n\n# Convert categorical variables to factors for clearer output\ndt[, education := factor(education, levels = c(\"None\", \"Primary\", \"Secondary\", \"Higher\"))]\n\n# View data structure\nprint(\"Data structure:\")\n\n[1] \"Data structure:\"\n\nstr(dt)\n\nClasses 'data.table' and 'data.frame':  1000 obs. of  8 variables:\n $ id             : int  1 2 3 4 5 6 7 8 9 10 ...\n $ age            : int  19 34 62 45 44 74 40 24 36 34 ...\n $ education      : Factor w/ 4 levels \"None\",\"Primary\",..: 3 3 3 2 4 3 3 2 3 3 ...\n $ female         : int  1 1 1 0 1 1 1 1 1 1 ...\n $ income         : num  15382 38850 10788 42119 17465 ...\n $ rural          : int  0 1 0 0 0 1 1 0 1 1 ...\n $ chronic_disease: int  1 0 1 1 1 0 0 0 0 0 ...\n $ satisfaction   : int  2 3 1 2 1 5 3 1 3 3 ...\n - attr(*, \".internal.selfref\")=&lt;externalptr&gt; \n\n# View the first 10 observations\nhead(dt)\n\n      id   age education female   income rural chronic_disease satisfaction\n   &lt;int&gt; &lt;int&gt;    &lt;fctr&gt;  &lt;int&gt;    &lt;num&gt; &lt;int&gt;           &lt;int&gt;        &lt;int&gt;\n1:     1    19 Secondary      1 15381.84     0               1            2\n2:     2    34 Secondary      1 38849.92     1               0            3\n3:     3    62 Secondary      1 10788.07     0               1            1\n4:     4    45   Primary      0 42118.52     0               1            2\n5:     5    44    Higher      1 17465.41     0               1            1\n6:     6    74 Secondary      1 18718.60     1               0            5\n\n\nNow let’s randomize using Bernoulli:\n\n# Make a copy of the data (so we can compare to other randomization methods below)\ndt_bern &lt;- copy(dt)\n\n# Bernoulli trial example\nset.seed(072111)\n\np &lt;- 0.5  # Probability of treatment assignment\n\n# Independent random assignment\ndt_bern[, treatment := rbinom(.N, 1, p)]\n\n# Check resulting allocation\ndt_bern[, .N, by = treatment]\n\n   treatment     N\n       &lt;int&gt; &lt;int&gt;\n1:         0   514\n2:         1   486\n\n\nAdvantages and Disadvantages\nAdvantages:\n\nSimple to implement\nCan randomize as participants arrive (no need to know full sample in advance)\nNo baseline data needed\n\nDisadvantages: - Random group sizes (can result in imbalanced treatment allocation) - Potential imbalance on key covariates - Vulnerable to implementation problems\nCase Study: The Canadian National Breast Screening Study\nThe Canadian National Breast Screening Study (CNBSS) provides a cautionary tale about vulnerabilities in Bernoulli-type randomization. This major randomized trial evaluated the effectiveness of mammography screening for breast cancer in the 1980s.\nThe study used a variant of simple alternating assignment—assigning the first woman to treatment, the second to control, and so on. However, several critical flaws emerged:\n\n\nPre-randomization examination: Women received clinical breast exams before randomization, providing information that could influence assignment\n\nKnowledge of the assignment schedule: Staff knew that assignments alternated, creating opportunities for manipulation\n\nInadequate concealment: Study coordinators could see which group the next woman would be assigned to\n\nEvidence of manipulation: Later audits found names overwritten, identities reversed, and lines skipped in assignment ledgers\n\nThe consequences were severe: women with palpable lumps were disproportionately assigned to the mammography group, creating a significant selection bias. The mammography group had a 68% higher incidence of advanced cancers at baseline! This likely masked any potential benefits of screening, as the study ultimately reported no mortality benefit from mammography.[^1]\nThis case highlights how vulnerable simple randomization schemes can be to manipulation, especially when those implementing the study have preferences about treatment assignment or when the randomization process is transparent and predictable. Note that there is nothing wrong with Bernoulli trials, but you should carefully consider how to impelment randomization to preserve the integrity of the randomization process. Especially when working with partner organizations (which we do all the time in health services research), one needs to work with these partners to devise a randomization protocol that fits their existing workflow but protects the randomization process."
  },
  {
    "objectID": "unit-2/lec-2-2.html#complete-randomization",
    "href": "unit-2/lec-2-2.html#complete-randomization",
    "title": "Unit 2.2: Randomization Techniques",
    "section": "Complete Randomization",
    "text": "Complete Randomization\nComplete randomization addresses some of the limitations of Bernoulli trials by fixing the number of units assigned to each treatment condition, ensuring the desired allocation ratio is achieved exactly.\nImplementation\n\n# Copy data\ndt_cr &lt;- copy(dt)\n\nset.seed(072111)  # Set seed for reproducibility\n\n# Parameters\np &lt;- 0.5          # Proportion to assign to treatment\n\n# Add a column with uniform random numbers from 0 to 1.\ndt_cr[, random_num := runif(.N, min = 0, max = 1)]\n\n# Sort by random number\nsetorder(dt_cr, random_num)\n\n# Assign first p% to treatment\ndt_cr[, treatment := 0]\ndt_cr[1:(.N*p), treatment := 1]\n\n# Check resulting allocation\ndt_cr[, .N, by = treatment]\n\n   treatment     N\n       &lt;num&gt; &lt;int&gt;\n1:         1   500\n2:         0   500\n\n\nIn complete randomization, we first determine exactly how many units will receive each treatment. Then we generate a random ordering of all units and assign the first \\(N_p\\) units to treatment and the remaining \\(N_0\\) units to control.\nAdvantages and Disadvantages\nAdvantages:\n\nGuarantees exactly the desired allocation ratio\nAvoids power loss from uneven group sizes - Still relatively simple to implement\n\nDisadvantages:\n\nRequires knowing the full sample in advance\nStill subject to chance imbalance on covariates\n\nLet’s check the balance we got for the two examples above by running t-tests comparing the treatment and control groups.\n\n# First let's create a function to perform t-tests and create a formatted results table\nrun_ttests &lt;- function(dt, title) {\n  results &lt;- data.table(\n    variable = character(),\n    method = character(),\n    mean_control = numeric(),\n    mean_treated = numeric(),\n    diff = numeric(),\n    p_value = numeric(),\n    significant = character()\n  )\n  \n  for (var in baseline_vars) {\n    # For categorical variables (factor), we need to handle differently\n    if (is.factor(dt[[var]])) {\n      # For each level of the factor\n      for (level in levels(dt[[var]])) {\n        # Create temporary binary indicator\n        dt[, temp := as.numeric(get(var) == level)]\n        \n        # Calculate means\n        mean_control &lt;- dt[treatment == 0, mean(temp)]\n        mean_treated &lt;- dt[treatment == 1, mean(temp)]\n        \n        # Run t-test\n        t_result &lt;- t.test(dt[treatment == 1, temp], \n                           dt[treatment == 0, temp])\n        \n        # Add to results\n        results &lt;- rbind(results, data.table(\n          variable = paste0(var, \": \", level),\n          method = title,\n          mean_control = mean_control,\n          mean_treated = mean_treated,\n          diff = mean_treated - mean_control,\n          p_value = t_result$p.value,\n          significant = ifelse(t_result$p.value &lt; 0.05, \"*\", \"\")\n        ))\n        \n        # Remove temporary variable\n        dt[, temp := NULL]\n      }\n    } else {\n      # For continuous and binary variables\n      mean_control &lt;- dt[treatment == 0, mean(get(var), na.rm = TRUE)]\n      mean_treated &lt;- dt[treatment == 1, mean(get(var), na.rm = TRUE)]\n      \n      # Run t-test\n      t_result &lt;- t.test(\n        dt[treatment == 1, get(var)], \n        dt[treatment == 0, get(var)]\n      )\n      \n      # Add to results\n      results &lt;- rbind(results, data.table(\n        variable = var,\n        method = title,\n        mean_control = mean_control,\n        mean_treated = mean_treated,\n        diff = mean_treated - mean_control,\n        p_value = t_result$p.value,\n        significant = ifelse(t_result$p.value &lt; 0.05, \"*\", \"\")\n      ))\n    }\n  }\n  \n  return(results)\n}\n\n# Now let's run the tests\n\n## List of all baseline covariates to test\nbaseline_vars &lt;- c(\"age\", \"education\", \"female\", \"income\", \"rural\", \"chronic_disease\", \"satisfaction\")\n\n# Run t-tests\nttest_bern &lt;- run_ttests(dt_bern, \"Bernoulli - t-test\")\nttest_cr &lt;- run_ttests(dt_cr, \"Complete - t-test\")\n\nprint(ttest_bern)\n\n                variable             method mean_control mean_treated\n                  &lt;char&gt;             &lt;char&gt;        &lt;num&gt;        &lt;num&gt;\n 1:                  age Bernoulli - t-test 5.029767e+01 4.860905e+01\n 2:      education: None Bernoulli - t-test 1.050584e-01 1.172840e-01\n 3:   education: Primary Bernoulli - t-test 3.112840e-01 3.209877e-01\n 4: education: Secondary Bernoulli - t-test 3.735409e-01 3.971193e-01\n 5:    education: Higher Bernoulli - t-test 2.101167e-01 1.646091e-01\n 6:               female Bernoulli - t-test 5.972763e-01 5.802469e-01\n 7:               income Bernoulli - t-test 3.698802e+04 3.183656e+04\n 8:                rural Bernoulli - t-test 3.988327e-01 4.074074e-01\n 9:      chronic_disease Bernoulli - t-test 3.171206e-01 3.086420e-01\n10:         satisfaction Bernoulli - t-test 2.964981e+00 2.917695e+00\n             diff    p_value significant\n            &lt;num&gt;      &lt;num&gt;      &lt;char&gt;\n 1: -1.688612e+00 0.14555945            \n 2:  1.222558e-02 0.53949726            \n 3:  9.703608e-03 0.74185085            \n 4:  2.357849e-02 0.44441213            \n 5: -4.550768e-02 0.06504122            \n 6: -1.702935e-02 0.58485993            \n 7: -5.151461e+03 0.06919766            \n 8:  8.574723e-03 0.78260046            \n 9: -8.478647e-03 0.77281943            \n10: -4.728507e-02 0.50045109            \n\nprint(ttest_cr)\n\n                variable            method mean_control mean_treated     diff\n                  &lt;char&gt;            &lt;char&gt;        &lt;num&gt;        &lt;num&gt;    &lt;num&gt;\n 1:                  age Complete - t-test       48.630       50.324    1.694\n 2:      education: None Complete - t-test        0.116        0.106   -0.010\n 3:   education: Primary Complete - t-test        0.320        0.312   -0.008\n 4: education: Secondary Complete - t-test        0.396        0.374   -0.022\n 5:    education: Higher Complete - t-test        0.168        0.208    0.040\n 6:               female Complete - t-test        0.584        0.594    0.010\n 7:               income Complete - t-test    31828.744    37140.081 5311.337\n 8:                rural Complete - t-test        0.404        0.402   -0.002\n 9:      chronic_disease Complete - t-test        0.316        0.310   -0.006\n10:         satisfaction Complete - t-test        2.914        2.970    0.056\n       p_value significant\n         &lt;num&gt;      &lt;char&gt;\n 1: 0.14434344            \n 2: 0.61514873            \n 3: 0.78582226            \n 4: 0.47518803            \n 5: 0.10571624            \n 6: 0.74823610            \n 7: 0.06446256            \n 8: 0.94865983            \n 9: 0.83809577            \n10: 0.42490327            \n\n\nChance Imbalance in Complete Randomization\nEven with perfect implementation of complete randomization, covariates may still be imbalanced by chance. In a simulation study using data from the National Longitudinal Survey of Youth (NLSY) with 722 subjects:[^1]\n\n~45% of randomizations had all covariates balanced\n~30% had one imbalanced covariate\nThe remaining had multiple imbalanced covariates\n\nThis raises two critical questions: 1. How can we ensure better balance in the design phase? 2. What should we do if imbalance occurs after randomization?\nOur next three approaches address the first question by improving balance through more sophisticated randomization techniques."
  },
  {
    "objectID": "unit-2/lec-2-2.html#re-randomization",
    "href": "unit-2/lec-2-2.html#re-randomization",
    "title": "Unit 2.2: Randomization Techniques",
    "section": "Re-randomization",
    "text": "Re-randomization\nRe-randomization attempts to improve covariate balance by generating multiple possible randomizations and selecting one with good balance.\nThere are two common approaches to re-randomization:\n\n\nThreshold approach: Generate randomizations until all p-values for covariate balance exceed a threshold (e.g., p &gt; 0.1)\n\nOptimization approach: Generate a large number of randomizations (e.g., 1,000) and select the one with the best overall balance\n\nThreshold Approach\n\n# Copy the data again\ndt_rerand_threshold &lt;- copy(dt)\n\n# Re-randomization - Threshold Approach:\n# Function to test balance on key covariates\ntest_balance &lt;- function(dt) {\n  # Define key covariates to check balance on\n  balance_vars &lt;- c(\"age\", \"education\", \"female\", \"income\", \"rural\", \"chronic_disease\", \"satisfaction\")\n  \n  # Store p-values\n  p_values &lt;- numeric(length(balance_vars))\n  names(p_values) &lt;- balance_vars\n  \n  for (i in seq_along(balance_vars)) {\n    var &lt;- balance_vars[i]\n    \n    # For categorical variables\n    if (is.factor(dt[[var]])) {\n      # Create model matrix (one-hot encoding)\n      formula_str &lt;- paste0(\"~ \", var, \" - 1\")\n      mm &lt;- model.matrix(as.formula(formula_str), data = dt)\n      # Test each level except the reference\n      p_vals_cat &lt;- numeric(ncol(mm))\n      for (j in 1:ncol(mm)) {\n        t_result &lt;- t.test(mm[dt$treatment == 1, j], mm[dt$treatment == 0, j])\n        p_vals_cat[j] &lt;- t_result$p.value\n      }\n      # Use minimum p-value (most imbalanced category)\n      p_values[i] &lt;- min(p_vals_cat)\n    } else {\n      # For continuous or binary variables\n      t_result &lt;- t.test(dt[treatment == 1, get(var)], dt[treatment == 0, get(var)])\n      p_values[i] &lt;- t_result$p.value\n    }\n  }\n  \n  return(p_values)\n}\n\n# Re-randomization with threshold approach\n# Continue generating randomizations until all p-values &gt; 0.10\nmax_attempts &lt;- 1000  # Safety limit to prevent infinite loops\nattempt &lt;- 0\nbalanced &lt;- FALSE\n\ncat(\"\\nPerforming re-randomization with threshold approach...\\n\")\n\n\nPerforming re-randomization with threshold approach...\n\nwhile (!balanced && attempt &lt; max_attempts) {\n  attempt &lt;- attempt + 1\n  \n  # Generate a new randomization\n  dt_rerand_threshold[, treatment := rbinom(.N, 1, 0.5)]\n  \n  # Test balance\n  p_values &lt;- test_balance(dt_rerand_threshold)\n  \n  # Check if all p-values are above threshold (0.10)\n  if (all(p_values &gt; 0.10)) {\n    balanced &lt;- TRUE\n    cat(\"Found balanced randomization after\", attempt, \"attempts\\n\")\n    cat(\"Balance p-values:\", paste(names(p_values), round(p_values, 4), collapse=\", \"), \"\\n\\n\")\n  } else if (attempt %% 100 == 0) {\n    cat(\"Completed\", attempt, \"attempts, continuing search...\\n\")\n  }\n}\n\nFound balanced randomization after 1 attempts\nBalance p-values: age 0.3607, education 0.1213, female 0.6851, income 0.595, rural 0.6301, chronic_disease 0.3026, satisfaction 0.1012 \n\nif (!balanced) {\n  cat(\"Warning: Could not find perfectly balanced randomization after\", max_attempts, \"attempts\\n\")\n  cat(\"Using best randomization found so far\\n\\n\")\n}\n\nOptimization approach\nYou can define the “best” overall balance in different ways. Here we’ll use the Mahalanobis distance.\n\n\n\n\n\n\nNote\n\n\n\nThe Mahalanobis distance is a statistical measure that quantifies the distance between a point and a distribution in multivariate space. The Mahalanobis distance is formally defined as:\n\\[\nd_{maha} = \\sqrt{(x_B - X_A)^TC^{-1}(x_B - x_A)}\n\\]\nWhere: • \\(x_A\\) and \\(x_B\\) are a pair of objects • \\(C\\) is the sample covariance matrix • \\(C^{-1}\\) is the inverse of the covariance matrix • \\(T\\) denotes the transpose operation\n\n\n\n# Copy data\n\ndt_rerand_optimal &lt;- copy(dt)\n\n# Re-randomization - Optimization Approach:\n# Generate multiple randomizations and select the one with best overall balance\n\ncat(\"Performing re-randomization with optimization approach...\\n\")\n\nPerforming re-randomization with optimization approach...\n\nn_candidates &lt;- 1000\nmahalanobis_distances &lt;- numeric(n_candidates)\n\n# Generate covariates matrix (need to handle factors separately)\ncat(\"Converting covariates to numeric for Mahalanobis distance calculation...\\n\")\n\nConverting covariates to numeric for Mahalanobis distance calculation...\n\ncov_vars &lt;- c(\"age\", \"female\", \"income\", \"rural\", \"chronic_disease\", \"satisfaction\")\n\n# Add dummy variables for education\nedu_dummies &lt;- model.matrix(~ education - 1, data = dt_rerand_optimal)\nX &lt;- cbind(\n  as.matrix(dt_rerand_optimal[, ..cov_vars]),\n  edu_dummies\n)\n\n# Calculate covariance matrix of covariates\nS &lt;- cov(X)\nS_inv &lt;- tryCatch({\n  solve(S)  # Try to compute inverse\n}, error = function(e) {\n  cat(\"Covariance matrix is singular, using pseudoinverse...\\n\")\n  # Use pseudoinverse if matrix is singular\n  library(MASS)\n  ginv(S)\n})\n\nCovariance matrix is singular, using pseudoinverse...\n\n# Generate candidate randomizations and calculate balance measure\nfor (i in 1:n_candidates) {\n  # Generate a new randomization\n  treatment_assign &lt;- rbinom(n, 1, 0.5)\n  \n  # Calculate difference in means\n  mean_diff &lt;- colMeans(X[treatment_assign == 1, ]) - colMeans(X[treatment_assign == 0, ])\n  \n  # Calculate Mahalanobis distance\n  mahalanobis_distances[i] &lt;- t(mean_diff) %*% S_inv %*% mean_diff\n  \n  if (i %% 200 == 0) cat(\"Generated\", i, \"candidate randomizations...\\n\")\n}\n\nGenerated 200 candidate randomizations...\nGenerated 400 candidate randomizations...\nGenerated 600 candidate randomizations...\nGenerated 800 candidate randomizations...\nGenerated 1000 candidate randomizations...\n\n# Find the randomization with smallest Mahalanobis distance\nbest_idx &lt;- which.min(mahalanobis_distances)\ncat(\"Selected optimal randomization (candidate\", best_idx, \"with Mahalanobis distance =\", \n    round(mahalanobis_distances[best_idx], 4), \")\\n\\n\")\n\nSelected optimal randomization (candidate 165 with Mahalanobis distance = 0 )\n\n# Generate the best randomization\nset.seed(best_idx)  # For reproducibility\ndt_rerand_optimal[, treatment := rbinom(.N, 1, 0.5)]\n\nDrawbacks of Re-randomization\nWhile re-randomization can improve balance, it has several limitations:\n\n\nOpaque constraints: The process creates a “black box” where it’s unclear what constraints are being imposed\n\nUnusual handling of outliers: Extreme values may force unusual allocation patterns\n\nComputational cost: May require many iterations, especially with multiple covariates\n\nPotential futility: If criteria are too strict, acceptable randomizations may be extremely rare\n\nStatistical inference complications: Standard methods don’t account for the re-randomization process\n\nLimited scope: Still cannot balance on unobserved covariates"
  },
  {
    "objectID": "unit-2/lec-2-2.html#stratified-block-randomization",
    "href": "unit-2/lec-2-2.html#stratified-block-randomization",
    "title": "Unit 2.2: Randomization Techniques",
    "section": "Stratified (Block) Randomization",
    "text": "Stratified (Block) Randomization\nStratified randomization (also called block randomization) directly addresses the balance issue by dividing the sample into strata based on covariates and randomizing separately within each stratum.\nIn this approach, we first create strata based on combinations of important covariates, then randomize separately within each stratum. This guarantees perfect balance on the stratification variables.\nSelecting Stratification Variables\nNot all covariates are equally important for stratification. Consider these guidelines:\n\n\nDiscrete variables are easier to implement than continuous ones\nPrioritize variables that strongly predict outcomes (baseline values of the outcome variable are especially important)\nInclude variables where heterogeneous effects are expected (facilitates subgroup analysis)\nBe careful about creating too many strata, which can lead to “small cell” problems\nHandling “Misfits”\nA practical challenge in stratified randomization occurs when strata sizes are not divisible by the number of treatment conditions (e.g., three people in a stratum with two treatment conditions). Options for these “misfits” include:\n\nRemove units randomly to create divisible strata\nCreate a separate stratum for misfits\nUse a different randomization approach for misfits\nImplementation\n\n# Copy the Data\ndt_strat &lt;- copy(dt)\n\n# Set the seed using Zoe's bday\nset.seed(072111)  # For reproducibility\n\n# Convert categorical variables to factors for clearer output\ndt_strat[, education := factor(education, levels = c(\"None\", \"Primary\", \"Secondary\", \"Higher\"))]\n\n# Create age quintiles\ndt_strat[, age_quintile := cut(age, \n                         breaks = quantile(age, probs = seq(0, 1, 0.2), na.rm = TRUE), \n                         labels = 1:5, \n                         include.lowest = TRUE)]\n\n# View data structure\nprint(\"Data structure:\")\n\n[1] \"Data structure:\"\n\nstr(dt_strat)\n\nClasses 'data.table' and 'data.frame':  1000 obs. of  9 variables:\n $ id             : int  1 2 3 4 5 6 7 8 9 10 ...\n $ age            : int  19 34 62 45 44 74 40 24 36 34 ...\n $ education      : Factor w/ 4 levels \"None\",\"Primary\",..: 3 3 3 2 4 3 3 2 3 3 ...\n $ female         : int  1 1 1 0 1 1 1 1 1 1 ...\n $ income         : num  15382 38850 10788 42119 17465 ...\n $ rural          : int  0 1 0 0 0 1 1 0 1 1 ...\n $ chronic_disease: int  1 0 1 1 1 0 0 0 0 0 ...\n $ satisfaction   : int  2 3 1 2 1 5 3 1 3 3 ...\n $ age_quintile   : Factor w/ 5 levels \"1\",\"2\",\"3\",\"4\",..: 1 2 4 3 3 5 2 1 2 2 ...\n - attr(*, \".internal.selfref\")=&lt;externalptr&gt; \n\n\n\n# Create strata ID by combining education and age quintile\ndt_strat[, strata := .GRP, by = .(education, age_quintile)]\n\n# Print strata information\nprint(\"Strata information:\")\n\n[1] \"Strata information:\"\n\ndt_strat[, .(count = .N), by = .(education, age_quintile, strata)][order(strata)]\n\n    education age_quintile strata count\n       &lt;fctr&gt;       &lt;fctr&gt;  &lt;int&gt; &lt;int&gt;\n 1: Secondary            1      1    86\n 2: Secondary            2      2    75\n 3: Secondary            4      3    71\n 4:   Primary            3      4    60\n 5:    Higher            3      5    42\n 6: Secondary            5      6    76\n 7:   Primary            1      7    66\n 8:   Primary            4      8    69\n 9:   Primary            5      9    65\n10:   Primary            2     10    56\n11:      None            5     11    21\n12:    Higher            4     12    36\n13:    Higher            1     13    42\n14:    Higher            2     14    38\n15:    Higher            5     15    30\n16:      None            2     16    29\n17: Secondary            3     17    77\n18:      None            3     18    17\n19:      None            4     19    24\n20:      None            1     20    20\n    education age_quintile strata count\n\n\nNow let’s randomize half of the observations in each strata to treatment and half to control. To ensure 50/50 split, we’ll use the Complete Randomization method within each strata:\n\ndt_strat[, treatment := 0]  # Initialize all to control\nfor (s in unique(dt_strat$strata)) {\n  # Get all units in this stratum\n  stratum_units &lt;- dt_strat[strata == s, id]\n  n_units &lt;- length(stratum_units)\n  \n  # Determine number to treat (half, rounded down)\n  n_treat &lt;- floor(n_units/2)\n  \n  # Misfits: If n_units is odd, flip a coin to decide whether to round up or down\n  if (n_units %% 2 == 1) {\n    if (runif(1) &lt; 0.5) n_treat &lt;- n_treat + 1\n  }\n  \n  # Randomly select units for treatment\n  treated_units &lt;- sample(stratum_units, n_treat)\n  \n  # Assign treatment\n  dt_strat[id %in% treated_units, treatment := 1]\n}\n\n# Print treatment assignment by strata for stratified randomization\nprint(\"Treatment assignment by strata in stratified randomization:\")\n\n[1] \"Treatment assignment by strata in stratified randomization:\"\n\ndt_strat[, .(N = .N, \n             n_treated = sum(treatment), \n             pct_treated = mean(treatment)*100), \n         by = strata][order(strata)]\n\n    strata     N n_treated pct_treated\n     &lt;int&gt; &lt;int&gt;     &lt;num&gt;       &lt;num&gt;\n 1:      1    86        43    50.00000\n 2:      2    75        38    50.66667\n 3:      3    71        36    50.70423\n 4:      4    60        30    50.00000\n 5:      5    42        21    50.00000\n 6:      6    76        38    50.00000\n 7:      7    66        33    50.00000\n 8:      8    69        34    49.27536\n 9:      9    65        33    50.76923\n10:     10    56        28    50.00000\n11:     11    21        11    52.38095\n12:     12    36        18    50.00000\n13:     13    42        21    50.00000\n14:     14    38        19    50.00000\n15:     15    30        15    50.00000\n16:     16    29        14    48.27586\n17:     17    77        39    50.64935\n18:     18    17         9    52.94118\n19:     19    24        12    50.00000\n20:     20    20        10    50.00000\n    strata     N n_treated pct_treated"
  },
  {
    "objectID": "unit-2/lec-2-2.html#matched-pairs-randomization",
    "href": "unit-2/lec-2-2.html#matched-pairs-randomization",
    "title": "Unit 2.2: Randomization Techniques",
    "section": "Matched Pairs Randomization",
    "text": "Matched Pairs Randomization\nMatched pairs randomization represents the extreme case of stratification, where each stratum contains exactly two similar units, and one is randomly assigned to treatment and one to control.\nIn this approach, we first create pairs of similar units based on a distance metric, then randomly assign one member of each pair to treatment and the other to control. This guarantees excellent balance on the matching variables."
  },
  {
    "objectID": "unit-2/lec-2-2.html#implementation-3",
    "href": "unit-2/lec-2-2.html#implementation-3",
    "title": "Unit 2.2: Randomization Techniques",
    "section": "Implementation",
    "text": "Implementation\n\nIn this example, we’re using what is referred to as a “greedy” matching algorithm. It is “greedy” because it makes locally optimal choices at each step without reconsidering earlier decisions, potentially missing the globally optimal solution. We’re using this for now because it is clear and simple to impelment.\nIn practice, “canned” matching commands are commonly used to form matches and these often use other optimal matching algorithms. A non-greedy, optimal matching algorithm would consider all possible ways to pair units and select the configuration that minimizes the total sum of distances across all pairs. This is computationally more intensive (often solved using network optimization methods like the Hungarian algorithm) but guarantees the global minimum total distance.\n\n\ndt_matched &lt;- copy(dt)\n\n# Set the seed using Zoe's bday\nset.seed(072111)  # For reproducibility\n\n# 1. Prepare data for matching\ncat(\"\\nPerforming matched randomization using Mahalanobis distance...\\n\")\n\n\nPerforming matched randomization using Mahalanobis distance...\n\n# Create matrix of variables to match on\nmatch_vars &lt;- c(\"age\", \"female\", \"income\", \"rural\", \"chronic_disease\")\n\n# Convert education to dummy variables for inclusion in distance calculation\nedu_dummies &lt;- model.matrix(~ education - 1, data = dt_matched)\ncolnames(edu_dummies) &lt;- paste0(\"edu_\", levels(dt_matched$education))\n\n# Combine numeric variables with education dummies\nX &lt;- cbind(as.matrix(dt_matched[, ..match_vars]), edu_dummies)\n\n# Calculate covariance matrix and inverse\nS &lt;- cov(X)\nS_inv &lt;- tryCatch({\n  solve(S)  # Try to compute inverse\n}, error = function(e) {\n  cat(\"Covariance matrix is singular, using pseudoinverse...\\n\")\n  # Use pseudoinverse if matrix is singular\n  library(MASS)\n  ginv(S)\n})\n\nCovariance matrix is singular, using pseudoinverse...\n\n# 2. Calculate pairwise Mahalanobis distances between all units\nn &lt;- nrow(dt_matched)\ndist_matrix &lt;- matrix(0, n, n)\n\n# Store original IDs to maintain correct mapping\noriginal_ids &lt;- dt_matched$id\n\ncat(\"Calculating Mahalanobis distances between all units...\\n\")\n\nCalculating Mahalanobis distances between all units...\n\nfor (i in 1:(n-1)) {\n  for (j in (i+1):n) {\n    # Calculate vector of differences between units i and j\n    diff &lt;- X[i,] - X[j,]\n    \n    # Calculate Mahalanobis distance\n    dist_matrix[i,j] &lt;- dist_matrix[j,i] &lt;- sqrt(t(diff) %*% S_inv %*% diff)\n  }\n  \n  if (i %% 100 == 0) cat(\"Processed\", i, \"of\", n, \"units...\\n\")\n}\n\nProcessed 100 of 1000 units...\nProcessed 200 of 1000 units...\nProcessed 300 of 1000 units...\nProcessed 400 of 1000 units...\nProcessed 500 of 1000 units...\nProcessed 600 of 1000 units...\nProcessed 700 of 1000 units...\nProcessed 800 of 1000 units...\nProcessed 900 of 1000 units...\n\n# 3. Create optimal pairs using greedy algorithm\ncat(\"Creating optimal pairs...\\n\")\n\nCreating optimal pairs...\n\nunpaired_idx &lt;- 1:n  # These are indices, not IDs\npairs_idx &lt;- list()  # Store pairs of indices\n\nwhile (length(unpaired_idx) &gt;= 2) {\n  # Find the nearest neighbor for the first unpaired unit\n  current_idx &lt;- unpaired_idx[1]\n  distances &lt;- dist_matrix[current_idx, unpaired_idx[-1]]\n  nearest_pos &lt;- which.min(distances)\n  nearest_idx &lt;- unpaired_idx[nearest_pos + 1]  # +1 because we excluded current from unpaired\n  \n  # Create a new pair of indices\n  pairs_idx &lt;- c(pairs_idx, list(c(current_idx, nearest_idx)))\n  \n  # Remove these indices from unpaired\n  unpaired_idx &lt;- unpaired_idx[!unpaired_idx %in% c(current_idx, nearest_idx)]\n  \n  if (length(pairs_idx) %% 50 == 0) cat(\"Created\", length(pairs_idx), \"pairs...\\n\")\n}\n\nCreated 50 pairs...\nCreated 100 pairs...\nCreated 150 pairs...\nCreated 200 pairs...\nCreated 250 pairs...\nCreated 300 pairs...\nCreated 350 pairs...\nCreated 400 pairs...\nCreated 450 pairs...\nCreated 500 pairs...\n\n# Handle leftover unit if odd number of units\nif (length(unpaired_idx) == 1) {\n  cat(\"Note: Odd number of units. Unit index\", unpaired_idx, \"will be randomly assigned.\\n\")\n  # This unit will be handled separately\n}\n\n# 4. Convert index pairs to ID pairs and assign treatments\npairs &lt;- lapply(pairs_idx, function(p) original_ids[p])\n\n# Reset pair_id and treatment in the dataset\ndt_matched[, pair_id := NA_integer_]\ndt_matched[, treatment := 0]  # Initialize all to control\n\ncat(\"Assigning pair IDs and treatment...\\n\")\n\nAssigning pair IDs and treatment...\n\nfor (i in seq_along(pairs)) {\n  # Assign pair ID to both units in the pair\n  dt_matched[id %in% pairs[[i]], pair_id := i]\n  \n  # Randomly select one unit for treatment\n  treated_id &lt;- sample(pairs[[i]], 1)\n  dt_matched[id == treated_id, treatment := 1]\n}\n\n# Handle leftover unit if exists\nif (length(unpaired_idx) == 1) {\n  leftover_id &lt;- original_ids[unpaired_idx]\n  \n  # Assign to a special \"pair\" ID\n  dt_matched[id == leftover_id, pair_id := length(pairs) + 1]\n  \n  # Randomly assign to treatment or control\n  if (runif(1) &lt; 0.5) {\n    dt_matched[id == leftover_id, treatment := 1]\n  }\n}\n\n# Verify pair assignments\npair_counts &lt;- dt_matched[!is.na(pair_id), .N, by = pair_id]\ncat(\"\\nVerifying pair assignments:\\n\")\n\n\nVerifying pair assignments:\n\ncat(\"Number of unique pairs:\", nrow(pair_counts), \"\\n\")\n\nNumber of unique pairs: 500 \n\ncat(\"Distribution of pair sizes:\\n\")\n\nDistribution of pair sizes:\n\nprint(table(pair_counts$N))\n\n\n  2 \n500 \n\n# Check treatment balance\ncat(\"\\nTreatment balance:\\n\")\n\n\nTreatment balance:\n\ncat(\"Treatment group size:\", dt_matched[treatment == 1, .N], \"\\n\")\n\nTreatment group size: 500 \n\ncat(\"Control group size:\", dt_matched[treatment == 0, .N], \"\\n\")\n\nControl group size: 500 \n\n# Display the first 10 pairs, sorted by pair_id\ndt_matched[pair_id %in% 1:10][order(pair_id), .(id, pair_id, treatment, age, education, income)]\n\n       id pair_id treatment   age education   income\n    &lt;int&gt;   &lt;int&gt;     &lt;num&gt; &lt;int&gt;    &lt;fctr&gt;    &lt;num&gt;\n 1:     1       1         1    19 Secondary 15381.84\n 2:   314       1         0    19      None 15029.52\n 3:     2       2         1    34 Secondary 38849.92\n 4:   377       2         0    33   Primary 35934.18\n 5:     3       3         0    62 Secondary 10788.07\n 6:   241       3         1    62 Secondary  9877.48\n 7:     4       4         0    45   Primary 42118.52\n 8:   449       4         1    45 Secondary 40293.28\n 9:     5       5         1    44    Higher 17465.41\n10:   846       5         0    44   Primary 17352.46\n11:     6       6         0    74 Secondary 18718.60\n12:   760       6         1    74 Secondary 18864.18\n13:     7       7         0    40 Secondary 13593.29\n14:   308       7         1    40 Secondary 15251.64\n15:     8       8         1    24   Primary 21565.13\n16:   468       8         0    24   Primary 22107.18\n17:     9       9         1    36 Secondary 22274.83\n18:   660       9         0    36    Higher 23510.64\n19:    10      10         1    34 Secondary  9627.04\n20:   490      10         0    34    Higher  9721.55\n       id pair_id treatment   age education   income\n\n\nAdvantages and Limitations\nAdvantages: - Achieves excellent balance on matching variables - Works well with continuous covariates - Reduces variance in treatment effect estimates\nLimitations: - Finding good matches becomes difficult with many covariates - May discard units that can’t be well-matched - Requires all baseline data before randomization - Analysis must account for pairing structure"
  },
  {
    "objectID": "unit-2/lec-2-2.html#verifying-balance-approaches-and-best-practices",
    "href": "unit-2/lec-2-2.html#verifying-balance-approaches-and-best-practices",
    "title": "Unit 2.2: Randomization Techniques",
    "section": "Verifying Balance: Approaches and Best Practices",
    "text": "Verifying Balance: Approaches and Best Practices\nAfter randomization, it’s crucial to verify that balance between the treatment and control group was achieved. Several approaches exist:\n\nIndividual Covariate Tests: Test balance one-by-one\nJoint Omnibus Test: Test that all covariates are jointly balanced (often combined with tests on each covariate)\nRegression-Based Tests: Regress treatment assignment on each covariate. Also allows you to control for strata or matched pair fixed effects.\n\nIndividual Covariate Tests\nThe most common approach is to test each covariate separately: - t-tests for continuous variables - Chi-square tests for categorical variables\nLet’s demonstrate this with data from the strata randomization example.\n\n# List of all baseline covariates to test\nbaseline_vars &lt;- c(\"age\", \"education\", \"female\", \"income\", \"rural\", \"chronic_disease\", \"satisfaction\")\n\n# Function to perform t-tests and create a formatted results table\nrun_ttests &lt;- function(dt, title) {\n  results &lt;- data.table(\n    variable = character(),\n    method = character(),\n    mean_control = numeric(),\n    mean_treated = numeric(),\n    diff = numeric(),\n    p_value = numeric(),\n    significant = character()\n  )\n  \n  for (var in baseline_vars) {\n    # For categorical variables (factor), we need to handle differently\n    if (is.factor(dt[[var]])) {\n      # For each level of the factor\n      for (level in levels(dt[[var]])) {\n        # Create temporary binary indicator\n        dt[, temp := as.numeric(get(var) == level)]\n        \n        # Calculate means\n        mean_control &lt;- dt[treatment == 0, mean(temp)]\n        mean_treated &lt;- dt[treatment == 1, mean(temp)]\n        \n        # Run t-test\n        t_result &lt;- t.test(dt[treatment == 1, temp], \n                           dt[treatment == 0, temp])\n        \n        # Add to results\n        results &lt;- rbind(results, data.table(\n          variable = paste0(var, \": \", level),\n          method = title,\n          mean_control = mean_control,\n          mean_treated = mean_treated,\n          diff = mean_treated - mean_control,\n          p_value = t_result$p.value,\n          significant = ifelse(t_result$p.value &lt; 0.05, \"*\", \"\")\n        ))\n        \n        # Remove temporary variable\n        dt[, temp := NULL]\n      }\n    } else {\n      # For continuous and binary variables\n      mean_control &lt;- dt[treatment == 0, mean(get(var), na.rm = TRUE)]\n      mean_treated &lt;- dt[treatment == 1, mean(get(var), na.rm = TRUE)]\n      \n      # Run t-test\n      t_result &lt;- t.test(\n        dt[treatment == 1, get(var)], \n        dt[treatment == 0, get(var)]\n      )\n      \n      # Add to results\n      results &lt;- rbind(results, data.table(\n        variable = var,\n        method = title,\n        mean_control = mean_control,\n        mean_treated = mean_treated,\n        diff = mean_treated - mean_control,\n        p_value = t_result$p.value,\n        significant = ifelse(t_result$p.value &lt; 0.05, \"*\", \"\")\n      ))\n    }\n  }\n  \n  return(results)\n}\n\n# Run t-tests for all randomization methods\nttest_strat &lt;- run_ttests(dt_strat, \"Stratified - t-test\")\n\n#Display Result\n\nJoint Omnibus Tests\nF-tests can test multiple covariates simultaneously, reducing the multiple testing problem.\n\n# Function to perform F-test\njoint_balance_test &lt;- function(dt) {\n  # Regress treatment on covariates\n  model &lt;- lm(treatment ~ age + income + female + education + chronic_disease, \n              data = dt)\n  \n  # Extract F-statistic and p-value for joint significance\n  f_test &lt;- summary(model)$fstatistic\n  f_value &lt;- f_test[1]\n  df1 &lt;- f_test[2]\n  df2 &lt;- f_test[3]\n  p_value &lt;- pf(f_value, df1, df2, lower.tail = FALSE)\n  \n  result &lt;- data.table(\n    f_value = f_value,\n    df1 = df1,\n    df2 = df2,\n    p_value = p_value\n  )\n  \n  return(result)\n}\n\n# Run joint test\njoint_test &lt;- joint_balance_test(dt_strat)\ncat(\"Joint balance test (F-test):\\n\")\n\nJoint balance test (F-test):\n\ncat(\"F(\", joint_test$df1, \",\", joint_test$df2, \") = \", \n    round(joint_test$f_value, 2), \", p = \", round(joint_test$p_value, 4), \"\\n\", sep=\"\")\n\nF(7,992) = 0.49, p = 0.841\n\n\nRegression-Based Tests\nRegress treatment assignment on each covariate; if randomization worked, coefficients should be insignificant.\nAgain, we’ll use the data created for the stratified randomization example. First, let’s look at this approach not controlling for strata fixed effects:\n\nlibrary(fixest) # For fixed effects\n\n# Function to perform OLS regressions without strata fixed effects\nrun_ols_no_strata &lt;- function(dt, title) {\n  results &lt;- data.table(\n    variable = character(),\n    method = character(),\n    coefficient = numeric(),\n    std_error = numeric(),\n    p_value = numeric(),\n    significant = character()\n  )\n  \n  for (var in baseline_vars) {\n    # Handle factor variables\n    if (is.factor(dt[[var]])) {\n      # Create one-hot encoding\n      for (level in levels(dt[[var]])[2:length(levels(dt[[var]]))]) {  # Skip first level (reference)\n        dt[, temp := as.numeric(get(var) == level)]\n        \n        # Run regression\n        reg &lt;- feols(temp ~ treatment, data = dt)\n        \n        # Add to results\n        results &lt;- rbind(results, data.table(\n          variable = paste0(var, \": \", level, \" vs \", levels(dt[[var]])[1]),\n          method = title,\n          coefficient = coef(reg)[\"treatment\"],\n          std_error = se(reg)[\"treatment\"],\n          p_value = pvalue(reg)[\"treatment\"],\n          significant = ifelse(pvalue(reg)[\"treatment\"] &lt; 0.05, \"*\", \"\")\n        ))\n        \n        # Remove temporary variable\n        dt[, temp := NULL]\n      }\n    } else {\n      # For continuous and binary variables\n      formula_str &lt;- paste0(var, \" ~ treatment\")\n      reg &lt;- feols(as.formula(formula_str), data = dt)\n      \n      # Add to results\n      results &lt;- rbind(results, data.table(\n        variable = var,\n        method = title,\n        coefficient = coef(reg)[\"treatment\"],\n        std_error = se(reg)[\"treatment\"],\n        p_value = pvalue(reg)[\"treatment\"],\n        significant = ifelse(pvalue(reg)[\"treatment\"] &lt; 0.05, \"*\", \"\")\n      ))\n    }\n  }\n  \n  return(results)\n}\n\n# Run OLS without strata for both randomization methods\nols_strat &lt;- run_ols_no_strata(dt_strat, \"Stratified  - OLS no Strata FE\")\n\n# Display results\nprint(ols_strat)\n\n                       variable                         method   coefficient\n                         &lt;char&gt;                         &lt;char&gt;         &lt;num&gt;\n1:                          age Stratified  - OLS no Strata FE  2.221876e-01\n2:   education: Primary vs None Stratified  - OLS no Strata FE -2.528040e-03\n3: education: Secondary vs None Stratified  - OLS no Strata FE  2.920047e-03\n4:    education: Higher vs None Stratified  - OLS no Strata FE -1.504024e-03\n5:                       female Stratified  - OLS no Strata FE -3.471256e-02\n6:                       income Stratified  - OLS no Strata FE  1.481540e+03\n7:                        rural Stratified  - OLS no Strata FE  1.477624e-02\n8:              chronic_disease Stratified  - OLS no Strata FE -4.050465e-02\n9:                 satisfaction Stratified  - OLS no Strata FE  4.446471e-02\n      std_error   p_value significant\n          &lt;num&gt;     &lt;num&gt;      &lt;char&gt;\n1: 1.160745e+00 0.8482370            \n2: 2.943325e-02 0.9315706            \n3: 3.080592e-02 0.9245019            \n4: 2.473571e-02 0.9515277            \n5: 3.112980e-02 0.2650800            \n6: 2.873209e+03 0.6062205            \n7: 3.104979e-02 0.6342580            \n8: 2.932947e-02 0.1675812            \n9: 7.016046e-02 0.5263844            \n\n\nThese should be very close to the t-tests above.\nNow let’s do this controlling for strata. The reason we’d want to do this is because, as we’ll see later, we need to estimate the treatment effect following the way the randomization was done, period. In other words, these need to be aligned. So, if we want to test our balance on baseline covariates, estimate the treatment effect once we have the endline data, it makes sense to include strata fixed effects here.\n\n# Function to perform OLS regressions with strata fixed effects\nrun_ols_with_strata &lt;- function(dt, title) {\n  results &lt;- data.table(\n    variable = character(),\n    method = character(),\n    coefficient = numeric(),\n    std_error = numeric(),\n    p_value = numeric(),\n    significant = character()\n  )\n  \n  for (var in baseline_vars) {\n    # Handle factor variables\n    if (is.factor(dt[[var]])) {\n      for (level in levels(dt[[var]])[2:length(levels(dt[[var]]))]) {  # Skip first level (reference)\n        dt[, temp := as.numeric(get(var) == level)]\n        \n        # Run regression with strata fixed effects\n        reg &lt;- feols(temp ~ treatment | strata, data = dt)\n        \n        # Add to results\n        results &lt;- rbind(results, data.table(\n          variable = paste0(var, \": \", level, \" vs \", levels(dt[[var]])[1]),\n          method = title,\n          coefficient = coef(reg)[\"treatment\"],\n          std_error = se(reg)[\"treatment\"],\n          p_value = pvalue(reg)[\"treatment\"],\n          significant = ifelse(pvalue(reg)[\"treatment\"] &lt; 0.05, \"*\", \"\")\n        ))\n        \n        # Remove temporary variable\n        dt[, temp := NULL]\n      }\n    } else {\n      # For continuous and binary variables\n      formula_str &lt;- paste0(var, \" ~ treatment | strata\")\n      reg &lt;- feols(as.formula(formula_str), data = dt)\n      \n      # Add to results\n      results &lt;- rbind(results, data.table(\n        variable = var,\n        method = title,\n        coefficient = coef(reg)[\"treatment\"],\n        std_error = se(reg)[\"treatment\"],\n        p_value = pvalue(reg)[\"treatment\"],\n        significant = ifelse(pvalue(reg)[\"treatment\"] &lt; 0.05, \"*\", \"\")\n      ))\n    }\n  }\n  \n  return(results)\n}\n\n# Run OLS with strata/pair fixed effects\nols_strat_fe &lt;- run_ols_with_strata(dt_strat, \"Stratified - OLS with strata FE\")\n\n# Display results\nprint(ols_strat_fe)\n\n                       variable                          method   coefficient\n                         &lt;char&gt;                          &lt;char&gt;         &lt;num&gt;\n1:                          age Stratified - OLS with strata FE    0.11773106\n2:   education: Primary vs None Stratified - OLS with strata FE    0.00000000\n3: education: Secondary vs None Stratified - OLS with strata FE    0.00000000\n4:    education: Higher vs None Stratified - OLS with strata FE    0.00000000\n5:                       female Stratified - OLS with strata FE   -0.03450692\n6:                       income Stratified - OLS with strata FE 1478.11717275\n7:                        rural Stratified - OLS with strata FE    0.01458971\n8:              chronic_disease Stratified - OLS with strata FE   -0.04085313\n9:                 satisfaction Stratified - OLS with strata FE    0.04348112\n      std_error   p_value significant\n          &lt;num&gt;     &lt;num&gt;      &lt;char&gt;\n1: 2.273435e-01 0.6105367            \n2:          NaN        NA        &lt;NA&gt;\n3:          NaN        NA        &lt;NA&gt;\n4:          NaN        NA        &lt;NA&gt;\n5: 2.982420e-02 0.2616078            \n6: 2.687661e+03 0.5887514            \n7: 2.894125e-02 0.6199764            \n8: 2.597797e-02 0.1323144            \n9: 8.089327e-02 0.5971538            \n\n\n\n\n\n\n\n\nNote\n\n\n\nNotice the regressions for education variables are not computing. Why do you think this is?\n\n\nStandardized Differences\nFor large samples, p-values become less informative as tiny imbalances become statistically significant. Standardized mean differences (SMDs) provide a sample size-independent measure:\n\\[SMD = \\frac{\\bar{X}_{treatment} - \\bar{X}_{control}}{\\sqrt{\\frac{s^2_{treatment} + s^2_{control}}{2}}}\\]\nA common rule of thumb is that an absolute SMD less than 0.1 indicates negligible imbalance.\n\n# Calculate standardized differences \nstandardized_diff &lt;- function(dt) {\n  covariates &lt;- c(\"age\", \"income\", \"female\", \"chronic_disease\")\n  results &lt;- data.table(\n    covariate = character(),\n    mean_treated = numeric(),\n    mean_control = numeric(),\n    sd_treated = numeric(),\n    sd_control = numeric(),\n    std_diff = numeric(),\n    stringsAsFactors = FALSE\n  )\n  \n  for (cov in covariates) {\n    # Calculate means and SDs by group\n    treated &lt;- dt[treatment == 1, get(cov)]  # Use get() to retrieve column dynamically\n    control &lt;- dt[treatment == 0, get(cov)]  # Use get() to retrieve column dynamically\n    \n    mean_treated &lt;- mean(treated, na.rm = TRUE)\n    mean_control &lt;- mean(control, na.rm = TRUE)\n    sd_treated &lt;- sd(treated, na.rm = TRUE)\n    sd_control &lt;- sd(control, na.rm = TRUE)\n    \n    # Calculate standardized difference\n    pooled_sd &lt;- sqrt((sd_treated^2 + sd_control^2) / 2)\n    std_diff &lt;- (mean_treated - mean_control) / pooled_sd\n    \n    results &lt;- rbind(results, data.table(\n      covariate = cov,\n      mean_treated = mean_treated,\n      mean_control = mean_control,\n      sd_treated = sd_treated,\n      sd_control = sd_control,\n      std_diff = std_diff\n    ))\n  }\n  \n  # Handle education separately (categorical variable)\n  if (\"education\" %in% names(dt)) {\n    # For each level of education\n    edu_levels &lt;- levels(dt$education)\n    for (level in edu_levels) {\n      # Create indicator for this level\n      dt[, temp := as.numeric(education == level)]\n      \n      # Calculate means by group\n      treated &lt;- dt[treatment == 1, mean(temp, na.rm = TRUE)]\n      control &lt;- dt[treatment == 0, mean(temp, na.rm = TRUE)]\n      \n      # For proportions, the SD is sqrt(p*(1-p))\n      sd_treated &lt;- sqrt(treated * (1-treated))\n      sd_control &lt;- sqrt(control * (1-control))\n      \n      # Calculate standardized difference\n      pooled_sd &lt;- sqrt((sd_treated^2 + sd_control^2) / 2)\n      # Avoid division by zero\n      if (pooled_sd == 0) {\n        std_diff &lt;- 0\n      } else {\n        std_diff &lt;- (treated - control) / pooled_sd\n      }\n      \n      results &lt;- rbind(results, data.table(\n        covariate = paste0(\"education: \", level),\n        mean_treated = treated,\n        mean_control = control,\n        sd_treated = sd_treated,\n        sd_control = sd_control,\n        std_diff = std_diff\n      ))\n      \n      # Remove temporary variable\n      dt[, temp := NULL]\n    }\n  }\n  \n  return(results)\n}\n\n# Calculate standardized differences\nstd_diffs &lt;- standardized_diff(dt_strat)\nprint(std_diffs)\n\n              covariate mean_treated mean_control   sd_treated   sd_control\n                 &lt;char&gt;        &lt;num&gt;        &lt;num&gt;        &lt;num&gt;        &lt;num&gt;\n1:                  age 4.958765e+01 4.936546e+01 1.833769e+01 1.836811e+01\n2:               income 3.522222e+04 3.374068e+04 4.624246e+04 4.459409e+04\n3:               female 5.717131e-01 6.064257e-01 4.953241e-01 4.890335e-01\n4:      chronic_disease 2.928287e-01 3.333333e-01 4.555144e-01 4.718785e-01\n5:      education: None 1.115538e-01 1.104418e-01 3.148167e-01 3.134396e-01\n6:   education: Primary 3.147410e-01 3.172691e-01 4.644127e-01 4.654132e-01\n7: education: Secondary 3.864542e-01 3.835341e-01 4.869367e-01 4.862465e-01\n8:    education: Higher 1.872510e-01 1.887550e-01 3.901129e-01 3.913139e-01\n       std_diff\n          &lt;num&gt;\n1:  0.012106399\n2:  0.032614530\n3: -0.070526901\n4: -0.087338046\n5:  0.003540005\n6: -0.005437661\n7:  0.006001020\n8: -0.003849426\n\n\nComprehensive “Table 1”\nNow, let’s create a comprehensive “Table 1” for the stratified random sample\n\n# Install and load required packages\nlibrary(tidyr)\nlibrary(dplyr)\nlibrary(gt)\n\n# Calculate standardized differences for stratified randomization\nstd_diff_strat &lt;- standardized_diff(dt_strat)\n\n# Function to extract p-values from all balance tests\nextract_pvalues &lt;- function(test_results) {\n  results &lt;- test_results[, .(variable, p_value)]\n  setnames(results, \"variable\", \"covariate\")\n  return(results)\n}\n\n# Extract p-values from different testing approaches for stratified randomization\npvals_strat_ttest &lt;- extract_pvalues(ttest_strat)\npvals_strat_ttest[, approach := \"t-test\"]\n\npvals_strat_ols &lt;- extract_pvalues(ols_strat)\npvals_strat_ols[, approach := \"OLS (no strata FE)\"]\n\npvals_strat_ols_fe &lt;- extract_pvalues(ols_strat_fe)\npvals_strat_ols_fe[, approach := \"OLS (with strata FE)\"]\n\n# Combine all p-values for stratified randomization\nstrat_pvals &lt;- rbind(\n  pvals_strat_ttest,\n  pvals_strat_ols,\n  pvals_strat_ols_fe\n)\n\n# Create a wide format of p-values\npvals_wide &lt;- strat_pvals %&gt;%\n  as.data.frame() %&gt;%\n  pivot_wider(\n    id_cols = covariate,\n    names_from = approach,\n    values_from = p_value\n  )\n\n# Join with standardized differences\nstrat_balance_table &lt;- merge(\n  std_diff_strat, \n  pvals_wide,\n  by = \"covariate\", \n  all = TRUE\n)\n\n# Mark significant values with asterisks directly in the data\nstrat_balance_table &lt;- strat_balance_table %&gt;%\n  mutate(\n    std_diff_mark = ifelse(abs(std_diff) &gt; 0.25, \n                           paste0(format(round(std_diff, 3), nsmall=3), \"†\"), \n                           format(round(std_diff, 3), nsmall=3)),\n    `t-test_mark` = ifelse(`t-test` &lt; 0.05, \n                          paste0(format(round(`t-test`, 3), nsmall=3), \"*\"), \n                          format(round(`t-test`, 3), nsmall=3)),\n    `OLS (no strata FE)_mark` = ifelse(`OLS (no strata FE)` &lt; 0.05, \n                                      paste0(format(round(`OLS (no strata FE)`, 3), nsmall=3), \"*\"), \n                                      format(round(`OLS (no strata FE)`, 3), nsmall=3)),\n    `OLS (with strata FE)_mark` = ifelse(`OLS (with strata FE)` &lt; 0.05, \n                                        paste0(format(round(`OLS (with strata FE)`, 3), nsmall=3), \"*\"), \n                                        format(round(`OLS (with strata FE)`, 3), nsmall=3))\n  )\n\n# Create gt table with the marked columns\nstrat_balance_table %&gt;%\n  select(covariate, std_diff_mark, `t-test_mark`, `OLS (no strata FE)_mark`, `OLS (with strata FE)_mark`) %&gt;%\n  gt() %&gt;%\n  # Rename columns\n  cols_label(\n    covariate = \"Baseline Covariate\",\n    std_diff_mark = \"Standardized Difference\",\n    `t-test_mark` = \"t-test\",\n    `OLS (no strata FE)_mark` = \"OLS (standard)\",\n    `OLS (with strata FE)_mark` = \"OLS (with strata FE)\"\n  ) %&gt;%\n  # Add column spanners for organization\n  tab_spanner(\n    label = \"Balance Measure\",\n    columns = \"std_diff_mark\"\n  ) %&gt;%\n  tab_spanner(\n    label = \"P-values by Testing Approach\",\n    columns = c(\"t-test_mark\", \"OLS (no strata FE)_mark\", \"OLS (with strata FE)_mark\")\n  ) %&gt;%\n  # Add title and footnotes\n  tab_header(\n    title = \"Table 1: Balance Assessment in Stratified Randomization\",\n    subtitle = \"Comparison of Different Testing Approaches\"\n  ) %&gt;%\n  tab_footnote(\n    footnote = \"† Standardized differences &gt;0.25, indicating potential imbalance.\",\n    locations = cells_column_labels(columns = \"std_diff_mark\")\n  ) %&gt;%\n  tab_footnote(\n    footnote = \"* P-values &lt;0.05, indicating statistically significant differences.\",\n    locations = cells_column_spanners(spanners = \"P-values by Testing Approach\")\n  ) %&gt;%\n  tab_footnote(\n    footnote = \"OLS with strata fixed effects is the most appropriate approach given the stratified design.\",\n    locations = cells_column_labels(columns = \"OLS (with strata FE)_mark\")\n  ) %&gt;%\n  # Nicer formatting\n  tab_options(\n    column_labels.font.weight = \"bold\",\n    column_labels.background.color = \"#EEEEEE\",\n    table.width = pct(100),\n    heading.background.color = \"#E6F0FF\",\n    heading.title.font.size = px(16),\n    heading.subtitle.font.size = px(14)\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 1: Balance Assessment in Stratified Randomization\n\n\nComparison of Different Testing Approaches\n\n\nBaseline Covariate\n\nBalance Measure\n\n\nP-values by Testing Approach1\n\n\n\nStandardized Difference2\n\nt-test\nOLS (standard)\nOLS (with strata FE)3\n\n\n\n\n\nage\n0.012\n0.848\n0.848\n0.611\n\n\nchronic_disease\n-0.087\n0.168\n0.168\n0.132\n\n\neducation: Higher\n-0.004\n0.952\nNA\nNA\n\n\neducation: Higher vs None\nNA\nNA\n0.952\nNA\n\n\neducation: None\n0.004\n0.955\nNA\nNA\n\n\neducation: Primary\n-0.005\n0.932\nNA\nNA\n\n\neducation: Primary vs None\nNA\nNA\n0.932\nNA\n\n\neducation: Secondary\n0.006\n0.925\nNA\nNA\n\n\neducation: Secondary vs None\nNA\nNA\n0.925\nNA\n\n\nfemale\n-0.071\n0.265\n0.265\n0.262\n\n\nincome\n0.033\n0.606\n0.606\n0.589\n\n\nrural\nNA\n0.634\n0.634\n0.620\n\n\nsatisfaction\nNA\n0.526\n0.526\n0.597\n\n\n\n\n\n1 * P-values &lt;0.05, indicating statistically significant differences.\n\n\n\n2 † Standardized differences &gt;0.25, indicating potential imbalance.\n\n\n\n3 OLS with strata fixed effects is the most appropriate approach given the stratified design.\n\n\n\n\n\n\nInterpreting Balance Results\nWhen reviewing balance tables, it’s important to consider:\n\nStatistical vs. Practical Significance: With large samples, even tiny differences can be statistically significant. Focus on the magnitude of differences (SMD) rather than p-values.\nMultiple Testing: With many covariates, expect some to show “significant” differences by chance. This is why joint tests and SMDs are often more informative.\nKey Predictors: Pay special attention to covariates that strongly predict outcomes. Imbalance on these variables is more concerning.\nVisual Inspection: Sometimes graphical displays of covariate distributions can complement statistical tests. Consider density plots or boxplots comparing treatment and control.\nOverall Assessment: Look at the pattern of results rather than focusing on any single test. If multiple measures suggest imbalance on important predictors, consider covariate adjustment in analysis.\n\nLet’s create visual assessments of balance as well:"
  },
  {
    "objectID": "unit-2/lec-2-2.html#complex-experimental-designs",
    "href": "unit-2/lec-2-2.html#complex-experimental-designs",
    "title": "Unit 2.2: Randomization Techniques",
    "section": "Complex Experimental Designs",
    "text": "Complex Experimental Designs\nBeyond the basic approaches, several complex randomization designs address specific research challenges:\nUnits of Randomization & Spillovers\nA critical decision is choosing what entity (e.g. person or cluster) to randomize:\nKey considerations:\n\nMatch observational unit when possible\nAlign with treatment delivery\nMinimize spillovers\nConsider statistical power\n\nCommon units:\n\n\nIndividual: Patients, students\n\nCluster: Villages, clinics, schools\n\nTime periods: Days, weeks, shifts\n\nNetworks: Households, peer groups\n\nThe tradeoff is often between statistical power (favoring individual randomization) and internal validity (which may require cluster randomization to minimize spillovers).\nThe Spillover Problem\nSpillovers occur when treatment affects untreated units, through:\n\nDirect interaction between units\nGeneral equilibrium effects\nInformation diffusion\nResource competition\n\nWhen spillovers are a concern, randomization at a higher level (clustering) can help minimize unwanted contamination. Alternatively, a two-stage randomization design can help measure spillover effects explicitly.\nTwo-Stage Randomization for Measuring Spillovers\nThis approach:\n\nFirst randomizes clusters to high or low treatment intensity\nThen randomizes individuals within clusters to treatment or control\n\nThis design allows measurement of:\n\nDirect treatment effects\nWithin-cluster spillovers\nBetween-cluster spillovers\nCluster Randomization\nCluster randomization assigns groups rather than individuals to treatment conditions: (we saw this last unit)\nExamples of clusters:\n\nSchools\nClinics\nVillages\nNeighborhoods\n\nThe key parameter affecting statistical power is the intraclass correlation coefficient (ICC), which measures how correlated outcomes are within clusters. The design effect formula shows how clustering reduces effective sample size:\n\\[DE = 1 + (m-1) \\times ICC\\]\nwhere \\(m\\) is the average cluster size. A larger design effect means a larger required sample size to achieve the same power.\nAnalysis needs to account for clustering through one of:\n\nCluster-robust standard errors\nMixed-effects models\nGeneralized estimating equations (GEE)\nFactorial Designs: Testing Multiple Treatments\nFactorial designs test multiple interventions simultaneously:\n\n\nIntervention B\nNo Intervention A\nIntervention A\n\n\n\nNo\nControl\nA only\n\n\nYes\nB only\nA and B\n\n\n\nThis approach:\n\nTests multiple treatments simultaneously\nEstimates main effects AND interactions\nUses resources efficiently\n\nFor example, with two interventions (A and B), we have four groups:\n\nNo intervention (control)\nIntervention A only\nIntervention B only\nBoth A and B\n\nFactorial designs are particularly valuable when:\n\nYou want to test combinations of interventions\nYou suspect treatments might interact (enhance or interfere with each other)\nYou need to maximize efficiency with limited resources\n\nHere’s how we might implement a 2×2 factorial design in R:\n\nlibrary(data.table)\nset.seed(072311)\nn &lt;- 200  # Total sample size\n\n# Create data table\ndt &lt;- data.table(id = 1:n)\n\n# First randomization for factor A\ndt[, A := c(rep(1, n/2), rep(0, n/2))[sample(1:n)]]\n\n# Second randomization for factor B\ndt[, B := c(rep(1, n/2), rep(0, n/2))[sample(1:n)]]\n\n# Create combined treatment variable\ndt[, treatment := paste0(\"A\", A, \"B\", B)]\n\n# Check allocation\ndt[, .N, by = treatment]\n\n   treatment     N\n      &lt;char&gt; &lt;int&gt;\n1:      A1B1    49\n2:      A0B1    51\n3:      A1B0    51\n4:      A0B0    49\n\n\nRandomized Phase-in Designs\nIn randomized phase-in designs, all units eventually receive treatment, but the timing of treatment is randomized.\nAdvantages:\n\nImproves compliance since everyone gets treatment eventually\nWorks well with resource constraints\nOffers a politically appealing compromise\n\nLimitations:\n\nMay create anticipation effects\nLimits long-term impact measurement\nEventually loses the control group\nAdaptive Randomization\nAdaptive designs update assignment probabilities based on accumulated data, balancing:\n\n\nExploration: Learning which treatment works best\n\nExploitation: Assigning more units to better treatments\n\nThe key *ethical** advantage is that fewer participants receive inferior treatments as evidence accumulates. We’ll cover adaptive designs in more detail in a future lecture.\nA simple example of response-adaptive randomization might look like this:\n\n# Simplified response-adaptive randomization simulation\nsimulate_adaptive_randomization &lt;- function(n_total = 100, \n                                          true_success_A = 0.3, \n                                          true_success_B = 0.5) {\n  # Initialize data storage\n  results &lt;- data.frame(\n    id = 1:n_total,\n    treatment = NA,\n    outcome = NA\n  )\n  \n  # Initial equal assignment probabilities\n  prob_A &lt;- 0.5\n  \n  # Track successes and failures\n  success_A &lt;- 0\n  total_A &lt;- 0\n  success_B &lt;- 0\n  total_B &lt;- 0\n  \n  # Simulate sequential enrollment\n  for (i in 1:n_total) {\n    # Assign treatment based on current probability\n    treatment &lt;- sample(c(\"A\", \"B\"), 1, prob = c(prob_A, 1-prob_A))\n    results$treatment[i] &lt;- treatment\n    \n    # Generate outcome based on true success rates\n    if (treatment == \"A\") {\n      outcome &lt;- rbinom(1, 1, true_success_A)\n      success_A &lt;- success_A + outcome\n      total_A &lt;- total_A + 1\n    } else {\n      outcome &lt;- rbinom(1, 1, true_success_B)\n      success_B &lt;- success_B + outcome\n      total_B &lt;- total_B + 1\n    }\n    results$outcome[i] &lt;- outcome\n    \n    # Update probability for next assignment using Beta prior\n    if (i &lt; n_total) {  # No need to update after last patient\n      # Beta-Bernoulli update (adding 1 for prior)\n      prob_A &lt;- rbeta(1, success_A + 1, total_A - success_A + 1) /\n               (rbeta(1, success_A + 1, total_A - success_A + 1) + \n                rbeta(1, success_B + 1, total_B - success_B + 1))\n    }\n  }\n  \n  return(results)\n}"
  },
  {
    "objectID": "unit-2/lec-2-2.html#practical-implementation-tips",
    "href": "unit-2/lec-2-2.html#practical-implementation-tips",
    "title": "Unit 2.2: Randomization Techniques",
    "section": "Practical Implementation Tips",
    "text": "Practical Implementation Tips\nRegardless of which randomization approach you choose, follow these best practices:\n\nCreate a single entry per randomization unit\nSort the file in a reproducible way\nSet and preserve a random seed\nAssign treatments\nSave assignments securely\nTest balance extensively\n\nAlways document your randomization procedure thoroughly to enhance transparency and reproducibility.\nBelow is a template for implementing and documenting randomization:\n\n# Randomization implementation template\n\n# 1. Document parameters\nstudy_name &lt;- \"My Clinical Trial\"\nrandomization_date &lt;- \"2025-02-25\"\nrandomization_conducted_by &lt;- \"J. Smith\"\nrandom_seed &lt;- 072311  # Document why this seed was chosen\nallocation_ratio &lt;- 0.5  # Equal allocation\nstratification_variables &lt;- c(\"site\", \"gender\")\n\n# 2. Set up reproducible environment\nset.seed(random_seed)\nlibrary(tidyverse)\n\n# 3. Load and prepare data\ndata &lt;- read.csv(\"participant_data.csv\")\n\n# 4. Implement randomization\n# Example: stratified randomization\n\n\n# 5. Check balance\n# Generate Balance Table\n\n# 6. Save results securely\n# a) Save randomization details\nrandomization_log &lt;- data.frame(\n  study_name = study_name,\n  date = randomization_date,\n  conducted_by = randomization_conducted_by,\n  seed = random_seed,\n  method = \"stratified\",\n  stratification_variables = paste(stratification_variables, collapse = \", \")\n)\nwrite.csv(randomization_log, \"randomization_log.csv\", row.names = FALSE)\n\n# b) Save assignment data\nwrite.csv(\n  data %&gt;% select(id, treatment, stratum_id), \n  \"treatment_assignments.csv\", \n  row.names = FALSE\n)\n\n# c) Save balance checks\nwrite.csv(balance_table, \"balance_checks.csv\", row.names = FALSE)"
  },
  {
    "objectID": "unit-2/lec-2-2.html#ethical-considerations-in-randomization",
    "href": "unit-2/lec-2-2.html#ethical-considerations-in-randomization",
    "title": "Unit 2.2: Randomization Techniques",
    "section": "Ethical Considerations in Randomization",
    "text": "Ethical Considerations in Randomization\nRandomization raises several ethical considerations:\n\n\nLong-term benefits vs. short-term resource distribution\n\nEquity in who receives potentially beneficial treatments\n\nTransparency with participants about randomization\n\nMinimizing harm from potentially ineffective interventions\n\nThese issues must be carefully considered and addressed in the design phase. Specific approaches that can address ethical concerns include:\n\n\nRandomized phase-in designs: Ensures everyone eventually receives the intervention\n\nAdaptive randomization: Skews allocation toward better-performing treatments over time\n\nRisk-based randomization: Targets interventions toward those most likely to benefit (we’ll cover in the future if time)\n\nMinimization of control group size: Uses unequal allocation ratios to minimize the number of participants not receiving intervention (could add this consideration to optimal allocation)"
  },
  {
    "objectID": "unit-2/lec-2-2.html#conclusion-which-method-when",
    "href": "unit-2/lec-2-2.html#conclusion-which-method-when",
    "title": "Unit 2.2: Randomization Techniques",
    "section": "Conclusion: Which Method When?",
    "text": "Conclusion: Which Method When?\nThere is no one-size-fits-all approach. The best randomization strategy depends on:\n\nThe research question\nAvailable baseline data\nLogistical constraints\nExpected heterogeneity in treatment effects\n\n\n\n\n\n\n\n\nApproach\nWhen to Use\nKey Consideration\n\n\n\nSimple\nLarge samples\nSimplicity\n\n\nStratified\nStrong predictors known\nNumber of strata\n\n\nMatched-Pair\nSmall samples\nFinding good matches\n\n\nRe-randomization\nBalance is critical\nComplexity of inference\n\n\nCluster\nGroup-level intervention\nICC and number of clusters\n\n\n\nThe choice of randomization method should be guided by:\n\nThe unit of randomization (individual vs. cluster)\nThe importance of balance on specific variables\nFeasibility constraints\nAnalysis plans\n\nHappy randomizing!"
  },
  {
    "objectID": "unit-2/unit-2-design-2.html",
    "href": "unit-2/unit-2-design-2.html",
    "title": "Unit 2.2: How to Randomize",
    "section": "",
    "text": "Read:\n\nJ-Pal Web Guide on Randomization\nEGAP: 10 Things to Know about Randomization\nBruhn and McKenzie, “In Pursuit of Balance”\n\n\n\n\n\n\nLecture Notes: Randomization Techniques\n\nLecture Slides\n\n\n\n\n\n\n(Comming Soon)",
    "crumbs": [
      "Unit 2: Design of Experiments",
      "Unit 2.2: How to Randomize"
    ]
  },
  {
    "objectID": "unit-2/unit-2-design-2.html#unit-overview",
    "href": "unit-2/unit-2-design-2.html#unit-overview",
    "title": "Unit 2.2: How to Randomize",
    "section": "",
    "text": "Read:\n\nJ-Pal Web Guide on Randomization\nEGAP: 10 Things to Know about Randomization\nBruhn and McKenzie, “In Pursuit of Balance”\n\n\n\n\n\n\nLecture Notes: Randomization Techniques\n\nLecture Slides\n\n\n\n\n\n\n(Comming Soon)",
    "crumbs": [
      "Unit 2: Design of Experiments",
      "Unit 2.2: How to Randomize"
    ]
  },
  {
    "objectID": "unit-2/unit-2-design-1.html",
    "href": "unit-2/unit-2-design-1.html",
    "title": "Unit 2.1: Optimal Experimental Design",
    "section": "",
    "text": "Read:\n\nList, Sadoff, Wagner, “So you want to run an experiment, now what? Some simple rules of thumb for optimal experimental design”\nOptional:\n\nSection 4 of Duflo, Glennerster, and Kremer, “USING RANDOMIZATION IN DEVELOPMENT ECONOMICS RESEARCH: A TOOLKIT”\nMcKenzie,Beyond baseline and follow-up: The case for more T in experiments\nWorld Bank Blog: Seven Ways to Improve Statistical Power without Increasing n\n\n\n\n\n\n\n\nLecture Notes: Optimal Experimental Design\n\nLecture Slides\n\nOptimal Experimental Design Shiny App\n\n\n\n\n\nNone yet!",
    "crumbs": [
      "Unit 2: Design of Experiments",
      "Unit 2.1: Optimal Experimental Design"
    ]
  },
  {
    "objectID": "unit-2/unit-2-design-1.html#unit-overview",
    "href": "unit-2/unit-2-design-1.html#unit-overview",
    "title": "Unit 2.1: Optimal Experimental Design",
    "section": "",
    "text": "Read:\n\nList, Sadoff, Wagner, “So you want to run an experiment, now what? Some simple rules of thumb for optimal experimental design”\nOptional:\n\nSection 4 of Duflo, Glennerster, and Kremer, “USING RANDOMIZATION IN DEVELOPMENT ECONOMICS RESEARCH: A TOOLKIT”\nMcKenzie,Beyond baseline and follow-up: The case for more T in experiments\nWorld Bank Blog: Seven Ways to Improve Statistical Power without Increasing n\n\n\n\n\n\n\n\nLecture Notes: Optimal Experimental Design\n\nLecture Slides\n\nOptimal Experimental Design Shiny App\n\n\n\n\n\nNone yet!",
    "crumbs": [
      "Unit 2: Design of Experiments",
      "Unit 2.1: Optimal Experimental Design"
    ]
  },
  {
    "objectID": "unit-5/lec-5-0-slides.html#learning-objectives",
    "href": "unit-5/lec-5-0-slides.html#learning-objectives",
    "title": "Experimental Analysis Challenges",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\n\nIdentify the main threats to internal validity that can arise while implementing an experiment\nUnderstand how these threats can bias the causal inference\nDiscuss strategies to mitigate these threats during implementation\nLearn some strategies to account for threats during analysis\n\nIntention-to-treat analysis\nLocal average treatment effect",
    "crumbs": [
      "Unit 5: Threats to Validity",
      "Lecture Slides: Unit 5.0: Threats to Validity"
    ]
  },
  {
    "objectID": "unit-5/lec-5-0-slides.html#review-the-four-key-exclusion-restrictions",
    "href": "unit-5/lec-5-0-slides.html#review-the-four-key-exclusion-restrictions",
    "title": "Experimental Analysis Challenges",
    "section": "Review: The Four Key Exclusion Restrictions",
    "text": "Review: The Four Key Exclusion Restrictions\n\nFour critical assumptions needed for valid causal inference:\n\n\nStable Unit Treatment Value Assumption (SUTVA): No interference between units and no hidden treatment versions\nObservability: Outcomes are observed for all units\nStatistical Independence: Treatment assignment independent of potential outcomes\nComplete Compliance: Treatment received equals treatment assigned",
    "crumbs": [
      "Unit 5: Threats to Validity",
      "Lecture Slides: Unit 5.0: Threats to Validity"
    ]
  },
  {
    "objectID": "unit-5/lec-5-0-slides.html#things-can-and-do-go-wrong-in-real-world-experiments",
    "href": "unit-5/lec-5-0-slides.html#things-can-and-do-go-wrong-in-real-world-experiments",
    "title": "Experimental Analysis Challenges",
    "section": "Things can (and do) go wrong in real-world experiments",
    "text": "Things can (and do) go wrong in real-world experiments\n\n\n\nDuring the conception phase, we design an experiment that enables us to answer our research questions\n\nBut in the implementation phase,\nmany things can go wrong",
    "crumbs": [
      "Unit 5: Threats to Validity",
      "Lecture Slides: Unit 5.0: Threats to Validity"
    ]
  },
  {
    "objectID": "unit-5/lec-5-0-slides.html#lecture-outline",
    "href": "unit-5/lec-5-0-slides.html#lecture-outline",
    "title": "Experimental Analysis Challenges",
    "section": "Lecture Outline",
    "text": "Lecture Outline\n\n1. Threats to Internal Validity\n\nSpillover effects: Units affect each other, violating 1. SUTVA\nHidden variation / Implementation Heterogeneity: Treatment varies across units, violating 1. SUTVA\nAttrition: Participants drop out, violating 2. Observability\nPartial or Non-compliance: Units don’t follow assigned treatment, violating 4. Complete Compliance\n\n2. Addressing Threats to Internal Validity in Analysis\n\nIntention-to-treat (ITT) analysis\nLocal average treatment effect (LATE)",
    "crumbs": [
      "Unit 5: Threats to Validity",
      "Lecture Slides: Unit 5.0: Threats to Validity"
    ]
  },
  {
    "objectID": "unit-5/lec-5-0-slides.html#sutva-two-critical-components",
    "href": "unit-5/lec-5-0-slides.html#sutva-two-critical-components",
    "title": "Experimental Analysis Challenges",
    "section": "SUTVA: Two Critical Components",
    "text": "SUTVA: Two Critical Components\n\nNo interference between units (No Spillovers):\n\n\nOne unit’s treatment does not affect another unit’s potential outcomes\n\\(Y_i(T_1,...,T_N) = Y_i(T_i)\\): The only effect of the treatment on unit \\(i\\) is through its own treatment\n\n\nNo hidden treatment versions:\n\n\nTreatment is identical for all units who receive it\nNo variation in treatment quality or implementation\nWhen violated: Biased treatment effect estimates and ambiguous causal interpretations",
    "crumbs": [
      "Unit 5: Threats to Validity",
      "Lecture Slides: Unit 5.0: Threats to Validity"
    ]
  },
  {
    "objectID": "unit-5/lec-5-0-slides.html#randomization",
    "href": "unit-5/lec-5-0-slides.html#randomization",
    "title": "Experimental Analysis Challenges",
    "section": "Randomization",
    "text": "Randomization",
    "crumbs": [
      "Unit 5: Threats to Validity",
      "Lecture Slides: Unit 5.0: Threats to Validity"
    ]
  },
  {
    "objectID": "unit-5/lec-5-0-slides.html#sutva-violation-spillover-effects",
    "href": "unit-5/lec-5-0-slides.html#sutva-violation-spillover-effects",
    "title": "Experimental Analysis Challenges",
    "section": "SUTVA Violation: Spillover Effects",
    "text": "SUTVA Violation: Spillover Effects\nSpillovers occur when the outcomes of untreated units are indirectly affected by the treatment given to others.\n\nSpillovers violate the key assumption that one unit’s treatment assignment has no effect on the outcomes of other units\nSpillovers are not limited to subjects in the study sample, but can affect anyone who is not treated\nCommon causes: geographic proximity, social networks\nMake it difficult or impossible to measure the impact of the program – Comparison group no longer serves as a valid estimate of the counterfactual",
    "crumbs": [
      "Unit 5: Threats to Validity",
      "Lecture Slides: Unit 5.0: Threats to Validity"
    ]
  },
  {
    "objectID": "unit-5/lec-5-0-slides.html#spillovers---outcomes",
    "href": "unit-5/lec-5-0-slides.html#spillovers---outcomes",
    "title": "Experimental Analysis Challenges",
    "section": "Spillovers - Outcomes",
    "text": "Spillovers - Outcomes\nSpillovers may not put a study in jeopardy if they are contained or measured, but are problematic if they affect the comparison group\n\nSpillovers can be positive (+) or negative (-)\n\n(+) Positive spillovers: comparison group benefits from treatment group\n(-) Negative spillovers: comparison group is harmed by treatment group\n\nSpillovers can cause impact to be underestimated or overestimated\nChannels: physical, informational/behavioral, and marketwide/general equilibrium",
    "crumbs": [
      "Unit 5: Threats to Validity",
      "Lecture Slides: Unit 5.0: Threats to Validity"
    ]
  },
  {
    "objectID": "unit-5/lec-5-0-slides.html#spillovers---example-1",
    "href": "unit-5/lec-5-0-slides.html#spillovers---example-1",
    "title": "Experimental Analysis Challenges",
    "section": "Spillovers - Example 1",
    "text": "Spillovers - Example 1\nHandwashing promotion campaign that provided soap and education about health benefits of and handwashing\nHow might spillovers occur?\nHow might they affect the outcome of the study?",
    "crumbs": [
      "Unit 5: Threats to Validity",
      "Lecture Slides: Unit 5.0: Threats to Validity"
    ]
  },
  {
    "objectID": "unit-5/lec-5-0-slides.html#spillovers---example-2",
    "href": "unit-5/lec-5-0-slides.html#spillovers---example-2",
    "title": "Experimental Analysis Challenges",
    "section": "Spillovers - Example 2",
    "text": "Spillovers - Example 2\nClinic-based intervention to improve the quality of care\nHow might spillovers occur?\nHow might they affect the outcome of the study?",
    "crumbs": [
      "Unit 5: Threats to Validity",
      "Lecture Slides: Unit 5.0: Threats to Validity"
    ]
  },
  {
    "objectID": "unit-5/lec-5-0-slides.html#sutva-what-and-why",
    "href": "unit-5/lec-5-0-slides.html#sutva-what-and-why",
    "title": "Experimental Analysis Challenges",
    "section": "SUTVA: What and Why",
    "text": "SUTVA: What and Why\n\nStable Unit Treatment Value Assumption (SUTVA) consists of two key components:\n\nNo interference between units\nNo hidden versions of treatments\n\nCritical assumption for identification in experimental research\nWhen SUTVA is violated:\n\nStandard estimators become biased\nCausal effects are not uniquely defined\nExternal validity is threatened",
    "crumbs": [
      "Unit 5: Threats to Validity",
      "Lecture Slides: Unit 5.0: Threats to Validity"
    ]
  },
  {
    "objectID": "unit-5/lec-5-0-slides.html#sutva-component-1-no-interference-between-units",
    "href": "unit-5/lec-5-0-slides.html#sutva-component-1-no-interference-between-units",
    "title": "Experimental Analysis Challenges",
    "section": "SUTVA Component 1: No Interference Between Units",
    "text": "SUTVA Component 1: No Interference Between Units\n\nDefinition: A unit’s potential outcomes are unaffected by other units’ treatment assignment\nFormal expression: For all units i and j (j≠i):\n\n\\(Y_i(d_i, d_j) = Y_i(d_i, d'_j)\\) for all possible treatments \\(d_j, d'_j\\)\n\nOften violated in:\n\nSocial network interventions\nEducational interventions\nMarket-based programs\nPublic health interventions",
    "crumbs": [
      "Unit 5: Threats to Validity",
      "Lecture Slides: Unit 5.0: Threats to Validity"
    ]
  },
  {
    "objectID": "unit-5/lec-5-0-slides.html#interference-spillover-effects",
    "href": "unit-5/lec-5-0-slides.html#interference-spillover-effects",
    "title": "Experimental Analysis Challenges",
    "section": "Interference (Spillover Effects)",
    "text": "Interference (Spillover Effects)\n\nDefinition: One unit’s treatment affects another unit’s outcome\nExamples:\n\nInformation spreads through social networks\nMarket equilibrium effects\nBehavioral responses to others’ treatment\n\nConsequence: Standard difference-in-means estimator becomes biased",
    "crumbs": [
      "Unit 5: Threats to Validity",
      "Lecture Slides: Unit 5.0: Threats to Validity"
    ]
  },
  {
    "objectID": "unit-5/lec-5-0-slides.html#real-world-violations-of-no-interference",
    "href": "unit-5/lec-5-0-slides.html#real-world-violations-of-no-interference",
    "title": "Experimental Analysis Challenges",
    "section": "Real-World Violations of No Interference",
    "text": "Real-World Violations of No Interference\n\n\nInformation Spillovers: Knowledge or behaviors spread through social networks\n\nExample: Agricultural technology adoption spreads to neighboring farmers\n\nPhysical Spillovers: Treatment physically affects nearby untreated units\n\nExample: Vaccine interventions create herd immunity\n\nPsychological Spillovers: Awareness of others’ treatment affects behavior\n\nExample: Control group students become discouraged when peers receive tutoring\n\nMarket Equilibrium Effects: Interventions shift prices/resources for all participants\n\nExample: Job training program affects labor market for all job seekers",
    "crumbs": [
      "Unit 5: Threats to Validity",
      "Lecture Slides: Unit 5.0: Threats to Validity"
    ]
  },
  {
    "objectID": "unit-5/lec-5-0-slides.html#types-of-treatment-effects-with-interference",
    "href": "unit-5/lec-5-0-slides.html#types-of-treatment-effects-with-interference",
    "title": "Experimental Analysis Challenges",
    "section": "Types of Treatment Effects with Interference",
    "text": "Types of Treatment Effects with Interference\n\nDirect treatment effect: Impact on treated units\nSpillover effect (indirect treatment effect): Impact on untreated units exposed to treated units\nOverall treatment effect: Combination of direct and spillover effects",
    "crumbs": [
      "Unit 5: Threats to Validity",
      "Lecture Slides: Unit 5.0: Threats to Validity"
    ]
  },
  {
    "objectID": "unit-5/lec-5-0-slides.html#formalizing-interference-under-sutva-violations",
    "href": "unit-5/lec-5-0-slides.html#formalizing-interference-under-sutva-violations",
    "title": "Experimental Analysis Challenges",
    "section": "Formalizing Interference Under SUTVA Violations",
    "text": "Formalizing Interference Under SUTVA Violations\n\nWhen SUTVA is violated, potential outcomes depend on the entire treatment assignment vector:\n\n\\(Y_i(d_i, \\mathbf{d}_{-i})\\) instead of just \\(Y_i(d_i)\\)\n\nFor N units and binary treatment, this means \\(2^N\\) potential outcomes instead of 2!\nComplicates estimation: more potential outcomes than observable data points",
    "crumbs": [
      "Unit 5: Threats to Validity",
      "Lecture Slides: Unit 5.0: Threats to Validity"
    ]
  },
  {
    "objectID": "unit-5/lec-5-0-slides.html#parameter-estimation-under-interference",
    "href": "unit-5/lec-5-0-slides.html#parameter-estimation-under-interference",
    "title": "Experimental Analysis Challenges",
    "section": "Parameter Estimation Under Interference",
    "text": "Parameter Estimation Under Interference\n\nDirect Treatment Effect: Effect on treated units \\(\\tau_{direct} = E[Y_i(1,\\mathbf{d}_{-i}) - Y_i(0,\\mathbf{d}_{-i})]\\)\nIndirect (Spillover) Effect: Effect on untreated units \\(\\tau_{indirect} = E[Y_i(0,\\mathbf{d}_{-i}) - Y_i(0,\\mathbf{0})]\\)\nTotal Effect: Combined direct and spillover effects \\(\\tau_{total} = p\\tau_{direct} + (1-p)\\tau_{indirect}\\)\nWhere p is the proportion treated",
    "crumbs": [
      "Unit 5: Threats to Validity",
      "Lecture Slides: Unit 5.0: Threats to Validity"
    ]
  },
  {
    "objectID": "unit-5/lec-5-0-slides.html#approaches-to-deal-with-interference",
    "href": "unit-5/lec-5-0-slides.html#approaches-to-deal-with-interference",
    "title": "Experimental Analysis Challenges",
    "section": "Approaches to Deal with Interference",
    "text": "Approaches to Deal with Interference\n\nLinear-in-means model\n\nAssumes linear relationship between outcomes and own/group treatment\n\nClustered randomized trials\n\nRandomize at group level to attenuate between-group spillovers\n\nRandomized saturation designs\n\nVary proportion of treated units within different groups\nEstimate spillover effects across groups with different saturation levels",
    "crumbs": [
      "Unit 5: Threats to Validity",
      "Lecture Slides: Unit 5.0: Threats to Validity"
    ]
  },
  {
    "objectID": "unit-5/lec-5-0-slides.html#linear-in-means-model",
    "href": "unit-5/lec-5-0-slides.html#linear-in-means-model",
    "title": "Experimental Analysis Challenges",
    "section": "Linear-in-Means Model",
    "text": "Linear-in-Means Model\n\nA common approach to estimating spillover effects:\n\n\\[Y_{ig} = \\alpha + \\beta D_{ig} + \\gamma S_{-ig} + \\delta X_{ig} + \\varepsilon_{ig}\\]\nWhere:\n\n\\(D_{ig}\\) is the treatment indicator for individual i in group g\n\\(S_{-ig}\\) is the proportion of others treated in the group\n\\(\\beta\\) captures direct effects\n\\(\\gamma\\) captures spillover effects\nLimitation: Assumes spillovers are linear in proportion treated",
    "crumbs": [
      "Unit 5: Threats to Validity",
      "Lecture Slides: Unit 5.0: Threats to Validity"
    ]
  },
  {
    "objectID": "unit-5/lec-5-0-slides.html#sutva-component-2-no-hidden-versions-of-treatment",
    "href": "unit-5/lec-5-0-slides.html#sutva-component-2-no-hidden-versions-of-treatment",
    "title": "Experimental Analysis Challenges",
    "section": "SUTVA Component 2: No Hidden Versions of Treatment",
    "text": "SUTVA Component 2: No Hidden Versions of Treatment\n\nDefinition: The treatment is identical across all units who receive it\nFormal expression: For a given treatment level d, all units receive exactly the same intervention\nCommon violations:\n\nProvider/implementer differences\nTreatment quality variations\nContextual variations\nMultiple treatment paths",
    "crumbs": [
      "Unit 5: Threats to Validity",
      "Lecture Slides: Unit 5.0: Threats to Validity"
    ]
  },
  {
    "objectID": "unit-5/lec-5-0-slides.html#real-world-hidden-treatment-variations",
    "href": "unit-5/lec-5-0-slides.html#real-world-hidden-treatment-variations",
    "title": "Experimental Analysis Challenges",
    "section": "Real-World Hidden Treatment Variations",
    "text": "Real-World Hidden Treatment Variations\n\n\nProvider Characteristics: Different implementation styles\n\nExample: Instructor quality in educational interventions\n\nDelivery Context: Variations in setting/environment\n\nExample: Telemedicine delivered across different internet connection speeds\n\nTreatment Intensity: Differences in “dosage” received\n\nExample: Varying attendance in a health education program\n\nUnintended Co-interventions: Unplanned additional treatments\n\nExample: Some doctors providing extra advice with prescribed medication",
    "crumbs": [
      "Unit 5: Threats to Validity",
      "Lecture Slides: Unit 5.0: Threats to Validity"
    ]
  },
  {
    "objectID": "unit-5/lec-5-0-slides.html#special-case-evaluation-driven-effects",
    "href": "unit-5/lec-5-0-slides.html#special-case-evaluation-driven-effects",
    "title": "Experimental Analysis Challenges",
    "section": "Special Case: “Evaluation-driven effects”",
    "text": "Special Case: “Evaluation-driven effects”\nEvaluation-driven effects occur when respondents change their behavior in response to the evaluation itself instead of the intervention.\nCommon causes: salience of being evaluated, social pressure",
    "crumbs": [
      "Unit 5: Threats to Validity",
      "Lecture Slides: Unit 5.0: Threats to Validity"
    ]
  },
  {
    "objectID": "unit-5/lec-5-0-slides.html#special-case-evaluation-driven-effects-1",
    "href": "unit-5/lec-5-0-slides.html#special-case-evaluation-driven-effects-1",
    "title": "Experimental Analysis Challenges",
    "section": "Special Case: “Evaluation-driven effects”",
    "text": "Special Case: “Evaluation-driven effects”\nThese include observer-driven effects and enumerator effects:\n\nHawthorne effects: Behavior changes due to attention from the study or intervention\nAnticipation effects: Comparison group changes behavior because they expect to receive the treatment later (particular concern for phase-ins)\nResentment/demoralization effects: Comparison group resents missing out on treatment and changes behavior\nDemand effects: Behavior changes due to perceptions of evaluator’s objectives\nSurvey effects: Being surveyed changes subsequent behavior",
    "crumbs": [
      "Unit 5: Threats to Validity",
      "Lecture Slides: Unit 5.0: Threats to Validity"
    ]
  },
  {
    "objectID": "unit-5/lec-5-0-slides.html#what-can-we-do-about-it-spillover-effects",
    "href": "unit-5/lec-5-0-slides.html#what-can-we-do-about-it-spillover-effects",
    "title": "Experimental Analysis Challenges",
    "section": "What can we do about it? Spillover Effects",
    "text": "What can we do about it? Spillover Effects\nApproach 1: Avoid Spillovers\n\nChoose level of randomization wisely, and randomize at a higher level if concerned about spillovers\nUse spatial or institutional buffer to limit spillovers between treatment and control groups\n\nApproach 2: Measure Spillovers\n\nCollect data on spillovers\nTwo-stage randomization",
    "crumbs": [
      "Unit 5: Threats to Validity",
      "Lecture Slides: Unit 5.0: Threats to Validity"
    ]
  },
  {
    "objectID": "unit-5/lec-5-0-slides.html#what-can-we-do-about-it-hidden-treatment-versions",
    "href": "unit-5/lec-5-0-slides.html#what-can-we-do-about-it-hidden-treatment-versions",
    "title": "Experimental Analysis Challenges",
    "section": "What can we do about it? Hidden Treatment Versions",
    "text": "What can we do about it? Hidden Treatment Versions\n\n\nStandardization: Create detailed protocols and implementation guidelines\n\nTraining sessions for implementers\nChecklists for implementation\nQuality monitoring systems\n\nMeasurement: Record potential sources of variation\n\nImplementer characteristics\nTreatment delivery details\nCompliance/adherence measures\n\nStatistical Adjustment: Control for observed variations\n\nInclude variation measures as covariates\nConduct subgroup analyses\nUse instrumental variables when appropriate",
    "crumbs": [
      "Unit 5: Threats to Validity",
      "Lecture Slides: Unit 5.0: Threats to Validity"
    ]
  },
  {
    "objectID": "unit-5/lec-5-0-slides.html#the-attrition-problem",
    "href": "unit-5/lec-5-0-slides.html#the-attrition-problem",
    "title": "Experimental Analysis Challenges",
    "section": "The Attrition Problem",
    "text": "The Attrition Problem\n\n\nAttrition: Participants dropping out of a study after initial assignment\nConsequence: Violates the Observability assumption\nEven small amounts of attrition can compromise identification",
    "crumbs": [
      "Unit 5: Threats to Validity",
      "Lecture Slides: Unit 5.0: Threats to Validity"
    ]
  },
  {
    "objectID": "unit-5/lec-5-0-slides.html#observability",
    "href": "unit-5/lec-5-0-slides.html#observability",
    "title": "Experimental Analysis Challenges",
    "section": "Observability",
    "text": "Observability\n\nDefinition: Outcomes are observed for all units, regardless of treatment status\n\n\\(P(R_i = 1) = 1\\) for all units \\(i\\)\nWhere \\(R_i\\) indicates whether outcome is observed\n\nCommon violations:\n\nSurvey non-response\nParticipant dropout (Attrition)\n\nWhen violated: Potential selection bias if attrition is related to treatment or potential outcomes",
    "crumbs": [
      "Unit 5: Threats to Validity",
      "Lecture Slides: Unit 5.0: Threats to Validity"
    ]
  },
  {
    "objectID": "unit-5/lec-5-0-slides.html#attrition-bias-example",
    "href": "unit-5/lec-5-0-slides.html#attrition-bias-example",
    "title": "Experimental Analysis Challenges",
    "section": "Attrition Bias Example",
    "text": "Attrition Bias Example\nSuppose we were interested in testing the impact of students wearing eyeglasses on test scores. What would we want to know?\nConsider these scenarios:\n\nIf everyone is in our data in the treatment and control group, what is the result?\nHow does this change if some people are missing from baseline only? From endline only?\nWhat if under-performing children don’t come to school when we give exams?",
    "crumbs": [
      "Unit 5: Threats to Validity",
      "Lecture Slides: Unit 5.0: Threats to Validity"
    ]
  },
  {
    "objectID": "unit-5/lec-5-0-slides.html#why-is-attrition-an-issue",
    "href": "unit-5/lec-5-0-slides.html#why-is-attrition-an-issue",
    "title": "Experimental Analysis Challenges",
    "section": "Why Is Attrition an Issue?",
    "text": "Why Is Attrition an Issue?\n\nCompletely random attrition\n\nPrimarily affects statistical power (smaller sample size)\nDoesn’t necessarily invalidate treatment effect estimate\n\nNon-random (selective) attrition\n\nRemaining participants systematically differ from those who drop out\nCan lead to biased treatment effect estimates\nOccurs when decision to stay is correlated with treatment or potential outcomes",
    "crumbs": [
      "Unit 5: Threats to Validity",
      "Lecture Slides: Unit 5.0: Threats to Validity"
    ]
  },
  {
    "objectID": "unit-5/lec-5-0-slides.html#formalizing-attrition",
    "href": "unit-5/lec-5-0-slides.html#formalizing-attrition",
    "title": "Experimental Analysis Challenges",
    "section": "Formalizing Attrition",
    "text": "Formalizing Attrition\n\nIntroduce a dummy variable \\(R_i\\):\n\n\\(R_i = 1\\) if subject remains in the study (responder)\n\\(R_i = 0\\) if subject dropped out (non-responder)\n\nObservability with attrition: We only observe \\(Y_i(T_i)\\) if \\(R_i = 1\\)\nStronger independence assumption needed: \\((Y_i(0), Y_i(1), R_i) \\perp T_i\\)",
    "crumbs": [
      "Unit 5: Threats to Validity",
      "Lecture Slides: Unit 5.0: Threats to Validity"
    ]
  },
  {
    "objectID": "unit-5/lec-5-0-slides.html#observability-in-the-potential-outcomes-framework",
    "href": "unit-5/lec-5-0-slides.html#observability-in-the-potential-outcomes-framework",
    "title": "Experimental Analysis Challenges",
    "section": "Observability in the Potential Outcomes Framework",
    "text": "Observability in the Potential Outcomes Framework\n\nWithout attrition, observed outcome equation: \\(Y_i = D_i Y_i(1) + (1-D_i)Y_i(0)\\)\nWith attrition, observed outcome equation becomes: \\(Y_i = \\begin{cases}\nD_i Y_i(1) + (1-D_i)Y_i(0), & \\text{if } R_i(D_i) = 1 \\\\\n\\text{missing}, & \\text{if } R_i(D_i) = 0\n\\end{cases}\\)\nResponse indicator is potentially treatment-dependent: \\(R_i = D_i R_i(1) + (1-D_i)R_i(0)\\)",
    "crumbs": [
      "Unit 5: Threats to Validity",
      "Lecture Slides: Unit 5.0: Threats to Validity"
    ]
  },
  {
    "objectID": "unit-5/lec-5-0-slides.html#internal-validity-for-respondents",
    "href": "unit-5/lec-5-0-slides.html#internal-validity-for-respondents",
    "title": "Experimental Analysis Challenges",
    "section": "Internal Validity for Respondents",
    "text": "Internal Validity for Respondents\n\nGoal: Valid treatment effect estimate for population of responders\nNaive estimator: Difference in means between treated and control among responders\nPotential bias: Responders in treatment group may differ from responders in control group\nAssumption for validity: \\((Y_i(0), Y_i(1)) \\perp T_i | R_i = 1\\)",
    "crumbs": [
      "Unit 5: Threats to Validity",
      "Lecture Slides: Unit 5.0: Threats to Validity"
    ]
  },
  {
    "objectID": "unit-5/lec-5-0-slides.html#internal-validity-for-entire-population",
    "href": "unit-5/lec-5-0-slides.html#internal-validity-for-entire-population",
    "title": "Experimental Analysis Challenges",
    "section": "Internal Validity for Entire Population",
    "text": "Internal Validity for Entire Population\n\nAverage treatment effect can be expressed as:\n\n\\[\n\\text{ATE} = P(R=1) \\cdot E[Y_i(1) - Y_i(0) | R_i = 1] \\\\\n+ P(R=0) \\cdot E[Y_i(1) - Y_i(0) | R_i = 0]\n\\]\n\nStrong assumption needed: \\(E[Y_i(1) - Y_i(0) | R_i = 1] = E[Y_i(1) - Y_i(0) | R_i = 0]\\)",
    "crumbs": [
      "Unit 5: Threats to Validity",
      "Lecture Slides: Unit 5.0: Threats to Validity"
    ]
  },
  {
    "objectID": "unit-5/lec-5-0-slides.html#difference-between-responders-effect-and-overall-effect",
    "href": "unit-5/lec-5-0-slides.html#difference-between-responders-effect-and-overall-effect",
    "title": "Experimental Analysis Challenges",
    "section": "Difference between responders’ effect and overall effect",
    "text": "Difference between responders’ effect and overall effect\n\nATE for entire sample as a weighted average: \\(\\tau = P[R_i = 1]\\tau(R_i = 1) + (1 - P[R_i = 1])\\tau(R_i = 0)\\)\nDifference between responders’ effect and overall effect: \\(\\tau(R_i = 1) - \\tau = P[R_i = 0](\\tau(R_i = 1) - \\tau(R_i = 0))\\)\nBias depends on:\n\nProportion of attritors \\((P[R_i = 0])\\)\nTreatment effect difference between responders and non-responders",
    "crumbs": [
      "Unit 5: Threats to Validity",
      "Lecture Slides: Unit 5.0: Threats to Validity"
    ]
  },
  {
    "objectID": "unit-5/lec-5-0-slides.html#preventing-attrition-problems",
    "href": "unit-5/lec-5-0-slides.html#preventing-attrition-problems",
    "title": "Experimental Analysis Challenges",
    "section": "Preventing Attrition Problems",
    "text": "Preventing Attrition Problems\nBest to try to prevent this problem rather than “deal with it”\n\nTry to track down people who leave\nCan choose a random sample, and devote resources to tracking them down\n\nBut we always need to check after if this is a problem…",
    "crumbs": [
      "Unit 5: Threats to Validity",
      "Lecture Slides: Unit 5.0: Threats to Validity"
    ]
  },
  {
    "objectID": "unit-5/lec-5-0-slides.html#testing-for-attrition-bias",
    "href": "unit-5/lec-5-0-slides.html#testing-for-attrition-bias",
    "title": "Experimental Analysis Challenges",
    "section": "Testing for Attrition Bias",
    "text": "Testing for Attrition Bias\nTesting depends on available data:\n\nWhen baseline outcome measures are available (Ideal case)\n\nCompare baseline outcomes across all combinations: (Treatment/Control × Retained/Attrited)\n\nWhen baseline covariates are available\n\nCompare baseline covariates across these groups\nChallenge: Which covariates to include?\n\nWhen no baseline information is available\n\nCompare attrition rates between treatment and control\nLimited evidence - equal rates don’t guarantee no problem",
    "crumbs": [
      "Unit 5: Threats to Validity",
      "Lecture Slides: Unit 5.0: Threats to Validity"
    ]
  },
  {
    "objectID": "unit-5/lec-5-0-slides.html#tests-using-baseline-outcome-data",
    "href": "unit-5/lec-5-0-slides.html#tests-using-baseline-outcome-data",
    "title": "Experimental Analysis Challenges",
    "section": "Tests Using Baseline Outcome Data",
    "text": "Tests Using Baseline Outcome Data\n\nGHO tests (Ghanem, Hirshleifer, Ortiz-Becerra, 2020):\n\nUse regression with baseline outcome as dependent variable: \\[\nY_{i0} = \\pi_{11}D_iR_i + \\pi_{01}(1-D_i)R_i + \\\\\n\\pi_{10}D_i(1-R_i) + \\pi_{00}(1-D_i)(1-R_i) + \\epsilon_i\n\\]\n\nFor valid estimate for responders, test: \\(E[Y_{i0}|D_i = 0, R_i = r] = E[Y_{i0}|D_i = 1, R_i = r]\\) for \\(r = 0, 1\\)\nFor valid estimate for entire population, test equality across all four means: \\(\\pi_{11} = \\pi_{01} = \\pi_{10} = \\pi_{00}\\)",
    "crumbs": [
      "Unit 5: Threats to Validity",
      "Lecture Slides: Unit 5.0: Threats to Validity"
    ]
  },
  {
    "objectID": "unit-5/lec-5-0-slides.html#analyzing-data-with-attrition",
    "href": "unit-5/lec-5-0-slides.html#analyzing-data-with-attrition",
    "title": "Experimental Analysis Challenges",
    "section": "Analyzing Data with Attrition",
    "text": "Analyzing Data with Attrition\n\nAvailable case analysis\n\nOnly analyze data from participants who remained\nValid only if attrition is random\n\nHorowitz and Manski bounds (Worst-case bounds)\n\nImpute extreme values for missing outcomes\nOften very wide, especially with moderate attrition\n\nInverse probability weighting (IPW)\n\nWeight observations by inverse probability of remaining\nAssumes attrition independent of potential outcomes conditional on covariates\n\nLee bounds\n\nTrim observed data to bound treatment effect\nAssumes monotonic attrition",
    "crumbs": [
      "Unit 5: Threats to Validity",
      "Lecture Slides: Unit 5.0: Threats to Validity"
    ]
  },
  {
    "objectID": "unit-5/lec-5-0-slides.html#design-tips-to-reduce-attrition",
    "href": "unit-5/lec-5-0-slides.html#design-tips-to-reduce-attrition",
    "title": "Experimental Analysis Challenges",
    "section": "Design Tips to Reduce Attrition",
    "text": "Design Tips to Reduce Attrition\n\nCollect Rich Baseline Data\n\nEssential for testing identification assumptions\nEnables adjustment methods if attrition occurs\n\nRemove Barriers to Contacting Participants\n\nCollect multiple contact methods\nEstablish location tracking procedures\nStreamline follow-up protocols\n\nConsider Populations Less Likely to Attrit\n\nBut balance with external validity concerns\nBe mindful of equity implications",
    "crumbs": [
      "Unit 5: Threats to Validity",
      "Lecture Slides: Unit 5.0: Threats to Validity"
    ]
  },
  {
    "objectID": "unit-5/lec-5-0-slides.html#more-practices-to-minimize-attrition",
    "href": "unit-5/lec-5-0-slides.html#more-practices-to-minimize-attrition",
    "title": "Experimental Analysis Challenges",
    "section": "More Practices to Minimize Attrition",
    "text": "More Practices to Minimize Attrition\n\nMaintain Regular Contact\n\nDon’t just collect data at baseline and endline\nSchedule intermediate check-ins\nBuild ongoing relationship with participants\n\nThink Like a Behavioral Economist\n\nProvide appropriate incentives\nChoose convenient times for data collection\nLeverage reciprocity and commitment\n\nMinimize Participant Burden\n\nKeep surveys concise\nMake participation as easy as possible\nReduce complexity of required tasks",
    "crumbs": [
      "Unit 5: Threats to Validity",
      "Lecture Slides: Unit 5.0: Threats to Validity"
    ]
  },
  {
    "objectID": "unit-5/lec-5-0-slides.html#complete-compliance",
    "href": "unit-5/lec-5-0-slides.html#complete-compliance",
    "title": "Experimental Analysis Challenges",
    "section": "Complete Compliance",
    "text": "Complete Compliance\n\nDefinition: Units receive exactly the treatment to which they were assigned\n\n\\(D_i = T_i\\) for all units \\(i\\)\n\nIn mathematical terms:\n\n\\(P(D_i = 1 | T_i = 1) = 1\\)\n\\(P(D_i = 0 | T_i = 0) = 1\\)\n\nWhen violated: Treatment effect estimates are diluted or biased\n\nIntention-to-treat (ITT) estimates may understate effects\nSelection effects may be reintroduced",
    "crumbs": [
      "Unit 5: Threats to Validity",
      "Lecture Slides: Unit 5.0: Threats to Validity"
    ]
  },
  {
    "objectID": "unit-5/lec-5-0-slides.html#imperfect-compliance",
    "href": "unit-5/lec-5-0-slides.html#imperfect-compliance",
    "title": "Experimental Analysis Challenges",
    "section": "Imperfect Compliance",
    "text": "Imperfect Compliance\n\nDefinition: Received treatment differs from assigned treatment\nTypes:\n\nOne-sided: Control group receives treatment OR treatment group doesn’t\nTwo-sided: Both types occur simultaneously\n\nExamples:\n\nDeworming treatment in schools\nInformation provision in voting experiments\nMarket experience interventions",
    "crumbs": [
      "Unit 5: Threats to Validity",
      "Lecture Slides: Unit 5.0: Threats to Validity"
    ]
  },
  {
    "objectID": "unit-5/lec-5-0-slides.html#complete-compliance-assumption",
    "href": "unit-5/lec-5-0-slides.html#complete-compliance-assumption",
    "title": "Experimental Analysis Challenges",
    "section": "Complete Compliance Assumption",
    "text": "Complete Compliance Assumption\n\nComplete compliance: Treatment assignment equals treatment received\nFormally: \\(D_i = Z_i\\) for all units \\(i\\)\nCritical for identifying the average treatment effect (ATE)\nReality: Often violated in field and even lab settings",
    "crumbs": [
      "Unit 5: Threats to Validity",
      "Lecture Slides: Unit 5.0: Threats to Validity"
    ]
  },
  {
    "objectID": "unit-5/lec-5-0-slides.html#four-types-of-participants",
    "href": "unit-5/lec-5-0-slides.html#four-types-of-participants",
    "title": "Experimental Analysis Challenges",
    "section": "Four Types of Participants",
    "text": "Four Types of Participants\nBased on potential treatment status:",
    "crumbs": [
      "Unit 5: Threats to Validity",
      "Lecture Slides: Unit 5.0: Threats to Validity"
    ]
  },
  {
    "objectID": "unit-5/lec-5-0-slides.html#four-types-of-participants-1",
    "href": "unit-5/lec-5-0-slides.html#four-types-of-participants-1",
    "title": "Experimental Analysis Challenges",
    "section": "Four Types of Participants",
    "text": "Four Types of Participants\n\nCompliers: Take treatment if and only if assigned to treatment\nAlways-takers: Take treatment regardless of assignment\nNever-takers: Never take treatment regardless of assignment\nDefiers: Take treatment if and only if assigned to control",
    "crumbs": [
      "Unit 5: Threats to Validity",
      "Lecture Slides: Unit 5.0: Threats to Validity"
    ]
  },
  {
    "objectID": "unit-5/lec-5-0-slides.html#analytical-approaches-under-imperfect-compliance",
    "href": "unit-5/lec-5-0-slides.html#analytical-approaches-under-imperfect-compliance",
    "title": "Experimental Analysis Challenges",
    "section": "Analytical Approaches Under Imperfect Compliance",
    "text": "Analytical Approaches Under Imperfect Compliance\n\nAs-treated analysis (NO!)\n\nCompare based on treatment actually received\nReintroduces selection problem\nValid only under strong assumptions\n\nIntention-to-treat (ITT) analysis\n\nCompare based on initial treatment assignment\nUnbiased estimate of effect of being offered treatment\nDiluted measure if compliance is low",
    "crumbs": [
      "Unit 5: Threats to Validity",
      "Lecture Slides: Unit 5.0: Threats to Validity"
    ]
  },
  {
    "objectID": "unit-5/lec-5-0-slides.html#itt-decomposition",
    "href": "unit-5/lec-5-0-slides.html#itt-decomposition",
    "title": "Experimental Analysis Challenges",
    "section": "ITT Decomposition",
    "text": "ITT Decomposition\nITT can be decomposed into:\n\\[\nITT = \\Pr(\\text{complier}) \\cdot \\tau_{compliers} + \\Pr(\\text{always-taker}) \\cdot 0 + \\Pr(\\text{never-taker}) \\cdot 0 + \\Pr(\\text{defier}) \\cdot \\tau_{defiers}\n\\]\n\nAlways-takers and never-takers have zero contribution\nUnder monotonicity (no defiers), simplifies further",
    "crumbs": [
      "Unit 5: Threats to Validity",
      "Lecture Slides: Unit 5.0: Threats to Validity"
    ]
  },
  {
    "objectID": "unit-5/lec-5-0-slides.html#randomization-as-an-instrumental-variable",
    "href": "unit-5/lec-5-0-slides.html#randomization-as-an-instrumental-variable",
    "title": "Experimental Analysis Challenges",
    "section": "Randomization as an Instrumental Variable",
    "text": "Randomization as an Instrumental Variable\n\nUses randomized assignment as instrument for endogenous received treatment\nKey assumptions:\n\nExclusion restriction: Assignment affects outcome only through received treatment\nMonotonicity: Assignment never decreases probability of receiving treatment\n\nEstimand: Local Average Treatment Effect (LATE)\n\nAverage effect for “compliers” only",
    "crumbs": [
      "Unit 5: Threats to Validity",
      "Lecture Slides: Unit 5.0: Threats to Validity"
    ]
  },
  {
    "objectID": "unit-5/lec-5-0-slides.html#calculating-the-late",
    "href": "unit-5/lec-5-0-slides.html#calculating-the-late",
    "title": "Experimental Analysis Challenges",
    "section": "Calculating the LATE",
    "text": "Calculating the LATE\n\\[\\text{LATE} = \\frac{\\text{ITT effect on outcome}}{\\text{ITT effect on treatment receipt}}\\]\n\nDenominator = proportion of compliers (under monotonicity)\nCan characterize compliers by comparing conditional ITT effects",
    "crumbs": [
      "Unit 5: Threats to Validity",
      "Lecture Slides: Unit 5.0: Threats to Validity"
    ]
  },
  {
    "objectID": "unit-5/lec-5-0-slides.html#first-stage-and-complier-share",
    "href": "unit-5/lec-5-0-slides.html#first-stage-and-complier-share",
    "title": "Experimental Analysis Challenges",
    "section": "First-Stage and Complier Share",
    "text": "First-Stage and Complier Share\n\nFirst stage: Effect of assignment on treatment receipt\nMeasures proportion of compliers under monotonicity\nCan be estimated via regression: \\(D_i = \\alpha + \\beta Z_i + \\varepsilon_i\\)\n\\(\\beta\\) represents complier share",
    "crumbs": [
      "Unit 5: Threats to Validity",
      "Lecture Slides: Unit 5.0: Threats to Validity"
    ]
  },
  {
    "objectID": "unit-5/lec-5-0-slides.html#one-sided-non-compliance",
    "href": "unit-5/lec-5-0-slides.html#one-sided-non-compliance",
    "title": "Experimental Analysis Challenges",
    "section": "One-Sided Non-Compliance",
    "text": "One-Sided Non-Compliance\n\nSpecial case where:\n\nEither control units can’t access treatment OR\nTreatment units all receive treatment\n\nLATE equals Treatment on the Treated (TOT)\nSimplifies denominator in IV formula\nCommon in settings with controlled access",
    "crumbs": [
      "Unit 5: Threats to Validity",
      "Lecture Slides: Unit 5.0: Threats to Validity"
    ]
  },
  {
    "objectID": "unit-5/lec-5-0-slides.html#who-are-the-compliers",
    "href": "unit-5/lec-5-0-slides.html#who-are-the-compliers",
    "title": "Experimental Analysis Challenges",
    "section": "Who Are the Compliers?",
    "text": "Who Are the Compliers?\n\nCannot directly identify individual compliers\nCan characterize compliers as a group:\n\nCompare distribution of characteristics\nFor binary characteristic \\(X\\): \\[\\frac{\\Pr(X=1|\\text{complier})}{\\Pr(X=1)} = \\frac{E[D_i|Z_i=1,X=1] - E[D_i|Z_i=0,X=1]}{E[D_i|Z_i=1] - E[D_i|Z_i=0]}\\]\n\nHelps assess generalizability of LATE",
    "crumbs": [
      "Unit 5: Threats to Validity",
      "Lecture Slides: Unit 5.0: Threats to Validity"
    ]
  },
  {
    "objectID": "unit-5/lec-5-0-slides.html#bounding-the-ate-with-non-compliance",
    "href": "unit-5/lec-5-0-slides.html#bounding-the-ate-with-non-compliance",
    "title": "Experimental Analysis Challenges",
    "section": "Bounding the ATE with Non-Compliance",
    "text": "Bounding the ATE with Non-Compliance\n\nEven without monotonicity, can place bounds on ATE if outcome is bounded\nSimilar to Horowitz-Manski bounds for attrition\nUses worst-case scenarios for potential outcomes of non-compliers\nBounds can be wide but provide range for potential ATE",
    "crumbs": [
      "Unit 5: Threats to Validity",
      "Lecture Slides: Unit 5.0: Threats to Validity"
    ]
  },
  {
    "objectID": "unit-5/lec-5-0-slides.html#design-tips-to-improve-compliance",
    "href": "unit-5/lec-5-0-slides.html#design-tips-to-improve-compliance",
    "title": "Experimental Analysis Challenges",
    "section": "Design Tips to Improve Compliance",
    "text": "Design Tips to Improve Compliance\n\nLeverage technology to monitor and maintain compliance\n\nTracking devices, electronic monitoring, digital verification\n\nMinimize complexity and participant burden\n\nClear instructions, simple protocols, reduced effort costs\n\nStrategic incentives placement\n\nFront-load or back-load depending on compliance concerns\n\nApply behavioral principles\n\nCommitment devices, social norms, default options, reminders",
    "crumbs": [
      "Unit 5: Threats to Validity",
      "Lecture Slides: Unit 5.0: Threats to Validity"
    ]
  },
  {
    "objectID": "unit-5/lec-5-0-slides.html#design-tips-to-improve-compliance-1",
    "href": "unit-5/lec-5-0-slides.html#design-tips-to-improve-compliance-1",
    "title": "Experimental Analysis Challenges",
    "section": "Design Tips to Improve Compliance",
    "text": "Design Tips to Improve Compliance\n\nEducation and supportive environment\n\nTraining, explanation of importance, regular reinforcement\n\nAttention to treatment administration\n\nStandardized protocols, administrator training, avoid contamination\nConsider separate administrators for treatment/control",
    "crumbs": [
      "Unit 5: Threats to Validity",
      "Lecture Slides: Unit 5.0: Threats to Validity"
    ]
  },
  {
    "objectID": "unit-5/lec-5-0-slides.html#take-home",
    "href": "unit-5/lec-5-0-slides.html#take-home",
    "title": "Experimental Analysis Challenges",
    "section": "Take-home",
    "text": "Take-home\n\nPerfect experiments exist only in theory\nAnalysis strategy should account for real-world challenges\nDesign choices can minimize (but rarely eliminate) issues\nMultiple approaches often needed for robust conclusions",
    "crumbs": [
      "Unit 5: Threats to Validity",
      "Lecture Slides: Unit 5.0: Threats to Validity"
    ]
  },
  {
    "objectID": "unit-5/lec-5-1-slides.html#analysis-basics",
    "href": "unit-5/lec-5-1-slides.html#analysis-basics",
    "title": "Lecture 3: Hypothesis Testing & Sample Size",
    "section": "Analysis Basics",
    "text": "Analysis Basics\n\nBecause we designed our experiment properly, analysis is EASY: Just compare the means in the treatment and control groups\nResults for intervention giving eyeglasses to primary school students:\n\n\n\n\n\n\n\n\n1. Average Test Score in Treatment Group\n2. Average Test Score in Control Group\nDifference (1-2)\n\n\n75\n68\n+7"
  },
  {
    "objectID": "unit-5/lec-5-1-slides.html#but-is-this-difference-statistically-significant",
    "href": "unit-5/lec-5-1-slides.html#but-is-this-difference-statistically-significant",
    "title": "Lecture 3: Hypothesis Testing & Sample Size",
    "section": "But is this difference statistically significant?",
    "text": "But is this difference statistically significant?"
  },
  {
    "objectID": "unit-5/lec-5-1-slides.html#we-need-to-do-a-statistical-test",
    "href": "unit-5/lec-5-1-slides.html#we-need-to-do-a-statistical-test",
    "title": "Lecture 3: Hypothesis Testing & Sample Size",
    "section": "We need to do a statistical test",
    "text": "We need to do a statistical test\n\nRemember from our previous discussions…\nWhat are we testing?\nWhat specifically is our Null Hypothesis for the eyeglasses program?"
  },
  {
    "objectID": "unit-5/lec-5-1-slides.html#hypothesis-testing",
    "href": "unit-5/lec-5-1-slides.html#hypothesis-testing",
    "title": "Lecture 3: Hypothesis Testing & Sample Size",
    "section": "Hypothesis Testing",
    "text": "Hypothesis Testing\n\nIn Impact Evaluation, we are interested in testing if the program had an effect\nIn other words, we want to test a Null Hypothesis that our program had NO effect\nIf it is not likely that our result was just due to chance/luck (e.g., because of our sample), we “reject the Null Hypothesis”\n\nUsually this means &lt;5% probability the result is because of luck\n\n\nIf we reject the Null Hypothesis, this means our program had a “statistically significant” impact"
  },
  {
    "objectID": "unit-5/lec-5-1-slides.html#we-could-do-a-t-test",
    "href": "unit-5/lec-5-1-slides.html#we-could-do-a-t-test",
    "title": "Lecture 3: Hypothesis Testing & Sample Size",
    "section": "We could do a t-test",
    "text": "We could do a t-test\nFormula for t-statistic:\n\\[\n\\begin{aligned}\nt &= \\frac{\\bar{x}_t - \\bar{x}_c}{\\sqrt{\\frac{var_t}{n_t} + \\frac{var_c}{n_c}}} \\\\\n&= \\frac{\\overbrace{\\text{Difference between Group Means}}^{\\text{Signal}}}{\\underbrace{SE(\\bar{x}_t - \\bar{x}_c)}_{\\text{Noise}}}\n\\end{aligned}\n\\]\nAnd see if this is large enough to “reject the Null”"
  },
  {
    "objectID": "unit-5/lec-5-1-slides.html#regression-form-of-the-test",
    "href": "unit-5/lec-5-1-slides.html#regression-form-of-the-test",
    "title": "Lecture 3: Hypothesis Testing & Sample Size",
    "section": "Regression Form of the Test",
    "text": "Regression Form of the Test\nActually, we can do the SAME test in regression form:\nRun the regression:\n\\[\n\\underbrace{Y_i}_{\\text{Outcome}} = \\underbrace{\\alpha}_{\\text{Constant}} + \\underbrace{\\beta}_{\\text{Effect}} \\underbrace{W_i}_{\\text{Treatment}} + \\underbrace{\\varepsilon_i}_{\\text{Error}}\n\\]\n\nHere, \\(\\beta\\) will give us the difference between treatment and control and tell us whether it is “statistically significant” or not.\nOur estimate will be unbiased because of random assignment."
  },
  {
    "objectID": "unit-5/lec-5-1-slides.html#maximizing-precision",
    "href": "unit-5/lec-5-1-slides.html#maximizing-precision",
    "title": "Lecture 3: Hypothesis Testing & Sample Size",
    "section": "Maximizing Precision",
    "text": "Maximizing Precision\nBut remember, we also want to be as precise as possible?\n\nPrecision and Accuracy in EstimationCan we increase precision even after we’ve run the experiment?"
  },
  {
    "objectID": "unit-5/lec-5-1-slides.html#using-covariates-to-improve-precision",
    "href": "unit-5/lec-5-1-slides.html#using-covariates-to-improve-precision",
    "title": "Lecture 3: Hypothesis Testing & Sample Size",
    "section": "Using Covariates to Improve Precision",
    "text": "Using Covariates to Improve Precision\nYes! By adding “covariates”…\nWe can control for other baseline variables in the regression to “suck up” extra variation:\n\\[\nY_i = \\alpha + \\beta W_i + \\gamma \\textcolor{red}{X_i} + \\varepsilon_i\n\\]\nWhat covariates should we control for?\n\nIf stratified randomization, add fixed effects for strata (e.g., if we stratify on county, add “dummy” variables for each county)\nThe baseline of the outcome variable\nOther covariates strongly related to the outcome\nIf cluster randomized: cluster level covariates can help a LOT…why?\n\nAlso report “raw” differences without covariates for transparency"
  },
  {
    "objectID": "unit-5/lec-5-1-slides.html#clustering",
    "href": "unit-5/lec-5-1-slides.html#clustering",
    "title": "Lecture 3: Hypothesis Testing & Sample Size",
    "section": "Clustering",
    "text": "Clustering\nWe need to be careful with “clustered” data\nRemember: Students in a school are similar…this makes our regular standard errors too small\nThis is easy to deal with: we use “cluster-robust standard errors” in the regression\nIn R, this is just a command:"
  },
  {
    "objectID": "unit-5/lec-5-1-slides.html#assessing-dealing-with-threats",
    "href": "unit-5/lec-5-1-slides.html#assessing-dealing-with-threats",
    "title": "Lecture 3: Hypothesis Testing & Sample Size",
    "section": "Assessing & Dealing with Threats",
    "text": "Assessing & Dealing with Threats\nIn the real world, things often don’t go perfectly…\nWe spoke before about 2 kinds of “threats”: - Attrition - Imperfect compliance\nWhen we analyze our data, we need to: 1. Assess whether these are problems, and see how big the problems are 2. Sometimes we can do things to address them"
  },
  {
    "objectID": "unit-5/lec-5-1-slides.html#attrition",
    "href": "unit-5/lec-5-1-slides.html#attrition",
    "title": "Lecture 3: Hypothesis Testing & Sample Size",
    "section": "Attrition",
    "text": "Attrition\nUsually, there will be some people that leave/cannot be found and are not in the endline survey\nThis is a problem if the proportion of “attriters” are different in the treatment and control group\n\nWhy is this a problem?\nWhy might this happen?\nIs it a problem if the rate of attrition is the same across groups, but is high? Say 30-40%?"
  },
  {
    "objectID": "unit-5/lec-5-1-slides.html#attrition-bias-example",
    "href": "unit-5/lec-5-1-slides.html#attrition-bias-example",
    "title": "Lecture 3: Hypothesis Testing & Sample Size",
    "section": "Attrition Bias Example",
    "text": "Attrition Bias Example\nSuppose we were interested in testing the impact of giving students eyeglasses on test scores\nConsider these scenarios: - If everyone is in our data in the treatment and control group, what is the result? - How does this change if some people are missing from baseline only? From endline only? - What if under-performing children (e.g., scores &lt;50) don’t come to school when we give exams?"
  },
  {
    "objectID": "unit-5/lec-5-1-slides.html#preventing-attrition-problems",
    "href": "unit-5/lec-5-1-slides.html#preventing-attrition-problems",
    "title": "Lecture 3: Hypothesis Testing & Sample Size",
    "section": "Preventing Attrition Problems",
    "text": "Preventing Attrition Problems\nBest to try to prevent this problem rather than “deal with it”\n\nTry to track down people who leave\nCan choose a random sample, and devote resources to tracking them down\n\nBut we always need to check after if this is a problem…"
  },
  {
    "objectID": "unit-5/lec-5-1-slides.html#assessing-attrition",
    "href": "unit-5/lec-5-1-slides.html#assessing-attrition",
    "title": "Lecture 3: Hypothesis Testing & Sample Size",
    "section": "Assessing Attrition",
    "text": "Assessing Attrition\nTo see how much of a problem this is in our experiment, we need to do two things:\n\nIs there “differential” attrition? Run the regression:\n\n\\[\nAttrit_i = \\alpha + \\beta W_i + \\varepsilon_i\n\\]\n\nHow are those who “attrit” different from those who don’t?\n\n\\[\nAttrit_i = \\alpha + \\gamma X_i + \\varepsilon_i\n\\]"
  },
  {
    "objectID": "unit-5/lec-5-1-slides.html#what-if-i-have-attrition",
    "href": "unit-5/lec-5-1-slides.html#what-if-i-have-attrition",
    "title": "Lecture 3: Hypothesis Testing & Sample Size",
    "section": "What if I have attrition?",
    "text": "What if I have attrition?\nThis is a difficult problem, but there are some things…\n“Bounding Approaches” can give you a range of effects under best case and worst case - Assume everyone that left got the lowest score…assume everyone that left got the best score…\n- Example: “Lee Bounds”\n- Often not very efficient"
  },
  {
    "objectID": "unit-5/lec-5-1-slides.html#imperfect-compliance",
    "href": "unit-5/lec-5-1-slides.html#imperfect-compliance",
    "title": "Lecture 3: Hypothesis Testing & Sample Size",
    "section": "Imperfect Compliance",
    "text": "Imperfect Compliance\nAnother potential problem is “Imperfect Compliance”…\nImperfect compliance: some in the control group get treatment and some in the treatment group do not… - This introduces selection bias\nFour types of people:\n\n\n\nWhat actually happened:\n\n\n\n\n\nTreated\nNot Treated\n\n\nTreatment Group\nOK\nNon-compliers\n\n\nControl Group\nNon-compliers\nOK\n\n\n\nWhat do we do with these non-compliers? Can we just drop/ignore them? NO!!! Why?"
  },
  {
    "objectID": "unit-5/lec-5-1-slides.html#selection-bias-example-eyeglasses",
    "href": "unit-5/lec-5-1-slides.html#selection-bias-example-eyeglasses",
    "title": "Lecture 3: Hypothesis Testing & Sample Size",
    "section": "Selection Bias Example: Eyeglasses",
    "text": "Selection Bias Example: Eyeglasses\nSuppose girls are more likely to wear their eyeglasses…\n\n\nTreatment Group\n\n\n\n\n\n\n\n\n\nStudent\nTreatment Group? (Given glasses)\nWore glasses?\nGender\nScore\n\n\n\nStudent 1\nYes\nYes\nGirl\n80\n\n\nStudent 2\nYes\nNo\nBoy\n70\n\n\nStudent 3\nYes\nYes\nGirl\n80\n\n\nStudent 4\nYes\nYes\nBoy\n70\n\n\nStudent 5\nYes\nYes\nGirl\n80\n\n\nStudent 6\nYes\nNo\nBoy\n70\n\n\n———\n———————————-\n—————\n——–\n—————\n\n\nFull Sample\n\n\n3 girls, 3 boys\n75.0\n\n\n———\n———————————-\n—————\n——–\n—————\n\n\nCompliers\n\n\n3 girls, 1 boys\n72.5\n\n\n\n\nControl Group\n\n\n\n\n\n\n\n\n\nStudent\nTreatment Group? (Given glasses)\nWore glasses?\nGender\nScore\n\n\n\nStudent 1\nNo\nNo\nBoy\n70\n\n\nStudent 2\nNo\nNo\nBoy\n70\n\n\nStudent 3\nNo\nYes\nGirl\n80\n\n\nStudent 4\nNo\nNo\nBoy\n70\n\n\nStudent 5\nNo\nYes\nGirl\n80\n\n\nStudent 6\nNo\nNo\nGirl\n80\n\n\n———\n———————————-\n—————\n——–\n—————\n\n\nFull Sample\n\n\n3 girls, 3 boys\n75.0\n\n\n———\n———————————-\n—————\n——–\n—————\n\n\nDefiers\n\n\n3 girls, 1 boys\n72.5\n\n\n\n\nIf we just compare based on who actually wore glasses (“per-protocol analysis”), the groups would be imbalanced!"
  },
  {
    "objectID": "unit-5/lec-5-1-slides.html#intention-to-treat-itt",
    "href": "unit-5/lec-5-1-slides.html#intention-to-treat-itt",
    "title": "Lecture 3: Hypothesis Testing & Sample Size",
    "section": "Intention to Treat (ITT)",
    "text": "Intention to Treat (ITT)\nWhat do we do?\n\nCompare individuals based on their ORIGINAL ASSIGNMENT to treatment or control group\nThis is called the “Intention to Treat” estimate\nIGNORE who actually got treated\n\n\\[ITT = E[Y_i|W_i = 1] - E[Y_i|W_i = 0]\\]"
  },
  {
    "objectID": "unit-5/lec-5-1-slides.html#treatment-on-the-treated-tot",
    "href": "unit-5/lec-5-1-slides.html#treatment-on-the-treated-tot",
    "title": "Lecture 3: Hypothesis Testing & Sample Size",
    "section": "Treatment on the Treated (TOT)",
    "text": "Treatment on the Treated (TOT)\nWe can also estimate the effect of ACTUAL treatment - What would this be in eyeglasses example? - This is called the “Treatment on the Treated” (TOT) - The ITT effect may be “too small” because of non-compliance - But the proportion treated is also smaller - What can we do?"
  },
  {
    "objectID": "unit-5/lec-5-1-slides.html#re-scaling-the-itt-estimate",
    "href": "unit-5/lec-5-1-slides.html#re-scaling-the-itt-estimate",
    "title": "Lecture 3: Hypothesis Testing & Sample Size",
    "section": "Re-scaling the ITT Estimate",
    "text": "Re-scaling the ITT Estimate\nWe can “re-scale” the ITT estimate\nUse the probability of treatment in each group to “re-scale” the ITT:\n\\[TOT = \\frac{E[Y_i|W_i = 1] - E[Y_i|W_i = 0]}{E[T_i|W_i = 1] - E[T_i|W_i = 0]}\\]\nWhere: - Numerator is the ITT - Denominator is the difference in probability of actually being treated between treatment and control groups"
  },
  {
    "objectID": "unit-5/lec-5-1-slides.html#eyeglasses-example",
    "href": "unit-5/lec-5-1-slides.html#eyeglasses-example",
    "title": "Lecture 3: Hypothesis Testing & Sample Size",
    "section": "Eyeglasses Example",
    "text": "Eyeglasses Example\n\n\n\n\n\n\n\n\nTreatment Group (gave eyeglasses)\nControl Group (Did not give eyeglasses)\n\n\n\n% Actually Got Treatment (Actually wore glasses)\n70%\n10%\n\n\nE[Outcome] (Test Scores)\n95\n82\n\n\n\n\nIntention to Treat: 95 - 82 = 13\nTreatment On the Treated (IV):\n\n\\[\nTOT = \\frac{95-82}{70\\%-10\\%} = \\frac{13}{0.6} = 21.67\n\\]"
  },
  {
    "objectID": "unit-5/lec-5-1-slides.html#assumptions",
    "href": "unit-5/lec-5-1-slides.html#assumptions",
    "title": "Lecture 3: Hypothesis Testing & Sample Size",
    "section": "Assumptions?",
    "text": "Assumptions?\nWhat are the Assumptions? What do we need for this to work?\n\nWe need the same assumptions as we do for IV:\n\nRelevance: \\(E[T_i|W_i = 1] - E[T_i|W_i = 0] \\neq 0\\)\n\nExclusion Restriction: \\(E[\\varepsilon_i|W_i = 1] = E[\\varepsilon_i|W_i = 0] = 0\\)"
  },
  {
    "objectID": "unit-5/lec-5-1-slides.html#instrumental-variables-for-tot",
    "href": "unit-5/lec-5-1-slides.html#instrumental-variables-for-tot",
    "title": "Lecture 3: Hypothesis Testing & Sample Size",
    "section": "Instrumental Variables for TOT",
    "text": "Instrumental Variables for TOT\nUsing Instrumental Variables to estimate TOT\nWe can calculate by hand like above, but: - Not efficient (remember we like to add covariates) - Harder to figure out standard errors for TOT\nEasier way: Use Instrumental Variables (IV) 1. “First Stage” Regression: \\(\\hat{T}_i = \\hat{\\alpha}_0 + \\hat{\\alpha}_1 W_i + \\hat{\\alpha}_i X_i\\) 2. Predict Probability of Treatment using first stage 3. “Second Stage” regression using predicted treatment status: \\(Y_i = \\beta_0 + \\beta_1 \\hat{T}_i + \\beta_i X_i + \\varepsilon_i\\)\n\\(\\beta_1\\) gives the TOT estimate"
  },
  {
    "objectID": "unit-5/lec-5-1-slides.html#r-implementation",
    "href": "unit-5/lec-5-1-slides.html#r-implementation",
    "title": "Lecture 3: Hypothesis Testing & Sample Size",
    "section": "R Implementation",
    "text": "R Implementation\nIn R, this is simple:"
  },
  {
    "objectID": "unit-5/lec-5-1-slides.html#secondaryintermediate-outcomes",
    "href": "unit-5/lec-5-1-slides.html#secondaryintermediate-outcomes",
    "title": "Lecture 3: Hypothesis Testing & Sample Size",
    "section": "Secondary/Intermediate Outcomes",
    "text": "Secondary/Intermediate Outcomes\nUntil now, we have talked about just one outcome: Y, the primary outcome\nBut, in addition to the primary outcome(s), you should also consider HOW/WHY a program worked to improve the primary outcome or not\nTo figure out what other outcomes to consider, you need to lay out your “Theory of Change”"
  },
  {
    "objectID": "unit-5/lec-5-1-slides.html#what-is-theory-of-change",
    "href": "unit-5/lec-5-1-slides.html#what-is-theory-of-change",
    "title": "Lecture 3: Hypothesis Testing & Sample Size",
    "section": "What is Theory of Change?",
    "text": "What is Theory of Change?\nA Theory of Change (ToC) documents the causal links between inputs, activities, outputs, intermediate and final outcomes, and identifies the underlying assumptions.\nAssumptions are what need to be true for the causal chain to operate."
  },
  {
    "objectID": "unit-5/lec-5-1-slides.html#theory-of-change-framework",
    "href": "unit-5/lec-5-1-slides.html#theory-of-change-framework",
    "title": "Lecture 3: Hypothesis Testing & Sample Size",
    "section": "Theory of Change Framework",
    "text": "Theory of Change Framework\n\nTheory of Change Framework"
  },
  {
    "objectID": "unit-5/lec-5-1-slides.html#theory-of-change-example",
    "href": "unit-5/lec-5-1-slides.html#theory-of-change-example",
    "title": "Lecture 3: Hypothesis Testing & Sample Size",
    "section": "Theory of Change Example",
    "text": "Theory of Change Example\n\nTheory of Change Example"
  },
  {
    "objectID": "unit-5/lec-5-1-slides.html#building-a-theory-of-change",
    "href": "unit-5/lec-5-1-slides.html#building-a-theory-of-change",
    "title": "Lecture 3: Hypothesis Testing & Sample Size",
    "section": "Building a Theory of Change",
    "text": "Building a Theory of Change\nSteps for creating a ToC: 1. Define intervention, objectives, outcomes (even potential unintended ones!) 2. Lay out main steps in causal chain 3. Identify underlying assumptions 4. Add temporal dimension 5. Identify key evaluation questions 6. Validate and revise"
  },
  {
    "objectID": "unit-5/lec-5-1-slides.html#example-iron-supplements",
    "href": "unit-5/lec-5-1-slides.html#example-iron-supplements",
    "title": "Lecture 3: Hypothesis Testing & Sample Size",
    "section": "Example: Iron Supplements",
    "text": "Example: Iron Supplements\n\n\nVitamins provided to schools to give to children\n\n→ ???? → ???? →\n\nLearning outcomes improve\n\nLet’s develop this Theory of Change together…"
  },
  {
    "objectID": "unit-5/lec-5-1-slides.html#multiple-outcomes",
    "href": "unit-5/lec-5-1-slides.html#multiple-outcomes",
    "title": "Lecture 3: Hypothesis Testing & Sample Size",
    "section": "Multiple Outcomes",
    "text": "Multiple Outcomes\nYou’ll see that you often come up with MANY outcomes\n\nYou can test many outcomes just as you did the primary outcomes\nBut, this can complicate our statistical analysis…"
  },
  {
    "objectID": "unit-5/lec-5-1-slides.html#multiple-outcomes-problem",
    "href": "unit-5/lec-5-1-slides.html#multiple-outcomes-problem",
    "title": "Lecture 3: Hypothesis Testing & Sample Size",
    "section": "Multiple Outcomes Problem",
    "text": "Multiple Outcomes Problem\nThe problem:\nThe more outcomes to test, the higher the chance that at least one is significantly affected by the program just by chance\nWhy?\nWhat can we do?"
  },
  {
    "objectID": "unit-5/lec-5-1-slides.html#dealing-with-multiple-outcomes",
    "href": "unit-5/lec-5-1-slides.html#dealing-with-multiple-outcomes",
    "title": "Lecture 3: Hypothesis Testing & Sample Size",
    "section": "Dealing with Multiple Outcomes",
    "text": "Dealing with Multiple Outcomes\nThree ways to deal with multiple outcomes:\n\nPre-specify the outcomes you will examine, and report all results (even not significant)\n\nCombine the variables into an index\n\nPrincipal components, GLS Weighting\nAdvantage: Powerful\nDisadvantage: You don’t really know what the index means\n\n\n\nAdjust your p-values for the number of tests\n\nSeveral approaches…Romano and Wolf (2005) common in economics"
  },
  {
    "objectID": "unit-5/lec-5-1-slides.html#analysis-review",
    "href": "unit-5/lec-5-1-slides.html#analysis-review",
    "title": "Lecture 3: Hypothesis Testing & Sample Size",
    "section": "Analysis Review",
    "text": "Analysis Review\n\nAnalysis Basics\nClustering\nAssessing & Dealing with Threats\n\nAttrition\nImperfect Compliance\n\n\nChoosing secondary outcomes\n\nTheory of Change\n\n\nMultiple outcomes and hypothesis testing"
  },
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Useful Reference Materials",
    "section": "",
    "text": "List, John. Experimental Economics: Theory and Practice, 2025. (Link available soon)\nChernozhukov, Victor, Christian Hansen, Nathan Kallus, Martin Spindler, and Vasilis Syrgkanis. Applied Causal Inference Powered by ML and AI, 2024.\nWager, Stefan. Causal Inference: A Statistical Learning Approach, 2024.\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. An Introduction to Statistical Learning: With Applications in R. Springer Texts in Statistics. New York, NY: Springer US, 2021.",
    "crumbs": [
      "Resources",
      "Useful Reference Materials"
    ]
  },
  {
    "objectID": "resources.html#key-texts",
    "href": "resources.html#key-texts",
    "title": "Useful Reference Materials",
    "section": "",
    "text": "List, John. Experimental Economics: Theory and Practice, 2025. (Link available soon)\nChernozhukov, Victor, Christian Hansen, Nathan Kallus, Martin Spindler, and Vasilis Syrgkanis. Applied Causal Inference Powered by ML and AI, 2024.\nWager, Stefan. Causal Inference: A Statistical Learning Approach, 2024.\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. An Introduction to Statistical Learning: With Applications in R. Springer Texts in Statistics. New York, NY: Springer US, 2021.",
    "crumbs": [
      "Resources",
      "Useful Reference Materials"
    ]
  },
  {
    "objectID": "resources.html#r-tutorials",
    "href": "resources.html#r-tutorials",
    "title": "Useful Reference Materials",
    "section": "R Tutorials",
    "text": "R Tutorials\nParadis, Emmanuel. R for Beginners, 2005.\nWickham, Hadley, and Mine Çetinkaya-Rundel. R for Data Science, 2nd ed. 2023.\nR “Cheat Sheet” Compilation.\nThe R Graph Gallery",
    "crumbs": [
      "Resources",
      "Useful Reference Materials"
    ]
  },
  {
    "objectID": "resources.html#websites",
    "href": "resources.html#websites",
    "title": "Useful Reference Materials",
    "section": "Websites",
    "text": "Websites\nField Experiments Website\nPapers in economic field experiments.\nAbdul Latif Jameel Poverty Action Lab (JPAL)\nA global research center based at MIT’s Economics Department that works to reduce poverty through rigorous scientific evidence and policy implementation\n\nJPAL maintains an excellent site with resources for randomized evaluations and more\n\nStanford Social Impact Lab\nMelissa Dell’s EconDL Website\n\nEconDL is a comprehensive resource detailing applications of Deep Learning in Economics. This is a companion website to the paper Deep Learning for Economists and aims to be a go-to resource for economists and other social scientists for applying tools provided by deep learning in their research.",
    "crumbs": [
      "Resources",
      "Useful Reference Materials"
    ]
  },
  {
    "objectID": "resources.html#data",
    "href": "resources.html#data",
    "title": "Useful Reference Materials",
    "section": "Data",
    "text": "Data\n\nStanford GSB Social Impact Lab Experiment Data\nMIMIC-IV (Medical Information Mart for Intensive Care IV)\nMIMIC-IV is an updated version of the MIMIC database, containing comprehensive de-identified electronic health records from patients admitted to Beth Israel Deaconess Medical Center in Boston, MA from 2008 to 2019. This freely accessible dataset includes data from over 257,000 distinct patients, yielding approximately 524,000 admission records.\nKey Features and Structure\nMIMIC-IV adopts a modular approach to data organization, highlighting data provenance and facilitating both individual and combined use of different data sources. The database is organized into three main modules:\n\nhosp: Contains hospital-wide data including admission/discharge/transfer records, laboratory values, microbiology cultures, medication orders, and administrative data from billing practices\nicu: Comprises data documented at the ICU bedside, including intravenous infusions, patient outputs, charted observations, and documentation of ongoing procedures\nnote: Contains discharge summaries, radiology reports, and other clinical notes in free-text format\nDetailed Description\nMIMIC Code Repository\n\nPapers With Code\nPapers with Code is a free and open resource platform dedicated to making machine learning research more accessible by connecting research papers with their code implementations.",
    "crumbs": [
      "Resources",
      "Useful Reference Materials"
    ]
  },
  {
    "objectID": "unit-4/unit-4.html",
    "href": "unit-4/unit-4.html",
    "title": "Unit 4: Mechanisms",
    "section": "",
    "text": "In this unit, we’ll be digging deeper into analysis of randomized trials, focusing on the analysis of heterogeneous treatment effects and the role of moderators. We’ll cover ways of thinking through the mechanisms of randomized trials as a way to guide data collection and analysis, and more technical details of how to estimate average treatment effects, heterogeneous treatment effects, and the role of moderators, using traditional methods and methods leveraging machine learning .",
    "crumbs": [
      "Unit 4: Mechanisms",
      "Unit 4: Mechanisms"
    ]
  },
  {
    "objectID": "unit-4/unit-4.html#unit-overview",
    "href": "unit-4/unit-4.html#unit-overview",
    "title": "Unit 4: Mechanisms",
    "section": "",
    "text": "In this unit, we’ll be digging deeper into analysis of randomized trials, focusing on the analysis of heterogeneous treatment effects and the role of moderators. We’ll cover ways of thinking through the mechanisms of randomized trials as a way to guide data collection and analysis, and more technical details of how to estimate average treatment effects, heterogeneous treatment effects, and the role of moderators, using traditional methods and methods leveraging machine learning .",
    "crumbs": [
      "Unit 4: Mechanisms",
      "Unit 4: Mechanisms"
    ]
  },
  {
    "objectID": "unit-4/unit-4.html#unit-4.0-pre-analysis-plans",
    "href": "unit-4/unit-4.html#unit-4.0-pre-analysis-plans",
    "title": "Unit 4: Mechanisms",
    "section": "Unit 4.0: Pre-analysis Plans",
    "text": "Unit 4.0: Pre-analysis Plans\n\nProject\nPre-Analysis Plan Template\nPaying for Anemia Reduction Analysis Plan\nTheory of Change Template\nHow to Build an UN-SILOED Theory of Change",
    "crumbs": [
      "Unit 4: Mechanisms",
      "Unit 4: Mechanisms"
    ]
  },
  {
    "objectID": "unit-4/unit-4.html#unit-4.1-average-treatment-effects",
    "href": "unit-4/unit-4.html#unit-4.1-average-treatment-effects",
    "title": "Unit 4: Mechanisms",
    "section": "Unit 4.1: Average Treatment Effects",
    "text": "Unit 4.1: Average Treatment Effects\n\nLecture 4.1 Slides\nUsing PDS Lasso to Select Controls in Field Experiments",
    "crumbs": [
      "Unit 4: Mechanisms",
      "Unit 4: Mechanisms"
    ]
  },
  {
    "objectID": "unit-4/unit-4.html#unit-4.2-heterogeneous-treatment-effects-moderation",
    "href": "unit-4/unit-4.html#unit-4.2-heterogeneous-treatment-effects-moderation",
    "title": "Unit 4: Mechanisms",
    "section": "Unit 4.2: Heterogeneous Treatment Effects & Moderation",
    "text": "Unit 4.2: Heterogeneous Treatment Effects & Moderation\n\nLecture 4.2 Slides\n[HTE Exercise]",
    "crumbs": [
      "Unit 4: Mechanisms",
      "Unit 4: Mechanisms"
    ]
  },
  {
    "objectID": "unit-4/lec-4-1-slides.html#review-the-potential-outcomes-framework",
    "href": "unit-4/lec-4-1-slides.html#review-the-potential-outcomes-framework",
    "title": "Lecture 4.1: Estimating Average Treatment Effects",
    "section": "Review: The Potential Outcomes Framework",
    "text": "Review: The Potential Outcomes Framework\n\n\nFor each unit \\(i\\), we define two potential outcomes:\n\n\\(Y_i(1)\\): Outcome if unit receives treatment\n\\(Y_i(0)\\): Outcome if unit does not receive treatment\n\nThe causal effect for unit \\(i\\): \\(\\delta_i = Y_i(1) - Y_i(0)\\)\nFundamental problem of causal inference: We only observe one potential outcome for each unit\nThe observed outcome: \\(Y_i = Y_i(W_i)\\) where \\(W_i \\in \\{0,1\\}\\) is the treatment assignment\n\n\n\nWe typically focus on the Average Treatment Effect (ATE): \\[\\tau = E[Y_i(1) - Y_i(0)]\\]",
    "crumbs": [
      "Unit 4: Mechanisms",
      "Lecture Slides: Unit 4.1: Estimating Average Treatment Effects"
    ]
  },
  {
    "objectID": "unit-4/lec-4-1-slides.html#why-randomization-works",
    "href": "unit-4/lec-4-1-slides.html#why-randomization-works",
    "title": "Lecture 4.1: Estimating Average Treatment Effects",
    "section": "Why Randomization Works",
    "text": "Why Randomization Works\n\n\nRandomization makes treatment assignment independent of potential outcomes: \\[W_i \\perp\\!\\!\\!\\perp (Y_i(0), Y_i(1))\\]\nThis means treatment and control groups are balanced on observable and unobservable characteristics\nIn expectation:\n\n\\(E[Y_i(1)|W_i=1] = E[Y_i(1)]\\)\n\\(E[Y_i(0)|W_i=0] = E[Y_i(0)]\\)\n\nThus, the difference in observed outcomes provides an unbiased estimate of the ATE",
    "crumbs": [
      "Unit 4: Mechanisms",
      "Lecture Slides: Unit 4.1: Estimating Average Treatment Effects"
    ]
  },
  {
    "objectID": "unit-4/lec-4-1-slides.html#the-difference-in-means-estimator",
    "href": "unit-4/lec-4-1-slides.html#the-difference-in-means-estimator",
    "title": "Lecture 4.1: Estimating Average Treatment Effects",
    "section": "The Difference-in-Means Estimator",
    "text": "The Difference-in-Means Estimator\nThe simplest approach to estimate the ATE in an RCT is the difference in means:\n\\[\\hat{\\tau}_{DM} = \\frac{1}{n_1}\\sum_{i:W_i=1}Y_i - \\frac{1}{n_0}\\sum_{i:W_i=0}Y_i\\]\n\n\n\\(n_1\\): Number of treated units\n\\(n_0\\): Number of control units\nProperties:\n\nUnbiased for the ATE: \\(E[\\hat{\\tau}_{DM}] = \\tau\\)\nVariance depends on outcome variation\nSimple to calculate and understand",
    "crumbs": [
      "Unit 4: Mechanisms",
      "Lecture Slides: Unit 4.1: Estimating Average Treatment Effects"
    ]
  },
  {
    "objectID": "unit-4/lec-4-1-slides.html#room-for-improvement",
    "href": "unit-4/lec-4-1-slides.html#room-for-improvement",
    "title": "Lecture 4.1: Estimating Average Treatment Effects",
    "section": "Room for Improvement",
    "text": "Room for Improvement\nDespite randomization, the difference-in-means estimator has limitations:\n\n\nMay have high variance, especially with small samples\nDoes not leverage baseline information\nDoes not account for the design of the experiment (e.g., stratification)\nSubject to chance imbalances between treatment and control\n\n\n\nKey Question: How can we improve precision while maintaining unbiasedness?",
    "crumbs": [
      "Unit 4: Mechanisms",
      "Lecture Slides: Unit 4.1: Estimating Average Treatment Effects"
    ]
  },
  {
    "objectID": "unit-4/lec-4-1-slides.html#lecture-roadmap",
    "href": "unit-4/lec-4-1-slides.html#lecture-roadmap",
    "title": "Lecture 4.1: Estimating Average Treatment Effects",
    "section": "Lecture Roadmap",
    "text": "Lecture Roadmap\nToday we’ll explore three approaches to estimating the ATE:\n\n\nStandard Regression Approach\n\nControlling for baseline covariates in RCTs\nANCOVA specification\n\nPost-Double Selection Lasso\n\nA principled approach to covariate selection\nBalancing precision and researcher degrees of freedom\n\nMachine Learning Approaches\n\nUsing flexible methods to model treatment effects\nAddressing non-linearities in outcome relationships",
    "crumbs": [
      "Unit 4: Mechanisms",
      "Lecture Slides: Unit 4.1: Estimating Average Treatment Effects"
    ]
  },
  {
    "objectID": "unit-4/lec-4-1-slides.html#standard-approach-to-ate-estimation-in-rcts",
    "href": "unit-4/lec-4-1-slides.html#standard-approach-to-ate-estimation-in-rcts",
    "title": "Lecture 4.1: Estimating Average Treatment Effects",
    "section": "Standard Approach to ATE Estimation in RCTs",
    "text": "Standard Approach to ATE Estimation in RCTs\n\\[Y_{it} = \\alpha + \\theta T_i + \\beta Y_{i(t-1)} + \\sum\\beta_i X_i + \\sum\\delta_s + \\varepsilon_i\\]\n\n\nThe equation above represents the canonical specification for ATE estimation in RCTs\nParameters of interest:\n\n\\(Y_{it}\\): Outcome variable for unit \\(i\\) at follow-up time \\(t\\)\n\\(T_i\\): Treatment indicator (1 = treated, 0 = control)\n\\(\\theta\\): Average treatment effect (our target)\n\\(Y_{i(t-1)}\\): Baseline value of the outcome\n\\(X_i\\): Vector of baseline covariates\n\\(\\delta_s\\): Strata fixed effects",
    "crumbs": [
      "Unit 4: Mechanisms",
      "Lecture Slides: Unit 4.1: Estimating Average Treatment Effects"
    ]
  },
  {
    "objectID": "unit-4/lec-4-1-slides.html#basic-specification-no-controls",
    "href": "unit-4/lec-4-1-slides.html#basic-specification-no-controls",
    "title": "Lecture 4.1: Estimating Average Treatment Effects",
    "section": "Basic Specification: No Controls",
    "text": "Basic Specification: No Controls\n\\[Y_{it} = \\alpha + \\theta T_i + \\varepsilon_i\\]\n\n\nSimplest specification: regress outcome on treatment indicator only\nValid in expectation due to randomization\n\\(\\theta\\) is unbiased estimate of the ATE\nCritical note on inference:\n\nUse robust standard errors to account for heteroskedasticity\nIf clustered randomization, use cluster-robust standard errors\n\\(SE(\\hat{\\theta})_{robust} = \\sqrt{\\frac{\\hat{\\sigma}^2_1}{n_1} + \\frac{\\hat{\\sigma}^2_0}{n_0}}\\)\n\nOften leaves substantial unexplained variation in the outcome\n\n\n\nThis approach is equivalent to a simple difference in means between treatment and control groups.",
    "crumbs": [
      "Unit 4: Mechanisms",
      "Lecture Slides: Unit 4.1: Estimating Average Treatment Effects"
    ]
  },
  {
    "objectID": "unit-4/lec-4-1-slides.html#adding-strata-fixed-effects",
    "href": "unit-4/lec-4-1-slides.html#adding-strata-fixed-effects",
    "title": "Lecture 4.1: Estimating Average Treatment Effects",
    "section": "Adding Strata Fixed Effects",
    "text": "Adding Strata Fixed Effects\n\\[Y_{it} = \\alpha + \\theta T_i + \\sum\\delta_s + \\varepsilon_i\\]\n\n\nWhen randomization is stratified, include fixed effects for each stratum\n\\(\\sum\\delta_s\\) represents indicator variables for each randomization stratum\nBenefits:\n\nAccounts for the design of the experiment\nImproves precision by removing between-strata variation\nEnsures valid inference (failure to include can lead to incorrect standard errors)\n\nImplementation:\n\nInclude dummy variables for all but one stratum\nOr use factor variables in statistical software (e.g., in R: factor(stratum))",
    "crumbs": [
      "Unit 4: Mechanisms",
      "Lecture Slides: Unit 4.1: Estimating Average Treatment Effects"
    ]
  },
  {
    "objectID": "unit-4/lec-4-1-slides.html#adding-baseline-controls",
    "href": "unit-4/lec-4-1-slides.html#adding-baseline-controls",
    "title": "Lecture 4.1: Estimating Average Treatment Effects",
    "section": "Adding Baseline Controls",
    "text": "Adding Baseline Controls\n\\[Y_{it} = \\alpha + \\theta T_i + \\beta Y_{i(t-1)} + \\sum\\beta_i X_i + \\sum\\delta_s + \\varepsilon_i\\]\n\n\nBaseline outcome (\\(Y_{i(t-1)}\\)):\n\nUsually provides largest precision gain\nFlexibly controls for autocorrelation\nPreferable to “difference-in-differences” in most RCTs\nCoefficient \\(\\beta\\) need not equal 1 (unlike DiD)\n\nAdditional covariates (\\(X_i\\)):\n\nMust be measured pre-treatment\nShould predict residual variation in outcome\nBalance checks not necessary to justify inclusion\nHelps account for chance imbalances\n\n\n\n\nThis approach (ANCOVA) is the gold standard for most RCTs with baseline data.",
    "crumbs": [
      "Unit 4: Mechanisms",
      "Lecture Slides: Unit 4.1: Estimating Average Treatment Effects"
    ]
  },
  {
    "objectID": "unit-4/lec-4-1-slides.html#considerations-on-including-covariate-controls",
    "href": "unit-4/lec-4-1-slides.html#considerations-on-including-covariate-controls",
    "title": "Lecture 4.1: Estimating Average Treatment Effects",
    "section": "Considerations on Including Covariate Controls",
    "text": "Considerations on Including Covariate Controls\n\\[Y_{it} = \\alpha + \\theta T_i + \\beta Y_{i(t-1)} + \\sum\\beta_i X_i + \\sum\\delta_s + \\varepsilon_i\\]\n\n\nResearch Design Considerations:\n\nPre-specify controls in analysis plan\nAvoid “fishing” for significant results\nBalance precision gains against complexity\nGuard against researcher degrees of freedom\n\n\nThe Selection Problem:\n\nWhich covariates should we include?\nHow many covariates are too many?\nHow do we handle many potential controls?\nCan we automate control selection?\n\n\n\nThis is where Post-Double Selection Lasso can provide a principled solution.",
    "crumbs": [
      "Unit 4: Mechanisms",
      "Lecture Slides: Unit 4.1: Estimating Average Treatment Effects"
    ]
  },
  {
    "objectID": "unit-4/lec-4-1-slides.html#pds-lasso-in-field-experiments",
    "href": "unit-4/lec-4-1-slides.html#pds-lasso-in-field-experiments",
    "title": "Lecture 4.1: Estimating Average Treatment Effects",
    "section": "PDS Lasso in Field Experiments",
    "text": "PDS Lasso in Field Experiments\nPost-Double Selection Lasso for Randomized Trials\n\n\nPDS Lasso: A principled approach to selecting control variables in field experiments\nAddresses the key question: Which covariates should we include in our regression?\nHelps balance competing concerns:\n\nIncluding relevant controls improves precision\nAd hoc selection creates researcher degrees of freedom (p-hacking concerns)\nToo many controls reduces power in small samples\n\nProvides a data-driven alternative to traditional ANCOVA approaches\nOriginally developed for observational studies but increasingly applied to RCTs\n\n\n\nKey Question: In a randomized experiment, why use a method designed for observational studies?",
    "crumbs": [
      "Unit 4: Mechanisms",
      "Lecture Slides: Unit 4.1: Estimating Average Treatment Effects"
    ]
  },
  {
    "objectID": "unit-4/lec-4-1-slides.html#the-pds-lasso-method",
    "href": "unit-4/lec-4-1-slides.html#the-pds-lasso-method",
    "title": "Lecture 4.1: Estimating Average Treatment Effects",
    "section": "The PDS Lasso Method",
    "text": "The PDS Lasso Method\nThree-Step Procedure:\n\nStep 1: Use Lasso to select controls that predict the outcome\n\nRegress Y on covariates X (excluding treatment)\nLasso penalty shrinks coefficients toward zero\nSet of selected variables: I₁\n\nStep 2: Use Lasso to select controls that predict treatment\n\nRegress T on covariates X\nSet of selected variables: I₂\n\nStep 3: Estimate treatment effect by OLS\n\nRegress Y on T and all selected controls (I = I₁ ∪ I₂)\nOptional “amelioration set” (I₃) of variables always included",
    "crumbs": [
      "Unit 4: Mechanisms",
      "Lecture Slides: Unit 4.1: Estimating Average Treatment Effects"
    ]
  },
  {
    "objectID": "unit-4/lec-4-1-slides.html#the-pds-lasso-method-1",
    "href": "unit-4/lec-4-1-slides.html#the-pds-lasso-method-1",
    "title": "Lecture 4.1: Estimating Average Treatment Effects",
    "section": "The PDS Lasso Method",
    "text": "The PDS Lasso Method\n\n\n\n\n\nflowchart TD\n    A[Potential Controls X] --&gt; B[\"Lasso Step 1: &lt;br&gt; Predict Outcome\"]\n    A --&gt; C[\"Lasso Step 2: &lt;br&gt; Predict Treatment\"]\n    B --&gt; D[Selected Controls I₁]\n    C --&gt; E[Selected Controls I₂]\n    D --&gt; F[Union of Controls &lt;br&gt; I₁ ∪ I₂ ∪ I₃]\n    E --&gt; F\n    G[Amelioration Set I₃] --&gt; F\n    F --&gt; H[Final OLS Regression &lt;br&gt; Y ~ T + Selected Controls]",
    "crumbs": [
      "Unit 4: Mechanisms",
      "Lecture Slides: Unit 4.1: Estimating Average Treatment Effects"
    ]
  },
  {
    "objectID": "unit-4/lec-4-1-slides.html#why-double-selection-in-rcts",
    "href": "unit-4/lec-4-1-slides.html#why-double-selection-in-rcts",
    "title": "Lecture 4.1: Estimating Average Treatment Effects",
    "section": "Why Double Selection in RCTs?",
    "text": "Why Double Selection in RCTs?\n\n\nThe Theoretical Paradox:\n\nIn perfectly randomized experiments, treatment is orthogonal to all covariates\nSo why model treatment prediction at all?\n\nPractical Justifications:\n\nSmall sample sizes lead to chance imbalances\nAttrition creates non-random final analysis samples\nField experiments often have both issues (average 15% attrition)\n\n\n\nKey Insights:\n\nLasso in Step 1 may miss variables with:\n\nModerate associations with outcomes (due to regularization)\nStrong correlations with treatment\n\nStep 2 provides a “second chance” to capture these important variables, enhancing robustness against:\n\nChance imbalances\nSelective attrition\n\nParticularly valuable when variables have moderate outcome correlations but strong treatment imbalances",
    "crumbs": [
      "Unit 4: Mechanisms",
      "Lecture Slides: Unit 4.1: Estimating Average Treatment Effects"
    ]
  },
  {
    "objectID": "unit-4/lec-4-1-slides.html#pds-lasso-in-practice",
    "href": "unit-4/lec-4-1-slides.html#pds-lasso-in-practice",
    "title": "Lecture 4.1: Estimating Average Treatment Effects",
    "section": "PDS Lasso in Practice",
    "text": "PDS Lasso in Practice\nCilliers, Elashmawy, and McKenzie (2024)\nEmpirical Findings from 780 Treatment Estimates:\n\nDespite many potential controls (median 182), PDS Lasso selects few (median 2)\nStep 1 (outcome prediction): Selects at least one variable in 71% of cases\nStep 2 (treatment prediction): Selects no variables in 57% of cases\nAlmost no overlap between variables selected in Steps 1 and 2\n\nImpact relative to ANCOVA:\n\nVery minimal changes in treatment estimates (median 0.01 SD)\nSlight reduction in standard errors (median ratio 0.992)\nYields larger standard errors in about 25% of cases\nImplied MDE reduction: only 0.9% at median",
    "crumbs": [
      "Unit 4: Mechanisms",
      "Lecture Slides: Unit 4.1: Estimating Average Treatment Effects"
    ]
  },
  {
    "objectID": "unit-4/lec-4-1-slides.html#pds-lasso-in-practice---cilliers-elashmawy-and-mckenzie-2024",
    "href": "unit-4/lec-4-1-slides.html#pds-lasso-in-practice---cilliers-elashmawy-and-mckenzie-2024",
    "title": "Lecture 4.1: Estimating Average Treatment Effects",
    "section": "PDS Lasso in Practice - Cilliers, Elashmawy, and McKenzie (2024)",
    "text": "PDS Lasso in Practice - Cilliers, Elashmawy, and McKenzie (2024)\n\nDistribution of Selected VariablesKey finding: PDS Lasso offers limited precision gains compared to standard ANCOVA approaches in most field experiments",
    "crumbs": [
      "Unit 4: Mechanisms",
      "Lecture Slides: Unit 4.1: Estimating Average Treatment Effects"
    ]
  },
  {
    "objectID": "unit-4/lec-4-1-slides.html#implementation-considerations",
    "href": "unit-4/lec-4-1-slides.html#implementation-considerations",
    "title": "Lecture 4.1: Estimating Average Treatment Effects",
    "section": "Implementation Considerations",
    "text": "Implementation Considerations\nKey Decisions When Using PDS Lasso:\n\n\nPenalty Parameter λ:\n\nControls degree of variable selection\nDefault “plug-in” λ: Increases with covariates and sample size\n\nTheoretically justified for causal inference\nTends to select few variables\n\nCross-validation: Select λ to minimize prediction error\n\nSelects many more variables (often 5-7x more)\nCan be unstable and risks overfitting\nLimited theoretical justification for causal inference\n\n\n\nInput Variable Selection:\n\n“Kitchen sink” approach common but problematic\n\nPenalty increases with number of potential controls\nToo many variables can lead to none being selected\n\nAmelioration Set (I₃):\n\nVariables always included regardless of selection\nRecommended inclusions:\n\nLagged dependent variable\nRandomization strata fixed effects\n\nGuards against underselection of important variables",
    "crumbs": [
      "Unit 4: Mechanisms",
      "Lecture Slides: Unit 4.1: Estimating Average Treatment Effects"
    ]
  },
  {
    "objectID": "unit-4/lec-4-1-slides.html#best-practices-for-pds-lasso",
    "href": "unit-4/lec-4-1-slides.html#best-practices-for-pds-lasso",
    "title": "Lecture 4.1: Estimating Average Treatment Effects",
    "section": "Best Practices for PDS Lasso",
    "text": "Best Practices for PDS Lasso\nChecklist for Effective Implementation:\n\n\nBe realistic about power gains\n\nExpect very small reductions in standard errors compared to ANCOVA\nDon’t rely on large improvements for power calculations\n\nInclude key variables in amelioration set\n\nLagged dependent variable\nRandomization strata or matched pair fixed effects\n\nBe judicious with input controls\n\nAvoid “kitchen sink” approach with hundreds of variables\nFocus on variables with potential outcome correlation\n\nHandle missing values carefully\n\nEnsure all input controls have no missing values (e.g., by dummying out)\nMissing values reduced final samples in many studies\n\nUse as a robustness check\n\nProvides a principled alternative to ad hoc control selection\nLarge coefficient changes may signal concerns about randomization",
    "crumbs": [
      "Unit 4: Mechanisms",
      "Lecture Slides: Unit 4.1: Estimating Average Treatment Effects"
    ]
  },
  {
    "objectID": "unit-4/lec-4-1-slides.html#key-takeaways",
    "href": "unit-4/lec-4-1-slides.html#key-takeaways",
    "title": "Lecture 4.1: Estimating Average Treatment Effects",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\n\nRegression adjustment improves precision\n\nIn randomized trials, it’s not needed for unbiasedness but helps with efficiency\nThe baseline outcome is typically the most important control\n\nStandard ANCOVA is a robust approach\n\nPre-specify key controls (baseline outcome, strata)\nWorks well in most field experiments\n\nPDS Lasso provides a principled control selection method\n\nBut offers limited gains over ANCOVA in most cases\nInclude key variables in the amelioration set\n\nMachine learning methods can help with non-linear relationships\n\nBut may not offer large gains in typical field experiment settings\nRequire careful implementation to maintain valid inference",
    "crumbs": [
      "Unit 4: Mechanisms",
      "Lecture Slides: Unit 4.1: Estimating Average Treatment Effects"
    ]
  },
  {
    "objectID": "unit-4/lec-4-1-slides.html#next-steps",
    "href": "unit-4/lec-4-1-slides.html#next-steps",
    "title": "Lecture 4.1: Estimating Average Treatment Effects",
    "section": "Next Steps",
    "text": "Next Steps\n\n\nDoubly Robust Estimation\n\nProtecting against model misspecification\n\nDouble/Debiased Machine Learning\n\nCombining machine learning with robust inference\n\nCausal Forests\n\nEstimating heterogeneous treatment effects",
    "crumbs": [
      "Unit 4: Mechanisms",
      "Lecture Slides: Unit 4.1: Estimating Average Treatment Effects"
    ]
  },
  {
    "objectID": "unit-3/lec-3-3-slides.html#unit-2-basic-ml-crash-course",
    "href": "unit-3/lec-3-3-slides.html#unit-2-basic-ml-crash-course",
    "title": "Tree-based Methods",
    "section": "Unit 2: Basic ML Crash Course",
    "text": "Unit 2: Basic ML Crash Course\n\nIntroduction to ML\nLasso and friends (Linear High-dimensional Regression)\nTree-based methods (Nonlinear)",
    "crumbs": [
      "Unit 3: Supervised Learning",
      "Lecture Slides: Unit 3.3: Tree-based Methods"
    ]
  },
  {
    "objectID": "unit-3/lec-3-3-slides.html#basic-ml-setup",
    "href": "unit-3/lec-3-3-slides.html#basic-ml-setup",
    "title": "Tree-based Methods",
    "section": "Basic ML Setup",
    "text": "Basic ML Setup\n\nFlexible functional forms\nLimit expressiveness via regularization\nLearn how much to regularize tuning\n\n\n\nWhat do the features imply about properties of \\(\\hat{f}\\) ?\nHow can we use \\(\\hat{f}\\) in applied data analyses?",
    "crumbs": [
      "Unit 3: Supervised Learning",
      "Lecture Slides: Unit 3.3: Tree-based Methods"
    ]
  },
  {
    "objectID": "unit-3/lec-3-3-slides.html#the-approximation-overfit-tradeoff",
    "href": "unit-3/lec-3-3-slides.html#the-approximation-overfit-tradeoff",
    "title": "Tree-based Methods",
    "section": "The Approximation-Overfit Tradeoff",
    "text": "The Approximation-Overfit Tradeoff\n\n\nThe Fundamental Challenge\nAs model complexity increases, we face two competing forces:\n\nApproximation error decreases as we better capture the true underlying function\nEstimation error increases as we begin to fit noise in our training data\n\nThis creates the bias-variance tradeoff that defines machine learning:\n\nSimple models: High bias, low variance\nComplex models: Low bias, high variance\n\n\n\n\n\nBias-variance tradeoff visualization. The blue curve represents test error, while the red curve shows training error. The gap widens as model complexity increases, indicating overfitting.",
    "crumbs": [
      "Unit 3: Supervised Learning",
      "Lecture Slides: Unit 3.3: Tree-based Methods"
    ]
  },
  {
    "objectID": "unit-3/lec-3-3-slides.html#supervised-learning",
    "href": "unit-3/lec-3-3-slides.html#supervised-learning",
    "title": "Tree-based Methods",
    "section": "Supervised Learning",
    "text": "Supervised Learning\nFor supervised learners, we need three things:\n\nFunction Class\nA regularizer\nOptimization algorithms to guide us",
    "crumbs": [
      "Unit 3: Supervised Learning",
      "Lecture Slides: Unit 3.3: Tree-based Methods"
    ]
  },
  {
    "objectID": "unit-3/lec-3-3-slides.html#choosing-a-regularization-parameter-using-k-fold-cross-validation",
    "href": "unit-3/lec-3-3-slides.html#choosing-a-regularization-parameter-using-k-fold-cross-validation",
    "title": "Tree-based Methods",
    "section": "Choosing a regularization parameter using k-fold Cross-validation",
    "text": "Choosing a regularization parameter using k-fold Cross-validation",
    "crumbs": [
      "Unit 3: Supervised Learning",
      "Lecture Slides: Unit 3.3: Tree-based Methods"
    ]
  },
  {
    "objectID": "unit-3/lec-3-3-slides.html#full-ml-exercise",
    "href": "unit-3/lec-3-3-slides.html#full-ml-exercise",
    "title": "Tree-based Methods",
    "section": "Full ML Exercise",
    "text": "Full ML Exercise",
    "crumbs": [
      "Unit 3: Supervised Learning",
      "Lecture Slides: Unit 3.3: Tree-based Methods"
    ]
  },
  {
    "objectID": "unit-3/lec-3-3-slides.html#the-regularization-spectrum-how-we-control-complexity",
    "href": "unit-3/lec-3-3-slides.html#the-regularization-spectrum-how-we-control-complexity",
    "title": "Tree-based Methods",
    "section": "The Regularization Spectrum: How We Control Complexity",
    "text": "The Regularization Spectrum: How We Control Complexity\nEvery model class has its own unique form of regularization that controls the bias-variance tradeoff. Understanding this spectrum reveals the fundamental unity behind seemingly diverse machine learning approaches.\n\n\n\n\n\n\n\n\n\nFunction Class\nRegularization Parameters\n\n\n\n\nLinear\nLASSO, ridge, elastic net\n\n\nDecision/regression trees\nDepth, leaves, leaf size, info gain\n\n\nRandom forest\nTrees, variables per tree, sample sizes, complexity\n\n\nNearest neighbors\nNumber of neighbors\n\n\nKernel regression\nBandwidth\n\n\nSplines\nNumber of knots, order\n\n\nNeural nets\nLayers, sizes, connectivity, drop-out, early stopping\n\n\n\n\n\n\nCross-cutting insight: While the specific mechanisms differ, regularization always involves restricting a model’s capacity to memorize training data, instead encouraging it to generalize underlying patterns. Tree-based methods share this fundamental principle with linear models, but implement it through structural constraints rather than coefficient penalties.",
    "crumbs": [
      "Unit 3: Supervised Learning",
      "Lecture Slides: Unit 3.3: Tree-based Methods"
    ]
  },
  {
    "objectID": "unit-3/lec-3-3-slides.html#lasso-regression-constrained-minimization-to-regularize",
    "href": "unit-3/lec-3-3-slides.html#lasso-regression-constrained-minimization-to-regularize",
    "title": "Tree-based Methods",
    "section": "Lasso Regression: Constrained Minimization to Regularize",
    "text": "Lasso Regression: Constrained Minimization to Regularize\n\nObjective: Minimize the sum of squared errors while keeping coefficients small\n\n\n\nConstrained Form\n\\[\n\\begin{align}\n\\min_{\\beta} &\\sum_{i=1}^{n} \\left(y_i - \\beta_0 - \\sum_{j=1}^{p} \\beta_j x_{ij}\\right)^2\\\\\n\\text{subject to } &\\sum_{j=1}^{p} |\\beta_j| \\leq t\n\\end{align}\n\\]\n\n\\(t \\geq 0\\) is the constraint parameter\nSmaller \\(t\\) means more regularization\n\\(t = 0\\) forces all \\(\\beta_j = 0\\)\n\n\n\nContours of RSS function and lasso constraint region (diamond). Solution occurs at corners, forcing coefficients to zero.",
    "crumbs": [
      "Unit 3: Supervised Learning",
      "Lecture Slides: Unit 3.3: Tree-based Methods"
    ]
  },
  {
    "objectID": "unit-3/lec-3-3-slides.html#lasso-lagrangian-form",
    "href": "unit-3/lec-3-3-slides.html#lasso-lagrangian-form",
    "title": "Tree-based Methods",
    "section": "Lasso: Lagrangian Form",
    "text": "Lasso: Lagrangian Form\n\\[\n\\min_{\\beta} \\sum_{i=1}^{n} \\left(y_i - \\beta_0 - \\sum_{j=1}^{p} \\beta_j x_{ij}\\right)^2 + \\lambda \\sum_{j=1}^{p} |\\beta_j|\n\\]\n\n\\(\\lambda \\geq 0\\) is the penalty parameter\n\\(\\lambda\\) and \\(t\\) have an inverse relationship\nAs \\(\\lambda \\rightarrow \\infty\\), all \\(\\beta_j \\rightarrow 0\\)",
    "crumbs": [
      "Unit 3: Supervised Learning",
      "Lecture Slides: Unit 3.3: Tree-based Methods"
    ]
  },
  {
    "objectID": "unit-3/lec-3-3-slides.html#geometric-interpretation",
    "href": "unit-3/lec-3-3-slides.html#geometric-interpretation",
    "title": "Tree-based Methods",
    "section": "Geometric Interpretation",
    "text": "Geometric Interpretation\n\n\n\nThe constraint region: \\(\\sum_{j=1}^{p} |\\beta_j| \\leq t\\) forms a diamond (L1 norm)\nUnlike ridge regression’s circular constraint (L2 norm)\nKey insight: The corners of the diamond often intersect with axes\nThis means some coefficients become exactly zero\n\n\n\nContours of RSS function and lasso constraint region (diamond). Solution occurs at corners, forcing coefficients to zero.",
    "crumbs": [
      "Unit 3: Supervised Learning",
      "Lecture Slides: Unit 3.3: Tree-based Methods"
    ]
  },
  {
    "objectID": "unit-3/lec-3-3-slides.html#lasso-lambda-and-coefficient-paths-relaxing-constraint",
    "href": "unit-3/lec-3-3-slides.html#lasso-lambda-and-coefficient-paths-relaxing-constraint",
    "title": "Tree-based Methods",
    "section": "Lasso Lambda and coefficient paths (relaxing constraint)",
    "text": "Lasso Lambda and coefficient paths (relaxing constraint)\n\nEach line is a coefficient. Lambda is “relaxed” moving from left to right",
    "crumbs": [
      "Unit 3: Supervised Learning",
      "Lecture Slides: Unit 3.3: Tree-based Methods"
    ]
  },
  {
    "objectID": "unit-3/lec-3-3-slides.html#why-lasso-performs-variable-selection",
    "href": "unit-3/lec-3-3-slides.html#why-lasso-performs-variable-selection",
    "title": "Tree-based Methods",
    "section": "Why Lasso Performs Variable Selection",
    "text": "Why Lasso Performs Variable Selection\n\nThe L1 penalty’s diamond shape makes it likely for solutions to occur at corners where some \\(\\beta_j = 0\\)\n\n\n\n\nSolution occurs where RSS contours touch constraint region\nCorners of diamond intersect with coordinate axes\nWhen solution is at a corner, some coefficients equal zero\nResult: Automatic variable selection\n\n\n\nComparison of lasso (diamond) vs. ridge (circle) vs. elastic net (fat diamond thingy) constraint regions.",
    "crumbs": [
      "Unit 3: Supervised Learning",
      "Lecture Slides: Unit 3.3: Tree-based Methods"
    ]
  },
  {
    "objectID": "unit-3/lec-3-3-slides.html#the-lasso-selection-problem",
    "href": "unit-3/lec-3-3-slides.html#the-lasso-selection-problem",
    "title": "Tree-based Methods",
    "section": "The Lasso Selection Problem",
    "text": "The Lasso Selection Problem\n\nChallenge: Different regularization paths can lead to different selected variables\n\n\nOLS:\nHealth = β₀ + β₁·Age + β₂·Income + β₃·Education + β₄·SES + … + βₙ·X_n + ε\n\n\nLasso (1):\nHealth = β₀ + β₁·Age + β₂·Income + β₃·Education + … + βₙ·X_n + ε\n\n\nLasso (2):\nHealth = β₀ + β₁·Age + + β₄·SES + … + βₙ·X_n + ε",
    "crumbs": [
      "Unit 3: Supervised Learning",
      "Lecture Slides: Unit 3.3: Tree-based Methods"
    ]
  },
  {
    "objectID": "unit-3/lec-3-3-slides.html#lasso-selection-problem",
    "href": "unit-3/lec-3-3-slides.html#lasso-selection-problem",
    "title": "Tree-based Methods",
    "section": "Lasso Selection Problem",
    "text": "Lasso Selection Problem\nKey implications:\n\nVariable selection depends heavily on choice of λ\nHighly correlated predictors compete for selection\nDifferent random seeds in cross-validation → different final models\nSelection can be unstable with small changes in data",
    "crumbs": [
      "Unit 3: Supervised Learning",
      "Lecture Slides: Unit 3.3: Tree-based Methods"
    ]
  },
  {
    "objectID": "unit-3/lec-3-3-slides.html#lasso-selection-instability",
    "href": "unit-3/lec-3-3-slides.html#lasso-selection-instability",
    "title": "Tree-based Methods",
    "section": "Lasso Selection Instability",
    "text": "Lasso Selection Instability\n\n\nLasso variable selection often exhibits instability across different runs, even on the same dataset:\n\nSmall changes in data can lead to completely different sets of selected variables\nHighly correlated predictors compete for selection\nCross-validation randomness affects final model composition\nBootstrapping or repeated CV shows selection frequencies\n\nImplications for practice: - Single Lasso runs may miss important variables - Ensemble approaches can provide more stable selection - Variable selection consistency is not guaranteed\n\n\n\n\nVariable selection across 10 iterations of Lasso on the same dataset. Black indicates selected; white indicates excluded. Notice the inconsistent pattern of selection.",
    "crumbs": [
      "Unit 3: Supervised Learning",
      "Lecture Slides: Unit 3.3: Tree-based Methods"
    ]
  },
  {
    "objectID": "unit-3/lec-3-3-slides.html#tree-based-methods-overview",
    "href": "unit-3/lec-3-3-slides.html#tree-based-methods-overview",
    "title": "Tree-based Methods",
    "section": "Tree-Based Methods: Overview",
    "text": "Tree-Based Methods: Overview\n\nPreviously: linear models, regularization (ridge, lasso, elastic net)\nNow: flexible, non-parametric approaches for prediction and classification\n\nKey Question:\nHow can we model non-linear relationships without assuming functional form?\n\nDecision trees and (mostly) their extensions…\nTree-based methods include random forests, bagged trees, boosted trees, which combine many decision trees in different ways (called an ensemble)..",
    "crumbs": [
      "Unit 3: Supervised Learning",
      "Lecture Slides: Unit 3.3: Tree-based Methods"
    ]
  },
  {
    "objectID": "unit-3/lec-3-3-slides.html#why-tree-based-methods",
    "href": "unit-3/lec-3-3-slides.html#why-tree-based-methods",
    "title": "Tree-based Methods",
    "section": "Why Tree-Based Methods?",
    "text": "Why Tree-Based Methods?\n\nPowerful non-parametric approaches that:\n\nMake no assumptions about functional form\nAutomatically model interactions between predictors\nHandle different types of predictors (continuous, categorical)\nAre highly interpretable (single trees)\nCan achieve excellent predictive performance (ensembles)\n\n\n\nPerfect for health services research, where relationships are often complex and non-linear!",
    "crumbs": [
      "Unit 3: Supervised Learning",
      "Lecture Slides: Unit 3.3: Tree-based Methods"
    ]
  },
  {
    "objectID": "unit-3/lec-3-3-slides.html#decision-trees-intuition",
    "href": "unit-3/lec-3-3-slides.html#decision-trees-intuition",
    "title": "Tree-based Methods",
    "section": "Decision Trees: Intuition",
    "text": "Decision Trees: Intuition\n\n\n\nTrees reflect human decision-making processes\nWe navigate a series of yes/no questions\nArrive at a prediction based on answers to those questions\n\n\nTrees segment predictor space into regions, then:\n\nFor regression: Predict mean outcome value in that region\nFor classification: Predict most common class in that region\n\nThe beauty is in the simplicity: - No formula needed - Can capture complex relationships without parametric assumptions",
    "crumbs": [
      "Unit 3: Supervised Learning",
      "Lecture Slides: Unit 3.3: Tree-based Methods"
    ]
  },
  {
    "objectID": "unit-3/lec-3-3-slides.html#example-surviving-the-titanic",
    "href": "unit-3/lec-3-3-slides.html#example-surviving-the-titanic",
    "title": "Tree-based Methods",
    "section": "Example: Surviving the Titanic",
    "text": "Example: Surviving the Titanic",
    "crumbs": [
      "Unit 3: Supervised Learning",
      "Lecture Slides: Unit 3.3: Tree-based Methods"
    ]
  },
  {
    "objectID": "unit-3/lec-3-3-slides.html#example-surviving-the-titanic-1",
    "href": "unit-3/lec-3-3-slides.html#example-surviving-the-titanic-1",
    "title": "Tree-based Methods",
    "section": "Example: Surviving the Titanic",
    "text": "Example: Surviving the Titanic",
    "crumbs": [
      "Unit 3: Supervised Learning",
      "Lecture Slides: Unit 3.3: Tree-based Methods"
    ]
  },
  {
    "objectID": "unit-3/lec-3-3-slides.html#age-class-and-survival",
    "href": "unit-3/lec-3-3-slides.html#age-class-and-survival",
    "title": "Tree-based Methods",
    "section": "Age, Class and Survival",
    "text": "Age, Class and Survival",
    "crumbs": [
      "Unit 3: Supervised Learning",
      "Lecture Slides: Unit 3.3: Tree-based Methods"
    ]
  },
  {
    "objectID": "unit-3/lec-3-3-slides.html#age-class-and-survival-1",
    "href": "unit-3/lec-3-3-slides.html#age-class-and-survival-1",
    "title": "Tree-based Methods",
    "section": "Age, Class and Survival",
    "text": "Age, Class and Survival",
    "crumbs": [
      "Unit 3: Supervised Learning",
      "Lecture Slides: Unit 3.3: Tree-based Methods"
    ]
  },
  {
    "objectID": "unit-3/lec-3-3-slides.html#age-class-and-survival-2",
    "href": "unit-3/lec-3-3-slides.html#age-class-and-survival-2",
    "title": "Tree-based Methods",
    "section": "Age, Class and Survival",
    "text": "Age, Class and Survival",
    "crumbs": [
      "Unit 3: Supervised Learning",
      "Lecture Slides: Unit 3.3: Tree-based Methods"
    ]
  },
  {
    "objectID": "unit-3/lec-3-3-slides.html#age-class-and-survival-3",
    "href": "unit-3/lec-3-3-slides.html#age-class-and-survival-3",
    "title": "Tree-based Methods",
    "section": "Age, Class and Survival",
    "text": "Age, Class and Survival",
    "crumbs": [
      "Unit 3: Supervised Learning",
      "Lecture Slides: Unit 3.3: Tree-based Methods"
    ]
  },
  {
    "objectID": "unit-3/lec-3-3-slides.html#age-class-and-survival-tree",
    "href": "unit-3/lec-3-3-slides.html#age-class-and-survival-tree",
    "title": "Tree-based Methods",
    "section": "Age, Class and Survival: Tree",
    "text": "Age, Class and Survival: Tree\n\n\n\n\n\nflowchart TD\n    A[Age &gt; 60?] --&gt; |Yes| B[Did not survive]\n    A --&gt; |No| C[Pclass = 1?]\n    C --&gt; |Yes| D[Survived]\n    C --&gt; |No| E[Pclass = 2?]\n    E --&gt; |Yes| F[Age &gt; 20?]\n    F --&gt; |Yes| G[Did not survive]\n    F --&gt; |No| H[Did not survive]\n    E --&gt; |No| I[Age &lt; 10?]\n    I --&gt; |Yes| J[Survived]\n    I --&gt; |No| K[Did not survive]\n    \n    %% Styling\n    classDef survived fill:#a8d5e5,stroke:#333,stroke-width:1px;\n    classDef notSurvived fill:#f8cecc,stroke:#333,stroke-width:1px;\n    \n    class D,J survived;\n    class B,G,H,K notSurvived;",
    "crumbs": [
      "Unit 3: Supervised Learning",
      "Lecture Slides: Unit 3.3: Tree-based Methods"
    ]
  },
  {
    "objectID": "unit-3/lec-3-3-slides.html#how-to-grow-a-tree",
    "href": "unit-3/lec-3-3-slides.html#how-to-grow-a-tree",
    "title": "Tree-based Methods",
    "section": "How to Grow a Tree?",
    "text": "How to Grow a Tree?\nThe general approach:\n\nSplit: Choose a predictor and value to split data\nRecurse: Apply the same process to resulting regions\nPrune: Simplify the tree to prevent overfitting\n\nThe key challenge: Finding the optimal splits",
    "crumbs": [
      "Unit 3: Supervised Learning",
      "Lecture Slides: Unit 3.3: Tree-based Methods"
    ]
  },
  {
    "objectID": "unit-3/lec-3-3-slides.html#growing-a-regression-tree",
    "href": "unit-3/lec-3-3-slides.html#growing-a-regression-tree",
    "title": "Tree-based Methods",
    "section": "Growing a Regression Tree",
    "text": "Growing a Regression Tree\nGoal: Partition predictor space into non-overlapping regions \\(R_1, R_2, ..., R_J\\) to minimize:\n\\[RSS = \\sum_{j=1}^{J} \\sum_{i \\in R_j} (y_i - \\hat{y}_{R_j})^2\\]\nWhere \\(\\hat{y}_{R_j}\\) is the mean response for observations in region \\(R_j\\)\n\nWe aim to create regions where the observations within each region are as homogeneous as possible with respect to the response variable.",
    "crumbs": [
      "Unit 3: Supervised Learning",
      "Lecture Slides: Unit 3.3: Tree-based Methods"
    ]
  },
  {
    "objectID": "unit-3/lec-3-3-slides.html#recursive-binary-splitting",
    "href": "unit-3/lec-3-3-slides.html#recursive-binary-splitting",
    "title": "Tree-based Methods",
    "section": "Recursive Binary Splitting",
    "text": "Recursive Binary Splitting\n\n\nConsider all predictors \\(X_j\\) and all possible cut points \\(s\\)\nFor each combination, split predictor space into two regions: \\[R_1(j,s) = \\{X|X_j &lt; s\\} \\text{ and } R_2(j,s) = \\{X|X_j \\geq s\\}\\]\nChoose split that minimizes: \\[\\sum_{i \\in R_1(j,s)} (y_i - \\hat{y}_{R_1})^2 + \\sum_{i \\in R_2(j,s)} (y_i - \\hat{y}_{R_2})^2\\]\nRepeat recursively on each resulting region",
    "crumbs": [
      "Unit 3: Supervised Learning",
      "Lecture Slides: Unit 3.3: Tree-based Methods"
    ]
  },
  {
    "objectID": "unit-3/lec-3-3-slides.html#classification-trees",
    "href": "unit-3/lec-3-3-slides.html#classification-trees",
    "title": "Tree-based Methods",
    "section": "Classification Trees",
    "text": "Classification Trees\n\nUsed when predicting categorical outcomes\nSame recursive splitting approach, but different splitting criteria\nInstead of RSS, we use measures of node purity\n\n\nCommon impurity measures: - Classification Error Rate: \\(1 - \\max_k(p_{mk})\\) - Gini Index: \\(\\sum_{k=1}^K p_{mk}(1-p_{mk})\\) - Entropy: \\(-\\sum_{k=1}^K p_{mk}\\log(p_{mk})\\)\nWhere \\(p_{mk}\\) is the proportion of observations in node \\(m\\) that belong to class \\(k\\)",
    "crumbs": [
      "Unit 3: Supervised Learning",
      "Lecture Slides: Unit 3.3: Tree-based Methods"
    ]
  },
  {
    "objectID": "unit-3/lec-3-3-slides.html#prediction-with-trees",
    "href": "unit-3/lec-3-3-slides.html#prediction-with-trees",
    "title": "Tree-based Methods",
    "section": "Prediction with Trees",
    "text": "Prediction with Trees\nOnce the tree is built:\n\nRegression: Predict the mean response value in the leaf node\nClassification: Predict the most common class in the leaf node\n\n\nTrees can also provide probability estimates (proportions of classes in each node)",
    "crumbs": [
      "Unit 3: Supervised Learning",
      "Lecture Slides: Unit 3.3: Tree-based Methods"
    ]
  },
  {
    "objectID": "unit-3/lec-3-3-slides.html#advantages-of-trees",
    "href": "unit-3/lec-3-3-slides.html#advantages-of-trees",
    "title": "Tree-based Methods",
    "section": "Advantages of Trees",
    "text": "Advantages of Trees\n\n\nInterpretability: Easy to explain and visualize\nNo preprocessing needed: No normalization or dummy variables required\nHandle mixed data types: Both numerical and categorical predictors\nAutomatically model interactions\nNo distributional assumptions\nHandle missing data (with appropriate implementation)",
    "crumbs": [
      "Unit 3: Supervised Learning",
      "Lecture Slides: Unit 3.3: Tree-based Methods"
    ]
  },
  {
    "objectID": "unit-3/lec-3-3-slides.html#disadvantages-of-trees",
    "href": "unit-3/lec-3-3-slides.html#disadvantages-of-trees",
    "title": "Tree-based Methods",
    "section": "Disadvantages of Trees",
    "text": "Disadvantages of Trees\n\n\nHigh variance: Small changes in data can result in very different trees\nCan overfit: Tend to learn noise in the data if not constrained\nGreedy algorithm: May not find global optimum\nLimited predictive accuracy (for single trees)\nBiased toward predictors with many possible splits",
    "crumbs": [
      "Unit 3: Supervised Learning",
      "Lecture Slides: Unit 3.3: Tree-based Methods"
    ]
  },
  {
    "objectID": "unit-3/lec-3-3-slides.html#the-overfitting-challenge",
    "href": "unit-3/lec-3-3-slides.html#the-overfitting-challenge",
    "title": "Tree-based Methods",
    "section": "The Overfitting Challenge",
    "text": "The Overfitting Challenge\n\nDeep trees fit training data perfectly but generalize poorly\nNeed a way to find right-sized trees",
    "crumbs": [
      "Unit 3: Supervised Learning",
      "Lecture Slides: Unit 3.3: Tree-based Methods"
    ]
  },
  {
    "objectID": "unit-3/lec-3-3-slides.html#tree-pruning",
    "href": "unit-3/lec-3-3-slides.html#tree-pruning",
    "title": "Tree-based Methods",
    "section": "Tree Pruning",
    "text": "Tree Pruning\n\nIdea: Grow a large tree, then prune it back to prevent overfitting\nApproach: Cost-complexity pruning with a penalty for tree complexity\n\n\nFind subtree \\(T \\subset T_0\\) that minimizes:\n\\[\\sum_{m=1}^{|T|} \\sum_{i:x_i \\in R_m} (y_i - \\hat{y}_{R_m})^2 + \\alpha|T|\\]\nwhere \\(|T|\\) is the number of terminal nodes and \\(\\alpha\\) is the complexity parameter",
    "crumbs": [
      "Unit 3: Supervised Learning",
      "Lecture Slides: Unit 3.3: Tree-based Methods"
    ]
  },
  {
    "objectID": "unit-3/lec-3-3-slides.html#cross-validation-for-optimal-tree-size",
    "href": "unit-3/lec-3-3-slides.html#cross-validation-for-optimal-tree-size",
    "title": "Tree-based Methods",
    "section": "Cross-Validation for Optimal Tree Size",
    "text": "Cross-Validation for Optimal Tree Size\n\nGrow a large tree \\(T_0\\)\nFor range of \\(\\alpha\\) values, find best subtree \\(T_\\alpha\\)\nUse K-fold cross-validation to select optimal \\(\\alpha\\)\nReturn the subtree \\(T_\\alpha\\) that corresponds to the optimal \\(\\alpha\\)\n\n\nRemember: We’re using the same cross-validation framework we learned with lasso!",
    "crumbs": [
      "Unit 3: Supervised Learning",
      "Lecture Slides: Unit 3.3: Tree-based Methods"
    ]
  },
  {
    "objectID": "unit-3/lec-3-3-slides.html#the-fundamental-problem-of-trees",
    "href": "unit-3/lec-3-3-slides.html#the-fundamental-problem-of-trees",
    "title": "Tree-based Methods",
    "section": "The Fundamental Problem of Trees",
    "text": "The Fundamental Problem of Trees\nSingle trees have: - High interpretability - Limited predictive performance\n\nCan we keep the flexibility of trees while improving predictive performance?",
    "crumbs": [
      "Unit 3: Supervised Learning",
      "Lecture Slides: Unit 3.3: Tree-based Methods"
    ]
  },
  {
    "objectID": "unit-3/lec-3-3-slides.html#ensemble-methods-to-the-rescue",
    "href": "unit-3/lec-3-3-slides.html#ensemble-methods-to-the-rescue",
    "title": "Tree-based Methods",
    "section": "Ensemble Methods to the Rescue!",
    "text": "Ensemble Methods to the Rescue!\nThe core idea: Combine many trees to reduce variance and improve predictions\n\nKey ensemble approaches:\n\nBagging: Bootstrap aggregation of trees\nRandom Forests: Bagging with random feature selection\nBoosting: Sequential tree building to correct errors",
    "crumbs": [
      "Unit 3: Supervised Learning",
      "Lecture Slides: Unit 3.3: Tree-based Methods"
    ]
  },
  {
    "objectID": "unit-3/lec-3-3-slides.html#bagging-trees",
    "href": "unit-3/lec-3-3-slides.html#bagging-trees",
    "title": "Tree-based Methods",
    "section": "Bagging Trees",
    "text": "Bagging Trees",
    "crumbs": [
      "Unit 3: Supervised Learning",
      "Lecture Slides: Unit 3.3: Tree-based Methods"
    ]
  },
  {
    "objectID": "unit-3/lec-3-3-slides.html#bagging-trees-1",
    "href": "unit-3/lec-3-3-slides.html#bagging-trees-1",
    "title": "Tree-based Methods",
    "section": "Bagging Trees",
    "text": "Bagging Trees",
    "crumbs": [
      "Unit 3: Supervised Learning",
      "Lecture Slides: Unit 3.3: Tree-based Methods"
    ]
  },
  {
    "objectID": "unit-3/lec-3-3-slides.html#bagging-bootstrap-aggregation",
    "href": "unit-3/lec-3-3-slides.html#bagging-bootstrap-aggregation",
    "title": "Tree-based Methods",
    "section": "Bagging (Bootstrap Aggregation)",
    "text": "Bagging (Bootstrap Aggregation)\n\nCreate \\(B\\) bootstrap samples from training data\nFit a deep, unpruned tree to each bootstrap sample\nAverage predictions across all trees (for regression)\nTake majority vote (for classification)\n\n\nKey insight: Averaging many high-variance, low-bias estimators gives a lower-variance, low-bias ensemble",
    "crumbs": [
      "Unit 3: Supervised Learning",
      "Lecture Slides: Unit 3.3: Tree-based Methods"
    ]
  },
  {
    "objectID": "unit-3/lec-3-3-slides.html#ensemble-prediction",
    "href": "unit-3/lec-3-3-slides.html#ensemble-prediction",
    "title": "Tree-based Methods",
    "section": "Ensemble Prediction",
    "text": "Ensemble Prediction\nBy averaging over many low bias but high variance models, we reduce the overall variance ⇒ more accurate model.\n\nthe average of all \\(B\\) predictions for regression trees: \\(\\hat{y}_{bag}(X) = \\frac{1}{B} \\sum_{i=1}^{B} \\hat{y}^i(X)\\)\nthe majority vote by all \\(B\\) predictions for classification trees: \\(\\hat{y}_{bag}(X) = \\underset{k}{\\arg\\max} \\sum_{i=1}^{B} I\\{\\hat{y}^i(X) = k\\}\\)\n\n\n\n\n\n\nflowchart TB\n    Forecast[\"Forecast for any new X\"] --&gt; DT1[\"DT 1&lt;br&gt;sample 1&lt;br&gt;random S%\"]\n    Forecast --&gt; DT2[\"DT 2&lt;br&gt;sample 2&lt;br&gt;random S%\"]\n    Forecast --&gt; dots1[\"...\"]\n    Forecast --&gt; dots2[\"...\"]\n    Forecast --&gt; dots3[\"...\"]\n    Forecast --&gt; DTB[\"DT B&lt;br&gt;sample B&lt;br&gt;random S%\"]",
    "crumbs": [
      "Unit 3: Supervised Learning",
      "Lecture Slides: Unit 3.3: Tree-based Methods"
    ]
  },
  {
    "objectID": "unit-3/lec-3-3-slides.html#majority-vote",
    "href": "unit-3/lec-3-3-slides.html#majority-vote",
    "title": "Tree-based Methods",
    "section": "Majority Vote",
    "text": "Majority Vote",
    "crumbs": [
      "Unit 3: Supervised Learning",
      "Lecture Slides: Unit 3.3: Tree-based Methods"
    ]
  },
  {
    "objectID": "unit-3/lec-3-3-slides.html#out-of-bag-error-estimation",
    "href": "unit-3/lec-3-3-slides.html#out-of-bag-error-estimation",
    "title": "Tree-based Methods",
    "section": "Out-of-Bag Error Estimation",
    "text": "Out-of-Bag Error Estimation\n\nEach bootstrap sample only uses ~2/3 of observations\nRemaining ~1/3 (“out-of-bag” or OOB) can be used as validation data\nFor each observation, average predictions from trees that didn’t use it in training\nProvides honest error estimate without separate validation set",
    "crumbs": [
      "Unit 3: Supervised Learning",
      "Lecture Slides: Unit 3.3: Tree-based Methods"
    ]
  },
  {
    "objectID": "unit-3/lec-3-3-slides.html#random-forests-decorrelating-trees",
    "href": "unit-3/lec-3-3-slides.html#random-forests-decorrelating-trees",
    "title": "Tree-based Methods",
    "section": "Random Forests: Decorrelating Trees",
    "text": "Random Forests: Decorrelating Trees\nRandom forests address a weakness of bagging:\n\nIn bagging, trees can be highly correlated if strong predictors dominate splits\n\n\nRandom forests solution:\n\nAt each split, consider only a random subset of \\(m\\) predictors\nTypically \\(m \\approx \\sqrt{p}\\) for classification, \\(m \\approx p/3\\) for regression\nFor example, with sample of 100 predictors, sample 10 in each split for class.\nDecorrelates trees, further reducing variance\n\n\n\nRandom Forest are Particularly useful when there is one strong predictor or a set of predictors that are highly correlated with each other. (Why?)",
    "crumbs": [
      "Unit 3: Supervised Learning",
      "Lecture Slides: Unit 3.3: Tree-based Methods"
    ]
  },
  {
    "objectID": "unit-3/lec-3-3-slides.html#boosting-sequential-learning",
    "href": "unit-3/lec-3-3-slides.html#boosting-sequential-learning",
    "title": "Tree-based Methods",
    "section": "Boosting: Sequential Learning",
    "text": "Boosting: Sequential Learning\nUnlike bagging/random forests (parallel tree building), boosting:\n\nFits trees sequentially\nEach tree focuses on errors of previous trees\nCombines many “weak learners” into a strong ensemble\n\n\nKey insight: Rather than averaging many independent predictions, we build an additive model of trees",
    "crumbs": [
      "Unit 3: Supervised Learning",
      "Lecture Slides: Unit 3.3: Tree-based Methods"
    ]
  },
  {
    "objectID": "unit-3/lec-3-3-slides.html#gradient-boosting-algorithm",
    "href": "unit-3/lec-3-3-slides.html#gradient-boosting-algorithm",
    "title": "Tree-based Methods",
    "section": "Gradient Boosting Algorithm",
    "text": "Gradient Boosting Algorithm\nFor regression (simplified):\n\nInitialize \\(\\hat{f}(x) = 0\\)\nFor \\(b = 1\\) to \\(B\\):\n\nCompute residuals \\(r_i = y_i - \\hat{f}(x_i)\\)\nFit a small tree to the residuals, get predictions \\(\\hat{h}_b(x)\\)\nUpdate model: \\(\\hat{f}(x) = \\hat{f}(x) + \\lambda \\hat{h}_b(x)\\)\nWhere \\(\\lambda\\) is the learning rate (shrinkage parameter)",
    "crumbs": [
      "Unit 3: Supervised Learning",
      "Lecture Slides: Unit 3.3: Tree-based Methods"
    ]
  },
  {
    "objectID": "unit-3/lec-3-3-slides.html#boosting-tuning-parameters",
    "href": "unit-3/lec-3-3-slides.html#boosting-tuning-parameters",
    "title": "Tree-based Methods",
    "section": "Boosting Tuning Parameters",
    "text": "Boosting Tuning Parameters\nThree key parameters control the model:\n\nNumber of trees \\(B\\): More trees can improve performance but may lead to overfitting\nTree depth \\(d\\): Controls complexity of individual trees (often shallow, 1-6 nodes)\nLearning rate \\(\\lambda\\): How quickly model adapts to errors (smaller values need more trees)\n\n\nAs with other regularization approaches, cross-validation helps select optimal parameters",
    "crumbs": [
      "Unit 3: Supervised Learning",
      "Lecture Slides: Unit 3.3: Tree-based Methods"
    ]
  },
  {
    "objectID": "unit-3/lec-3-3-slides.html#gradient-boosting-vs.-random-forests",
    "href": "unit-3/lec-3-3-slides.html#gradient-boosting-vs.-random-forests",
    "title": "Tree-based Methods",
    "section": "Gradient Boosting vs. Random Forests",
    "text": "Gradient Boosting vs. Random Forests\n\nRandom Forests\n\nTrees built independently\nTry to average out errors\nLess prone to overfitting\nEasier to tune\n\nGradient Boosting - Trees built sequentially\n\nTry to correct previous errors\nMore prone to overfitting\nOften higher accuracy, if tuned properly",
    "crumbs": [
      "Unit 3: Supervised Learning",
      "Lecture Slides: Unit 3.3: Tree-based Methods"
    ]
  },
  {
    "objectID": "unit-3/lec-3-3-slides.html#xgboost-state-of-the-art-boosting",
    "href": "unit-3/lec-3-3-slides.html#xgboost-state-of-the-art-boosting",
    "title": "Tree-based Methods",
    "section": "XGBoost: State-of-the-Art Boosting",
    "text": "XGBoost: State-of-the-Art Boosting\n\neXtreme Gradient Boosting (XGBoost) is a highly optimized boosting implementation\nRegularization term in objective function\nEfficient handling of sparse data\nParallel processing\nOften wins machine learning competitions\n\n\nXGBoost Objective:\n\\[\\text{Obj} = \\sum_{i=1}^n l(y_i, \\hat{y}_i) + \\sum_{k=1}^K \\Omega(f_k)\\]\nWhere \\(\\Omega(f) = \\gamma T + \\frac{1}{2}\\lambda ||\\omega||^2\\) penalizes complex trees",
    "crumbs": [
      "Unit 3: Supervised Learning",
      "Lecture Slides: Unit 3.3: Tree-based Methods"
    ]
  },
  {
    "objectID": "unit-3/lec-3-3-slides.html#applications-in-health-services-research",
    "href": "unit-3/lec-3-3-slides.html#applications-in-health-services-research",
    "title": "Tree-based Methods",
    "section": "Applications in Health Services Research",
    "text": "Applications in Health Services Research\n\nPredicting hospital readmissions: Capturing complex interactions between comorbidities\nIdentifying high-risk patients: Finding non-linear patterns in health indicators\nClaims data analysis: Handling mixed data types from administrative datasets\nMedical imaging: Feature extraction and classification\nTreatment effect heterogeneity: Identifying subgroups with different responses to interventions",
    "crumbs": [
      "Unit 3: Supervised Learning",
      "Lecture Slides: Unit 3.3: Tree-based Methods"
    ]
  },
  {
    "objectID": "unit-3/lec-3-3-slides.html#tuning-tree-based-methods",
    "href": "unit-3/lec-3-3-slides.html#tuning-tree-based-methods",
    "title": "Tree-based Methods",
    "section": "Tuning Tree-Based Methods",
    "text": "Tuning Tree-Based Methods\n\nSingle trees: Prune using cross-validation to select complexity parameter\nBagging: Typically only need to set number of trees (usually 100-500)\nRandom Forests: Number of trees and mtry (predictors per split)\nBoosting: Number of trees, learning rate, tree depth\n\n\nRemember: Cross-validation is essential for all these models!",
    "crumbs": [
      "Unit 3: Supervised Learning",
      "Lecture Slides: Unit 3.3: Tree-based Methods"
    ]
  },
  {
    "objectID": "unit-3/lec-3-3-slides.html#key-takeaways",
    "href": "unit-3/lec-3-3-slides.html#key-takeaways",
    "title": "Tree-based Methods",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\n\nTree-based methods offer flexible, non-parametric modeling\nSingle trees prioritize interpretability\nEnsemble methods dramatically improve predictive performance\nRandom Forests: Easy to tune, robust performance\nBoosting: Often highest accuracy, but requires careful tuning\nTree ensembles provide variable importance and partial dependence for interpretation",
    "crumbs": [
      "Unit 3: Supervised Learning",
      "Lecture Slides: Unit 3.3: Tree-based Methods"
    ]
  },
  {
    "objectID": "unit-3/lec-3-3-slides.html#next-steps",
    "href": "unit-3/lec-3-3-slides.html#next-steps",
    "title": "Tree-based Methods",
    "section": "Next Steps",
    "text": "Next Steps\n\nLab exercise: Implementing tree-based methods in R\nReadings: Chapter 8 in “Introduction to Statistical Learning”\nComing up: Causal trees and forests - using tree-based methods for causal inference!",
    "crumbs": [
      "Unit 3: Supervised Learning",
      "Lecture Slides: Unit 3.3: Tree-based Methods"
    ]
  },
  {
    "objectID": "computing.html",
    "href": "computing.html",
    "title": "Computing",
    "section": "",
    "text": "This page is a guide for “best practices” for writing code and working with data. It is a work in progress, so please let Sean or the TA know if you have any suggestions for improvement.\n\n\n\n\n\n\nThis is meant as a general quide. For specific instructions on completing labs and assignments, please refer to the lab and assignment instructions.\n\n\n\n\n\n\nIPA Data Science, Engineering, and Technology Handbook\nCoding for Economists\n\n\n\n\n\n\nA package manager helps to standardize how you install and update software on your computer. Generally, you want to use a package manager to install any programs that are used globally on your computer. By “globally”, we mean that it is a program that is used across many projects and computing environments.\n\n\nIf using a Windows computer with Windows 10 or later, use the Windows Package Manager, winget.\nExample:\n# Install a single program (e.g. GitHub for command line)\nwinget install GitHub.cli\n\n\n\nIf using a macOS computer, use Homebrew.\nExample:\n# Install a single program (e.g. GitHub for command line)\nbrew install gh\n\n\n\n\n# Windows Install r\nwinget install --id R-project.R\n\n# macOS Install R\nbrew install --cask r\n\n\n\n\n\n\n\nScared of R? Not to worry! You’ll pick it up as we go - I promise! During class we will go through code slowly at first and spend some time explaining what each line of code does. You’ll be R literate in no time!\nIf you do want to learn a bit of R on your own, here are some excellent resources:\n\nR for Data Science by Hadley Wickham\n\n\n\nNope, you cannot turn in assignments in Stata, sorry!  Why? It’s not just that it’s much harder to run the class when we’re using different software (it is!), but there are several reasons that I strongly believe it is best for you to become comfortable with R:\n\nR is FREEEEE\nR has an open source ecosystem. This means\n\nImmediate access to cutting-edge statistical methods through community contributions\nExtensive library of packages for specialized analyses\nIntegration capabilities with other tools like Git, Python, and database systems\n\nR is just better for advanced data analysis\n\nAbility to handle multiple datasets simultaneously\nSuperior graphical capabilities\nBetter tools for web scraping, JSON parsing, and database querying\n\nIn industry, evidence that R has 5x greater demand than Stata on the job market\n\n\n\n\n\n\n\nLearning Strategy\n\n\n\nRather than directly mapping Stata commands to R, it’s more effective to first understand R’s fundamental concepts and then gradually build up to specific tasks like data management and regression models.\n\n\nHere are some excellent tools for Stata people to learn R:\n\nStata2R Website\nWorld Bank DIME R for Advanced Stata Users Workshop",
    "crumbs": [
      "Resources",
      "Computing"
    ]
  },
  {
    "objectID": "computing.html#useful-guides",
    "href": "computing.html#useful-guides",
    "title": "Computing",
    "section": "",
    "text": "IPA Data Science, Engineering, and Technology Handbook\nCoding for Economists",
    "crumbs": [
      "Resources",
      "Computing"
    ]
  },
  {
    "objectID": "computing.html#computer-setup",
    "href": "computing.html#computer-setup",
    "title": "Computing",
    "section": "",
    "text": "A package manager helps to standardize how you install and update software on your computer. Generally, you want to use a package manager to install any programs that are used globally on your computer. By “globally”, we mean that it is a program that is used across many projects and computing environments.\n\n\nIf using a Windows computer with Windows 10 or later, use the Windows Package Manager, winget.\nExample:\n# Install a single program (e.g. GitHub for command line)\nwinget install GitHub.cli\n\n\n\nIf using a macOS computer, use Homebrew.\nExample:\n# Install a single program (e.g. GitHub for command line)\nbrew install gh\n\n\n\n\n# Windows Install r\nwinget install --id R-project.R\n\n# macOS Install R\nbrew install --cask r",
    "crumbs": [
      "Resources",
      "Computing"
    ]
  },
  {
    "objectID": "computing.html#using-r",
    "href": "computing.html#using-r",
    "title": "Computing",
    "section": "",
    "text": "Scared of R? Not to worry! You’ll pick it up as we go - I promise! During class we will go through code slowly at first and spend some time explaining what each line of code does. You’ll be R literate in no time!\nIf you do want to learn a bit of R on your own, here are some excellent resources:\n\nR for Data Science by Hadley Wickham\n\n\n\nNope, you cannot turn in assignments in Stata, sorry!  Why? It’s not just that it’s much harder to run the class when we’re using different software (it is!), but there are several reasons that I strongly believe it is best for you to become comfortable with R:\n\nR is FREEEEE\nR has an open source ecosystem. This means\n\nImmediate access to cutting-edge statistical methods through community contributions\nExtensive library of packages for specialized analyses\nIntegration capabilities with other tools like Git, Python, and database systems\n\nR is just better for advanced data analysis\n\nAbility to handle multiple datasets simultaneously\nSuperior graphical capabilities\nBetter tools for web scraping, JSON parsing, and database querying\n\nIn industry, evidence that R has 5x greater demand than Stata on the job market\n\n\n\n\n\n\n\nLearning Strategy\n\n\n\nRather than directly mapping Stata commands to R, it’s more effective to first understand R’s fundamental concepts and then gradually build up to specific tasks like data management and regression models.\n\n\nHere are some excellent tools for Stata people to learn R:\n\nStata2R Website\nWorld Bank DIME R for Advanced Stata Users Workshop",
    "crumbs": [
      "Resources",
      "Computing"
    ]
  },
  {
    "objectID": "unit-6/unit-6.html",
    "href": "unit-6/unit-6.html",
    "title": "Unit 6: Generalizability and Scalability",
    "section": "",
    "text": "Discusses the concepts of generalizability (external validity) and scalability in translating research findings into policy and practice.\nCovers common challenges like “voltage drop” and List’s Five Vital Signs for diagnosing scaling failures.\nExplores strategies for designing research for scale (e.g., backward induction, Option C).\nPresents frameworks (e.g., Glennerster) for assessing the applicability of existing evidence to new contexts.\n\n\n\n\nRead:\n\nList, J. A. (2024). Optimally generate policy-based evidence before scaling. Nature, 626(7998), 491–499. https://doi.org/10.1038/s41586-023-06972-y\nReview: Framework for Generalizability (Glennerster, based on J-PAL materials discussed in lecture)\n\nListen: Internal Validity Podcast\n\n\n\n\n\nLecture: From Petri Dish to Policy: Generating and Applying Scalable Experimental Insights",
    "crumbs": [
      "Unit 6: Generalizability & Scalability",
      "Unit 6: Generalizability & Scalability"
    ]
  },
  {
    "objectID": "unit-6/unit-6.html#unit-overview",
    "href": "unit-6/unit-6.html#unit-overview",
    "title": "Unit 6: Generalizability and Scalability",
    "section": "",
    "text": "Discusses the concepts of generalizability (external validity) and scalability in translating research findings into policy and practice.\nCovers common challenges like “voltage drop” and List’s Five Vital Signs for diagnosing scaling failures.\nExplores strategies for designing research for scale (e.g., backward induction, Option C).\nPresents frameworks (e.g., Glennerster) for assessing the applicability of existing evidence to new contexts.\n\n\n\n\nRead:\n\nList, J. A. (2024). Optimally generate policy-based evidence before scaling. Nature, 626(7998), 491–499. https://doi.org/10.1038/s41586-023-06972-y\nReview: Framework for Generalizability (Glennerster, based on J-PAL materials discussed in lecture)\n\nListen: Internal Validity Podcast\n\n\n\n\n\nLecture: From Petri Dish to Policy: Generating and Applying Scalable Experimental Insights",
    "crumbs": [
      "Unit 6: Generalizability & Scalability",
      "Unit 6: Generalizability & Scalability"
    ]
  },
  {
    "objectID": "unit-1/HPM883 introductory analysis.html",
    "href": "unit-1/HPM883 introductory analysis.html",
    "title": "HPM883 introductory survey analysis",
    "section": "",
    "text": "install.packages(c(\"dplyr\", \"skimr\", \"cluster\", \"tidyr\", \"ggplot2\", \"knitr\", \"Hmisc\"))\n\nlibrary(dplyr)\nlibrary(skimr)\nlibrary(cluster)\nlibrary(tidyr)\nlibrary(ggplot2)\nlibrary(knitr)\nlibrary(Hmisc)"
  },
  {
    "objectID": "unit-1/HPM883 introductory analysis.html#calculate-borda-count-scores-for-q12how-do-you-like-to-learn",
    "href": "unit-1/HPM883 introductory analysis.html#calculate-borda-count-scores-for-q12how-do-you-like-to-learn",
    "title": "HPM883 introductory survey analysis",
    "section": "Calculate Borda count scores for Q12:How do you like to learn?",
    "text": "Calculate Borda count scores for Q12:How do you like to learn?\n\n### Creat data frame of ranking questions\nq12ranking_df &lt;- data.frame(introsurvey_data$Q1, introsurvey_data$Q12_1, introsurvey_data$Q12_2, introsurvey_data$Q12_3, introsurvey_data$Q12_4, introsurvey_data$Q12_5, introsurvey_data$Q12_6)\n\n### label ranking questions\nlabel(q12ranking_df$introsurvey_data.Q1) &lt;- \"Full Name\"\nlabel(q12ranking_df$introsurvey_data.Q12_1,) &lt;- \"How do you like to learn: Hands-on guided coding practice\"\nlabel(q12ranking_df$introsurvey_data.Q12_2) &lt;- \"How do you like to learn: Group discussions\"\nlabel(q12ranking_df$introsurvey_data.Q12_3) &lt;- \"How do you like to learn: Visual explanations/graphs\"\nlabel(q12ranking_df$introsurvey_data.Q12_4) &lt;- \"How do you like to learn: Independent reading\"\nlabel(q12ranking_df$introsurvey_data.Q12_5) &lt;- \"How do you like to learn: Independent exercises\"\nlabel(q12ranking_df$introsurvey_data.Q12_6) &lt;- \"How do you like to learn: Sleeping with a textbook under my pillow\"\n\n### Function to calculate Borda count:\nq12calculate_borda &lt;- function(q12rankings_df) {\n### Get number of candidates (excluding voter column if present)\nq12n_candidates &lt;- ncol(q12rankings_df)\nif(\"Q1\" %in% colnames(q12rankings_df)) {\n  q12n_candidates &lt;- q12n_candidates - 1\n  }\n  \n### Initialize scores dictionary\nq12scores &lt;- rep(0, q12n_candidates)\nnames(q12scores) &lt;- colnames(q12rankings_df)[!colnames(q12rankings_df) %in% \"Q1\"]\n  \n### Calculate points for each ranking\nfor(i in 1:nrow(q12rankings_df)) {\n  q12ranks &lt;- as.numeric(q12rankings_df[i, !colnames(q12rankings_df) %in% \"Q1\"])\n  \n### Assign points: n-rank points for each position\n  q12points &lt;- q12n_candidates - q12ranks\n  q12scores &lt;- q12scores + q12points\n  }\n  \n### Create results dataframe\n  q12_results &lt;- data.frame(\n    Q12_Candidate = names(q12scores),\n    Q12_Score = q12scores\n  )\n  \n### Sort by score in descending order\n  q12_results &lt;- q12_results[order(-q12_results$Q12_Score),]\n  \n  return(q12_results)\n}\n\n### Calculate Borda count scores\nq12_results &lt;- q12calculate_borda(q12ranking_df)\n\n### Rename candidates\nq12_results$Q12_Candidate[q12_results$Q12_Candidate == \"introsurvey_data.Q1\"] &lt;- \"Full Name\"\nq12_results$Q12_Candidate[q12_results$Q12_Candidate == \"introsurvey_data.Q12_1\"] &lt;- \"How do you like to learn: Hands-on guided coding practice\"\nq12_results$Q12_Candidate[q12_results$Q12_Candidate == \"introsurvey_data.Q12_2\"] &lt;- \"How do you like to learn: Group discussions\"\nq12_results$Q12_Candidate[q12_results$Q12_Candidate == \"introsurvey_data.Q12_3\"] &lt;- \"How do you like to learn: Visual explanations/graphs\"\nq12_results$Q12_Candidate[q12_results$Q12_Candidate == \"introsurvey_data.Q12_4\"] &lt;- \"How do you like to learn: Independent reading\"\nq12_results$Q12_Candidate[q12_results$Q12_Candidate == \"introsurvey_data.Q12_5\"] &lt;- \"How do you like to learn: Independent exercises\"\nq12_results$Q12_Candidate[q12_results$Q12_Candidate == \"introsurvey_data.Q12_6\"] &lt;- \"How do you like to learn: Sleeping with a textbook under my pillow\"\n\n### View Borda count scores\nprint(q12_results)"
  },
  {
    "objectID": "unit-1/HPM883 introductory analysis.html#calculate-borda-count-scores-for-q19-please-rank-the-following-topics-in-order-of-preference",
    "href": "unit-1/HPM883 introductory analysis.html#calculate-borda-count-scores-for-q19-please-rank-the-following-topics-in-order-of-preference",
    "title": "HPM883 introductory survey analysis",
    "section": "Calculate Borda count scores for Q19: Please rank the following topics in order of preference",
    "text": "Calculate Borda count scores for Q19: Please rank the following topics in order of preference\n\n### Creat data frame of ranking questions\nq19ranking_df &lt;- data.frame(introsurvey_data$Q1, introsurvey_data$Q19_1, introsurvey_data$Q19_2, introsurvey_data$Q19_3, introsurvey_data$Q19_4, introsurvey_data$Q19_5, introsurvey_data$Q19_6, introsurvey_data$Q19_7, introsurvey_data$Q19_8, introsurvey_data$Q19_9)\n\n### label ranking questions\nlabel(q19ranking_df$introsurvey_data.Q1) &lt;- \"Full Name\"\nlabel(q19ranking_df$introsurvey_data.Q19_1,) &lt;- \"Topics in order of preference: Policy Learning\"\nlabel(q19ranking_df$introsurvey_data.Q19_2) &lt;- \"Topics in order of preference: Other\"\nlabel(q19ranking_df$introsurvey_data.Q19_3) &lt;- \"Topics in order of preference: Text-as-data\"\nlabel(q19ranking_df$introsurvey_data.Q19_4) &lt;- \"Topics in order of preference: Other\"\nlabel(q19ranking_df$introsurvey_data.Q19_5) &lt;- \"Topics in order of preference: Surrogate Indexes\"\nlabel(q19ranking_df$introsurvey_data.Q19_6) &lt;- \"Topics in order of preference: Double/De-biased Machine Learning\"\nlabel(q19ranking_df$introsurvey_data.Q19_7) &lt;- \"Topics in order of preference: Adaptive Experimental Design\"\nlabel(q19ranking_df$introsurvey_data.Q19_8) &lt;- \"Topics in order of preference: Embeddings\"\nlabel(q19ranking_df$introsurvey_data.Q19_9) &lt;- \"Topics in order of preference: Other\"\n\n### Drop the student who did not fill this question\nq19ranking_df &lt;- q19ranking_df %&gt;% filter(q19ranking_df$introsurvey_data.Q1 != \"Christian Osei\")\nq19ranking_df &lt;- q19ranking_df %&gt;% filter(q19ranking_df$introsurvey_data.Q1 != \"Yeshaben Patel\")\n\n### Function to calculate Borda count:\nq19calculate_borda &lt;- function(q19rankings_df) {\n### Get number of candidates (excluding voter column if present)\nq19n_candidates &lt;- ncol(q19rankings_df)\nif(\"Q1\" %in% colnames(q19rankings_df)) {\n  q19n_candidates &lt;- q19n_candidates - 1\n  }\n  \n### Initialize scores dictionary\nq19scores &lt;- rep(0, q19n_candidates)\nnames(q19scores) &lt;- colnames(q19rankings_df)[!colnames(q19rankings_df) %in% \"Q1\"]\n  \n### Calculate points for each ranking\nfor(i in 1:nrow(q19rankings_df)) {\n  q19ranks &lt;- as.numeric(q19rankings_df[i, !colnames(q19rankings_df) %in% \"Q1\"])\n  \n### Assign points: n-rank points for each position\n  q19points &lt;- q19n_candidates - q19ranks\n  q19scores &lt;- q19scores + q19points\n  }\n  \n### Create results dataframe\n  q19_results &lt;- data.frame(\n    Q19_Candidate = names(q19scores),\n    Q19_Score = q19scores\n  )\n  \n### Sort by score in descending order\n  q19_results &lt;- q19_results[order(-q19_results$Q19_Score),]\n  \n  return(q19_results)\n}\n\n### Calculate Borda count scores\nq19_results &lt;- q19calculate_borda(q19ranking_df)\n\n### Rename candidates\nq19_results$Q19_Candidate[q19_results$Q19_Candidate == \"introsurvey_data.Q1\"] &lt;- \"Full Name\"\nq19_results$Q19_Candidate[q19_results$Q19_Candidate == \"introsurvey_data.Q19_1\"] &lt;- \"Topics in order of preference: Policy Learning\"\nq19_results$Q19_Candidate[q19_results$Q19_Candidate == \"introsurvey_data.Q19_2\"] &lt;- \"Topics in order of preference: Other\"\nq19_results$Q19_Candidate[q19_results$Q19_Candidate == \"introsurvey_data.Q19_3\"] &lt;- \"Topics in order of preference: Text-as-data\"\nq19_results$Q19_Candidate[q19_results$Q19_Candidate == \"introsurvey_data.Q19_4\"] &lt;- \"Topics in order of preference: Other\"\nq19_results$Q19_Candidate[q19_results$Q19_Candidate == \"introsurvey_data.Q19_5\"] &lt;- \"Topics in order of preference: Surrogate Indexes\"\nq19_results$Q19_Candidate[q19_results$Q19_Candidate == \"introsurvey_data.Q19_6\"] &lt;- \"Topics in order of preference: Double/De-biased Machine Learning\"\nq19_results$Q19_Candidate[q19_results$Q19_Candidate == \"introsurvey_data.Q19_7\"] &lt;- \"Topics in order of preference: Adaptive Experimental Design\"\nq19_results$Q19_Candidate[q19_results$Q19_Candidate == \"introsurvey_data.Q19_8\"] &lt;- \"Topics in order of preference: Embeddings\"\nq19_results$Q19_Candidate[q19_results$Q19_Candidate == \"introsurvey_data.Q19_9\"] &lt;- \"Topics in order of preference: Other\"\n\n### View Borda count scores\nprint(q19_results)"
  },
  {
    "objectID": "unit-1/lec-1-1.html",
    "href": "unit-1/lec-1-1.html",
    "title": "Unit 1: Internal Validity",
    "section": "",
    "text": "Internal validity is the cornerstone of experimental design, crucial for establishing a causal relationship between treatment and outcome. It ensures that the observed effects in an experiment can be confidently attributed to the treatment rather than other confounding factors. This lecture explores the core principles and challenges of internal validity, including the role of randomization, exclusion restrictions, and the assignment mechanism. We also discuss practical applications and the complexities of ensuring compliance and observability in experimental settings."
  },
  {
    "objectID": "unit-1/lec-1-1.html#scenario-1",
    "href": "unit-1/lec-1-1.html#scenario-1",
    "title": "Unit 1: Internal Validity",
    "section": "Scenario 1",
    "text": "Scenario 1\nSuppose you are considering whether a new diet is linked to lower risk of inflammatory arthritis.\nYou observe that in a given sample: - A small fraction of individuals on the diet have inflammatory arthritis. - A large fraction of individuals not on the diet have inflammatory arthritis.\nBased on this, you recommend that everyone pursue this new diet, but rates of inflammatory arthritis are unaffected.\nWhat happened?"
  },
  {
    "objectID": "unit-1/lec-1-1.html#scenario-2",
    "href": "unit-1/lec-1-1.html#scenario-2",
    "title": "Unit 1: Internal Validity",
    "section": "Scenario 2",
    "text": "Scenario 2\nConsider a policy designed to reduce emergency room visits by offering free preventive care check-ups. Policymakers observed that communities with higher uptake of preventive care have fewer emergency visits. They implement the policy nationwide, expecting a significant reduction in emergency room visits. However, the policy shows no measurable impact.\nWhat happened?"
  },
  {
    "objectID": "unit-1/lec-1-1.html#explanation",
    "href": "unit-1/lec-1-1.html#explanation",
    "title": "Unit 1: Internal Validity",
    "section": "Explanation",
    "text": "Explanation\nIn each case, you were unable to see what would have happened to each individual if the alternative action had been applied.\nIn scenario 1, it could be that individuals who chose the diet may already have healthier lifestyles, including better exercise and reduced stress levels, which are known to reduce the risk of inflammatory arthritis. The observed differences may be due to these factors rather than the diet itself. Without randomization or controlling for confounding factors, you cannot attribute causality to the diet. The association observed may not reflect the causal effect of the diet.\nIn scenario 2, the observed association between preventive care and lower emergency room visits may reflect that communities with higher uptake of preventive care might already have better healthcare access, socioeconomic conditions, or health awareness—all factors that independently reduce emergency visits. Implementing the policy broadly, without considering these confounders, does not account for the variation in baseline conditions.\nThe lack of this information is what prevents inference about causation from association."
  },
  {
    "objectID": "unit-1/lec-1-1.html#randomization",
    "href": "unit-1/lec-1-1.html#randomization",
    "title": "Unit 1: Internal Validity",
    "section": "Randomization",
    "text": "Randomization\nAs we saw, the key source of bias arises from the assignment mechanism, or why a unit is assigned to treatment or control. Endogeneity can arise from units being assigned logically or based on percieved benefit, which is a function of their characteristics. There are two basic approaches to dealing with endogeneity:\n\nModel-based methods: Model the selection bias and then remove it mechanically\nDesign-based methods: Use the design of the experiment and randomization to remove selection bias\n\nIn randomized experiments, the assignment mechanism is controlled, and therefore known, by the researcher. This is a unique and key difference from naturally-occuring data where the assignment mechanisms is neither controlled nor typically known. The experimental apporach is often referred to as the “gold standard” for estimating casual effects because randomization can be made to ensure that the assignment is independent of individual characteristics, eliminating bias.\nTo satisfy, the basic restrictions on the assignment mechanism for randomized experiments are:\n\nThree Conditions for a Valid Assignment Mechanism\n\nNon-zero probability: Each individual has a positive probability of being assigned to treatment or control.\nIndividualism: Assignments are independent across individuals and do not depend on others’ potential outcomes.\nUnconfoundedness: Treatment assignment is orthogonal to potential outcomes.\n\nThese conditions ensure that observed differences between groups are attributable to the treatment rather than pre-existing differences."
  },
  {
    "objectID": "unit-1/lec-1-1.html#footnotes",
    "href": "unit-1/lec-1-1.html#footnotes",
    "title": "Unit 1: Internal Validity",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis example is adapted from Scott Cunninham’s example in the Mixtape book.↩︎"
  },
  {
    "objectID": "templates.html",
    "href": "templates.html",
    "title": "Templates & Guides",
    "section": "",
    "text": "Pre-Analysis Plan Template\nRCT Analysis Template Repository (GitHub)\nJ-Pal Sample Size and Power Repository",
    "crumbs": [
      "Resources",
      "Templates"
    ]
  },
  {
    "objectID": "templates.html#templates",
    "href": "templates.html#templates",
    "title": "Templates & Guides",
    "section": "",
    "text": "Pre-Analysis Plan Template\nRCT Analysis Template Repository (GitHub)\nJ-Pal Sample Size and Power Repository",
    "crumbs": [
      "Resources",
      "Templates"
    ]
  },
  {
    "objectID": "templates.html#guides",
    "href": "templates.html#guides",
    "title": "Templates & Guides",
    "section": "Guides",
    "text": "Guides\nPeer Review Guide",
    "crumbs": [
      "Resources",
      "Templates"
    ]
  },
  {
    "objectID": "lab.html",
    "href": "lab.html",
    "title": "Labs",
    "section": "",
    "text": "More labs to be added as the semester progresses.\n\n\n\n\n\n\n\n\n\nNo.\n\n\nTitle\n\n\nDue date\n\n\n\n\n\n\nLab 1\n\n\nThe Hospital of Uncertain Outcomes\n\n\nInternal Validity, Potential Outcomes\n\n\n\n\nLab 1 Solutions\n\n\nThe Hospital of Uncertain Outcomes\n\n\nInternal Validity, Potential Outcomes\n\n\n\n\nLab 2\n\n\nPower by Simulation\n\n\nConducting power calculations by simulation, first without clusters and then with clusters.\n\n\n\n\nLab 2: Power by Simulation\n\n\n(Possible)Solutions\n\n\nConducting power calculations by simulation, first without clusters and then with clusters.\n\n\n\n\nLab 3\n\n\nOptimizing HBV Vaccination Strategies\n\n\nOptimal experimental design and randomization techniques for vaccine interventions\n\n\n\n\nLab Assignment: Peer Review of Pre-Analysis Plan\n\n\n \n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "unit-0/lec-0.html#course-summary",
    "href": "unit-0/lec-0.html#course-summary",
    "title": "Course Introduction",
    "section": "Course Summary",
    "text": "Course Summary\nExperiments and Machine Learning for Health Services Research\nPrimary Goal: Equip you with tools to design, analyze, and interpret field experiments and apply machine learning to health services research.\n\nKey Focus Areas:\nField experiments for causal inference.\nMachine learning for prediction and causal analysis.\nApplications:\nHealth policy evaluations, resource allocation, and improving healthcare decision-making."
  },
  {
    "objectID": "unit-0/lec-0.html#about-me-dr.-sean-sylvia",
    "href": "unit-0/lec-0.html#about-me-dr.-sean-sylvia",
    "title": "Course Introduction",
    "section": "About Me: Dr. Sean Sylvia",
    "text": "About Me: Dr. Sean Sylvia\n\n\nResearch: Designing & evaluating innovative approaches to improve health service delivery, globally.\n\nExperimental methods, machine learning, economics of digital health\n\nUNC Health Policy & Management (7 years)\n\nCarolina Population Center\nSheps Center for Health Services Research\n\nPreviously: Renmin University of China, World Bank\nTraining: U Maryland (PhD), Stanford (pre-doc)"
  },
  {
    "objectID": "unit-0/lec-0.html#proud-randomista",
    "href": "unit-0/lec-0.html#proud-randomista",
    "title": "Course Introduction",
    "section": "Proud “Randomista”",
    "text": "Proud “Randomista”\n\n\n\nExtensive fieldwork using randomized controlled trials (RCTs)\nStarting in 2006, I’ve worked with colleagues on ~20 RCTs in China, Africa, and North Carolina.\n\n[Hot Tip: Collaborate. Collaborate. Collaborate.]\n\nKey projects include:\n\nField experiments in rural China on primary healthcare\nDesign and evaluation of community health worker interventions for early childhood health and development\nBehavioral nudges to recruit providers to MAT training for Opiod Use Disorder in North Carolina\n\n\n\nSCROLL DOWN TO READ Nobel Prize in Economics 2019 was awarded jointly to Abhijit Banerjee, Esther Duflo and Michael Kremer “for their experimental approach to alleviating global poverty”"
  },
  {
    "objectID": "unit-0/lec-0.html#teaching-assistant-yumeng-du",
    "href": "unit-0/lec-0.html#teaching-assistant-yumeng-du",
    "title": "Course Introduction",
    "section": "Teaching Assistant: Yumeng Du",
    "text": "Teaching Assistant: Yumeng Du\nPhD Student in Health Policy and Management (Economics Track)\nResearch Interests:\n- Evaluation of digital health programs for underserved populations\nBackground:\n- MSc in Public Health, London School of Hygiene and Tropical Medicine (LSHTM), UK\n- MBBS, Central South University, China\n- Former RA at UNC-China Project,implementing an RCT on rural telemedicine kiosk program\nRole:\n- Available for technical support and office hours\n- Assisting with labs and project feedback"
  },
  {
    "objectID": "unit-0/lec-0.html#experiments-and-machine-learning-for-hsr",
    "href": "unit-0/lec-0.html#experiments-and-machine-learning-for-hsr",
    "title": "Course Introduction",
    "section": "Experiments and Machine Learning for HSR",
    "text": "Experiments and Machine Learning for HSR\n\n\n\nRandomized experiments: the gold standard for testing mechanisms and interpretability.\nMachine Learning: Ideal for useful predictions when interpretability or theory is less critical.\nBridging the Divide:\n\nNew methods apply core ML principles to causal inference.\n\nAs we’ll see, parts of causal inference problems can be recast as prediction problems.\n\nPowerful tools for theory-building and expanding scope of feasible research.\n\n\n\n\n\n\n\n\n\nNote\n\n\nImportant!\nYou’ll hear the term “causal machine learning” in the literature. This is a misnomer!\nMachine Learning is about prediction. Period. It does not magically solve the causal inference problem.\nIt CAN:\n\nHelp strengthen plausibility of existing exclusion restriction in some cases\nAllow us to study treatment effect heterogeneity in new ways"
  },
  {
    "objectID": "unit-0/lec-0.html#reproducibility",
    "href": "unit-0/lec-0.html#reproducibility",
    "title": "Course Introduction",
    "section": "Reproducibility",
    "text": "Reproducibility\nReproducibility is a Fundamental Principle of Science\n\nGood experiments are necessarily reproducible.\n\nClear, well-documented protocols, reproducible data collection, and clear and reproducible analysis.\nThe “reproducibility crisis” in some fields (e.g. psychology). This is actually a good thing! Only possible to confirm results if they are reproducible.\n\nReproducibility and ML:\n\nExploratory analyses, when systematic, help build reliable theories.\nML provides tools for:\n\nConducting reproducible exploratory research.\nIdentifying robust treatment effect heterogeneity."
  },
  {
    "objectID": "unit-0/lec-0.html#causal-inference-as-a-missing-data-problem",
    "href": "unit-0/lec-0.html#causal-inference-as-a-missing-data-problem",
    "title": "Course Introduction",
    "section": "Causal Inference as a Missing Data Problem",
    "text": "Causal Inference as a Missing Data Problem\n\n\n\nThe Fundamental Problem of Causal Inference:\n\nWe cannot observe a unit in multiple states of the world simultaneously.\nRepresented as “missing data” in potential outcomes.\n\nExperimentation Solves under some key assumptions:\n\nSUTVA.\nObservability.\nComplete Compliance.\nStatistical Independence.\n\n\n\n\n\n\nUnit\nTreatment (W)\nY(0)\nY(1)\nObserved Outcome\n\n\n\n\n1\n1\n?\n5\n5\n\n\n2\n0\n3\n?\n3\n\n\n3\n1\n?\n6\n6\n\n\n4\n0\n4\n?\n4\n\n\n\n\nProblem: We can only observe either (Y(0)) or (Y(1)) but not both for any individual."
  },
  {
    "objectID": "unit-0/lec-0.html#randomization-recovers-the-ate-on-average.",
    "href": "unit-0/lec-0.html#randomization-recovers-the-ate-on-average.",
    "title": "Course Introduction",
    "section": "Randomization recovers the ATE on average.",
    "text": "Randomization recovers the ATE on average.\n\nPotential outcomes: \\(Y_i(1)\\) and \\(Y_i(0)\\)\nTreatment indicator: \\(W_i = 1\\) if treated, \\(W_i = 0\\) if control\nObserved outcome: \\(Y_i = W_iY_i(1) + (1 - W_i)Y_i(0)\\)\nAverage Treatment Effect (ATE): \\(\\tau = \\mathbb{E}[Y_i(1) - Y_i(0)]\\)\n\nRandomization ensures that treatment is independent of potential outcomes:\n\\[\nW_i \\perp \\!\\!\\! \\perp (Y_i(0), Y_i(1)),\n\\]\nHence, \\(\\mathbb{E}[Y | W_i = w] = \\mathbb{E}[Y_i(w) | W_i = w] = \\mathbb{E}[Y_i(w)]\\) for all \\(w \\in \\{0, 1\\}\\).\nTherefore, the difference between \\(Y_i(1)\\) and \\(Y_i(0)\\) is the ATE:\n\\[\n\\mathbb{E}[Y_i(1) - Y_i(0)] = \\mathbb{E}[Y_i(1)] - \\mathbb{E}[Y_i(0)] = \\tau.\n\\]"
  },
  {
    "objectID": "unit-0/lec-0.html#experimental-problems-ep1-ep2",
    "href": "unit-0/lec-0.html#experimental-problems-ep1-ep2",
    "title": "Course Introduction",
    "section": "Experimental Problems: EP1 & EP2",
    "text": "Experimental Problems: EP1 & EP2\nEP1: Internal Validity\n\nThe Effects of Causes (Internal Validity):\n\nDo assumptions in the potential outcomes framework hold?\n\nThe Causes of Effects (Mediators and Moderators):\n\nWhat drives the effects we see?\n\n\nEP2: External Validity\n\nGeneralizability: Would it work in different settings?\n\nPeople / populations\nContexts\n\nScalability: Can it scale to create meaningful impact?"
  },
  {
    "objectID": "unit-0/lec-0.html#data-generation-and-modeling-for-causal-inference",
    "href": "unit-0/lec-0.html#data-generation-and-modeling-for-causal-inference",
    "title": "Course Introduction",
    "section": "Data Generation and Modeling for Causal Inference",
    "text": "Data Generation and Modeling for Causal Inference\n\n\n\nControlled: researcher knows and controls the assignment mechanism\nUncontrolled: researcher assignment mechanism neither controls nor knows the assignment mechanism\n\n\n\n\nSource: List (2009)\n\n\n\n\nLab Experiments: Controlled settings, abstract framing, imposed rules.\nField Experiments:\n\nArtefactual Field Experiment (AFE): Non-standard subject pool.\nFramed Field Experiment (FFE): Adds field-specific context.\nNatural Field Experiment (NFE): Subjects unaware they are in an experiment.\n\nSurvey Experiments:\n\nEmbedded in survey designs."
  },
  {
    "objectID": "unit-0/lec-0.html#criteria-that-define-field-experiments",
    "href": "unit-0/lec-0.html#criteria-that-define-field-experiments",
    "title": "Course Introduction",
    "section": "7 Criteria that define field experiments",
    "text": "7 Criteria that define field experiments\n\nExperimental Subjects: Population and Selection\n\nThe nature of the subject pool\nThe nature of the information that subjects bring to the experimental task\nSelection into the experiment\n\nExperimental Environment\n\nThe nature of the commodity in the experiment\nThe nature of the experimental task\nThe nature of the stakes\nExperimental proclamation"
  },
  {
    "objectID": "unit-0/lec-0.html#choosing-the-right-experiment",
    "href": "unit-0/lec-0.html#choosing-the-right-experiment",
    "title": "Course Introduction",
    "section": "Choosing the Right Experiment",
    "text": "Choosing the Right Experiment\n\nWeigh Costs and Benefits:\n\nBenefits:\n\nEP1: Internal validity.\nEP2: Generalizability and scalability.\n\nCosts:\n\nMonetary\nLogistical\nOpportunity costs."
  },
  {
    "objectID": "unit-0/lec-0.html#units-and-topics",
    "href": "unit-0/lec-0.html#units-and-topics",
    "title": "Course Introduction",
    "section": "Units and Topics",
    "text": "Units and Topics\n\nFoundations of Causal Inference:\n\nMissing data problem and potential outcomes.\nRandomization and the ATE.\n\nExperimental Design:\n\nPower analysis and randomization strategies.\n\nMachine Learning for Causal Inference:\n\nLasso, random forests, and causal forests.\n\nViolations of Internal Validity:\n\nSUTVA, compliance issues, and observability.\n\nScaling and External Validity:\n\nGeneralizability and implementation challenges."
  },
  {
    "objectID": "unit-0/lec-0.html#weekly-ish-format",
    "href": "unit-0/lec-0.html#weekly-ish-format",
    "title": "Course Introduction",
    "section": "Weekly-ish Format",
    "text": "Weekly-ish Format\n\nTwo(-ish) sessions per topic:\n\nLecture: Introduces key concepts and theory.\nLab: Hands-on practice with coding and data analysis.\n\nFlipped(-ish) Classroom Approach\n\nPre-class readings and recorded materials.\nInteractive sessions focused on data exploration and applied learning.\nCollaborative problem-solving in small groups."
  },
  {
    "objectID": "unit-0/lec-0.html#assessments",
    "href": "unit-0/lec-0.html#assessments",
    "title": "Course Introduction",
    "section": "Assessments",
    "text": "Assessments\n\nPre-class Quizzes (10%):\n\nTest your understanding of key concepts\nRequest clarification on specific points\n\nLab Assignments (25%): Practical applications and analysis.\nExams (35%): Evaluate theoretical and practical understanding.\nSemester Project (30%): Research application.\n\nPre-analysis Plan (15%): Develop and document a structured analysis plan.\nFinal Paper (15%): Synthesize course material into a research application.\nDetails on class project to come."
  },
  {
    "objectID": "unit-0/lec-0.html#resources-and-support",
    "href": "unit-0/lec-0.html#resources-and-support",
    "title": "Course Introduction",
    "section": "Resources and Support",
    "text": "Resources and Support\n\nCourse website with materials and updates: Course Website\nWeekly office hours with Dr. Sylvia and Yumeng\nCommunication channels:\n\nSlack: Join the course Slack to ask questions, share resources, and engage with your peers, the instructor, and the TA. (For personal matters, please use e-mail)\nEmail: Please use e-mail only for personal matters. (For anything related to the course material or coding questions, please use Slack.)\n\nGradescope: Submit assignments."
  },
  {
    "objectID": "unit-0/unit-0-foundations-2.html",
    "href": "unit-0/unit-0-foundations-2.html",
    "title": "Unit 0: Foundations",
    "section": "",
    "text": "Guest Lecture by Dr. Dorien Emmers\n\n\n\n\n\n\nClass will be remote-only\n\n\n\nJoin from the comfort of your own home!\nZoom link: https://unc.zoom.us/j/7840211370?omn=97363476255\n\n\n\n\nSkim: Using Community Health Workers to Deliver a Scalable Integrated Parenting Program in Rural China: A Cluster Randomized Controlled Trial\n\nOptional\n\nDownload Replication Data and Code: \nDr. Emmers wrote this analysis code in Stata (Boo! ). Try converting it to R using the stata2r package.\n\n\n\n\n\n\n\n\nPerusall\n\n\n\nPerusall is a free online platform that allows you to collaboratively annotate content with your classmates. Here is a link to the Perusall page for this course: HPM 883. If needed, the class enrollment code is SYLVIA-ZXTWH.\n\nAlthough it is a good way to engage with your classmates around the material, it is not required that you comment on Perusall. If you wish, you can just download the readings directly.\n\n\n\nSubmit: Any questions you have for Dr. Emmers? Submit to the #guest-lecture-questions channel on Slack\n\n\n\n\n\nSlides\nZoom Recording\n\n\n\n\n\n\n\nIntroductory Survey Due by end of day on Jan 15\n\n\n\nIntroductory Class Survey",
    "crumbs": [
      "Unit 0: Foundations",
      "Unit 0.1: A Field Experiment in Health Services Research"
    ]
  },
  {
    "objectID": "unit-0/unit-0-foundations-2.html#overview",
    "href": "unit-0/unit-0-foundations-2.html#overview",
    "title": "Unit 0: Foundations",
    "section": "",
    "text": "Guest Lecture by Dr. Dorien Emmers\n\n\n\n\n\n\nClass will be remote-only\n\n\n\nJoin from the comfort of your own home!\nZoom link: https://unc.zoom.us/j/7840211370?omn=97363476255\n\n\n\n\nSkim: Using Community Health Workers to Deliver a Scalable Integrated Parenting Program in Rural China: A Cluster Randomized Controlled Trial\n\nOptional\n\nDownload Replication Data and Code: \nDr. Emmers wrote this analysis code in Stata (Boo! ). Try converting it to R using the stata2r package.\n\n\n\n\n\n\n\n\nPerusall\n\n\n\nPerusall is a free online platform that allows you to collaboratively annotate content with your classmates. Here is a link to the Perusall page for this course: HPM 883. If needed, the class enrollment code is SYLVIA-ZXTWH.\n\nAlthough it is a good way to engage with your classmates around the material, it is not required that you comment on Perusall. If you wish, you can just download the readings directly.\n\n\n\nSubmit: Any questions you have for Dr. Emmers? Submit to the #guest-lecture-questions channel on Slack\n\n\n\n\n\nSlides\nZoom Recording\n\n\n\n\n\n\n\nIntroductory Survey Due by end of day on Jan 15\n\n\n\nIntroductory Class Survey",
    "crumbs": [
      "Unit 0: Foundations",
      "Unit 0.1: A Field Experiment in Health Services Research"
    ]
  },
  {
    "objectID": "unit-0/prep-1.html",
    "href": "unit-0/prep-1.html",
    "title": "Class Preparation",
    "section": "",
    "text": "In preparation for our upcoming class on [Insert Topic Here], please complete the following activities. These materials will set the foundation for our in-class discussions and activities.\n\n\n\n\n\n[Insert Reading Title and Link Here]\n[Insert Additional Reading Title and Link Here]\n\n\n\n\n\n\n[Insert Video Title and Link Here]\n[Insert Additional Video Title and Link Here]\n\n\n\n\n\nComplete the pre-class quiz on Sakai: [Insert Link Here]\nDeadline: [Insert Date and Time]\n\n\n\n\nConsider the following questions as you review the materials: 1. [Insert Question Here] 2. [Insert Additional Question Here] 3. [Insert Additional Question Here]\n\n\n\n\nPrepare notes summarizing key concepts from the readings and videos. These will be helpful for in-class discussions.\n\nRemember: This course uses a flipped classroom approach. Completing these tasks prior to class is essential for your active participation and understanding during our sessions."
  },
  {
    "objectID": "unit-0/prep-1.html#overview",
    "href": "unit-0/prep-1.html#overview",
    "title": "Class Preparation",
    "section": "",
    "text": "In preparation for our upcoming class on [Insert Topic Here], please complete the following activities. These materials will set the foundation for our in-class discussions and activities."
  },
  {
    "objectID": "unit-0/prep-1.html#required-readings",
    "href": "unit-0/prep-1.html#required-readings",
    "title": "Class Preparation",
    "section": "",
    "text": "[Insert Reading Title and Link Here]\n[Insert Additional Reading Title and Link Here]"
  },
  {
    "objectID": "unit-0/prep-1.html#required-videos",
    "href": "unit-0/prep-1.html#required-videos",
    "title": "Class Preparation",
    "section": "",
    "text": "[Insert Video Title and Link Here]\n[Insert Additional Video Title and Link Here]"
  },
  {
    "objectID": "unit-0/prep-1.html#quiz",
    "href": "unit-0/prep-1.html#quiz",
    "title": "Class Preparation",
    "section": "",
    "text": "Complete the pre-class quiz on Sakai: [Insert Link Here]\nDeadline: [Insert Date and Time]"
  },
  {
    "objectID": "unit-0/prep-1.html#reflection-questions",
    "href": "unit-0/prep-1.html#reflection-questions",
    "title": "Class Preparation",
    "section": "",
    "text": "Consider the following questions as you review the materials: 1. [Insert Question Here] 2. [Insert Additional Question Here] 3. [Insert Additional Question Here]"
  },
  {
    "objectID": "unit-0/prep-1.html#notes",
    "href": "unit-0/prep-1.html#notes",
    "title": "Class Preparation",
    "section": "",
    "text": "Prepare notes summarizing key concepts from the readings and videos. These will be helpful for in-class discussions.\n\nRemember: This course uses a flipped classroom approach. Completing these tasks prior to class is essential for your active participation and understanding during our sessions."
  },
  {
    "objectID": "ex-simulate.html",
    "href": "ex-simulate.html",
    "title": "Simulating Experimental Data with Noncompliance",
    "section": "",
    "text": "In this example, we simulate data for a simple randomized experiment with one treatment and one control group. The randomization is at the individual level. Additionally, we introduce noncompliance, where some individuals assigned to the treatment group do not take the treatment.\n\n\nDefine the parameters for the experiment, such as the number of participants, compliance rate, and treatment effects.\n\n# Set seed for reproducibility\nset.seed(123)\n\n# Parameters\nn &lt;- 500  # Total number of participants\nprob_treatment &lt;- 0.5  # Probability of being assigned to treatment\ncompliance_rate &lt;- 0.8  # Proportion of treated participants who comply\n\n# Treatment effect\nbaseline_mean &lt;- 50  # Mean outcome for control group\ntreatment_effect &lt;- 10  # Additional effect for treated participants\n\n\nSimulate random assignment to treatment and compliance behavior.\n\n# Simulate random assignment\nassignment &lt;- rbinom(n, 1, prob_treatment)  # 1 = assigned to treatment, 0 = control\n\n# Simulate compliance behavior\n# If assigned to treatment, comply with probability `compliance_rate`\ncompliance &lt;- ifelse(assignment == 1, rbinom(n, 1, compliance_rate), 0)  # 1 = complied, 0 = did not comply\n\n\nSimulate the outcome variable based on compliance and treatment assignment.\n\n# Simulate outcomes\noutcome &lt;- baseline_mean + \n  (assignment * compliance * treatment_effect) +  # Effect for those who comply\n  rnorm(n, mean = 0, sd = 5)  # Add random noise\n\n# Combine data into a data frame\nexperiment_data &lt;- data.frame(\n  ID = 1:n,\n  Assignment = assignment,\n  Compliance = compliance,\n  Outcome = outcome\n)\n\n# View the first few rows of the data\nhead(experiment_data)\n\n  ID Assignment Compliance  Outcome\n1  1          0          0 46.99054\n2  2          1          1 55.03151\n3  3          0          0 55.13393\n4  4          1          1 63.75531\n5  5          1          1 52.45417\n6  6          0          0 49.52426\n\n\n\nExport the simulated data to a CSV file for further analysis.\n\n# Export the data to a CSV file\nwrite.csv(experiment_data, \"simulated_experiment_data.csv\", row.names = FALSE)\n\n# Confirm export\ncat(\"Data has been exported to 'simulated_experiment_data.csv'\")\n\nData has been exported to 'simulated_experiment_data.csv'",
    "crumbs": [
      "Semester Project",
      "Data Simulation Example"
    ]
  },
  {
    "objectID": "ex-simulate.html#step-1-setup-and-parameters",
    "href": "ex-simulate.html#step-1-setup-and-parameters",
    "title": "Simulating Experimental Data with Noncompliance",
    "section": "",
    "text": "Define the parameters for the experiment, such as the number of participants, compliance rate, and treatment effects.\n\n# Set seed for reproducibility\nset.seed(123)\n\n# Parameters\nn &lt;- 500  # Total number of participants\nprob_treatment &lt;- 0.5  # Probability of being assigned to treatment\ncompliance_rate &lt;- 0.8  # Proportion of treated participants who comply\n\n# Treatment effect\nbaseline_mean &lt;- 50  # Mean outcome for control group\ntreatment_effect &lt;- 10  # Additional effect for treated participants",
    "crumbs": [
      "Semester Project",
      "Data Simulation Example"
    ]
  },
  {
    "objectID": "ex-simulate.html#step-2-random-assignment-and-compliance",
    "href": "ex-simulate.html#step-2-random-assignment-and-compliance",
    "title": "Simulating Experimental Data with Noncompliance",
    "section": "",
    "text": "Simulate random assignment to treatment and compliance behavior.\n\n# Simulate random assignment\nassignment &lt;- rbinom(n, 1, prob_treatment)  # 1 = assigned to treatment, 0 = control\n\n# Simulate compliance behavior\n# If assigned to treatment, comply with probability `compliance_rate`\ncompliance &lt;- ifelse(assignment == 1, rbinom(n, 1, compliance_rate), 0)  # 1 = complied, 0 = did not comply",
    "crumbs": [
      "Semester Project",
      "Data Simulation Example"
    ]
  },
  {
    "objectID": "ex-simulate.html#step-3-simulate-outcomes",
    "href": "ex-simulate.html#step-3-simulate-outcomes",
    "title": "Simulating Experimental Data with Noncompliance",
    "section": "",
    "text": "Simulate the outcome variable based on compliance and treatment assignment.\n\n# Simulate outcomes\noutcome &lt;- baseline_mean + \n  (assignment * compliance * treatment_effect) +  # Effect for those who comply\n  rnorm(n, mean = 0, sd = 5)  # Add random noise\n\n# Combine data into a data frame\nexperiment_data &lt;- data.frame(\n  ID = 1:n,\n  Assignment = assignment,\n  Compliance = compliance,\n  Outcome = outcome\n)\n\n# View the first few rows of the data\nhead(experiment_data)\n\n  ID Assignment Compliance  Outcome\n1  1          0          0 46.99054\n2  2          1          1 55.03151\n3  3          0          0 55.13393\n4  4          1          1 63.75531\n5  5          1          1 52.45417\n6  6          0          0 49.52426",
    "crumbs": [
      "Semester Project",
      "Data Simulation Example"
    ]
  },
  {
    "objectID": "ex-simulate.html#step-4-export-data-to-csv",
    "href": "ex-simulate.html#step-4-export-data-to-csv",
    "title": "Simulating Experimental Data with Noncompliance",
    "section": "",
    "text": "Export the simulated data to a CSV file for further analysis.\n\n# Export the data to a CSV file\nwrite.csv(experiment_data, \"simulated_experiment_data.csv\", row.names = FALSE)\n\n# Confirm export\ncat(\"Data has been exported to 'simulated_experiment_data.csv'\")\n\nData has been exported to 'simulated_experiment_data.csv'",
    "crumbs": [
      "Semester Project",
      "Data Simulation Example"
    ]
  },
  {
    "objectID": "course-communication.html",
    "href": "course-communication.html",
    "title": "Communication",
    "section": "",
    "text": "Slack will be our primary communication hub. An invitation to the course Slack will be shared on the first day of class. Use this place to ask questions, share resources, and engage with your peers, the instructor, and the TA. (For personal or sensitive matters, please use e-mail)\n\n\n\n\n\n\nImportant:\n\n\n\nSlack will not be continuously monitored by the instructor or TA outside of posted office hours. At other times, students are expected to help/engage with one another to foster a shared learning experience. If a question hasn’t been addressed, the TA or Sean will respond but not immediately or even the same day.\nYou may DM Sean or the TA in Slack (but, again, don’t expect a response immediately or even the same day.)\n\n\n\nThe #announcements channel will be used to share course announcements.\nUse the #help channel as a first stop for general questions about the class or for technical assistance.\nUse the #general channel to discuss lab assignments and discuss/share resources related to course topics.\nUse the #interesting-stuff channel to share relevant articles, podcasts, packages, data sources, and anything else relevant that you think your classmates will benefit from.\n\n\n\n\n\n\n\nDon’t post any sensitive or personal information in the class Slack\n\n\n\nSlack is only for general class discussions, announcements, and collaboration. In particular, please do not share or post any of the following:\n\nIndividual or group grades, GPA, or other academic evaluations or records.\nSocial Security Numbers\nPersonal information about tuition payments, financial aid, or scholarships.\nAny information on accommodations or health-related information.\nAny other sensitive information\n\nFor sensitive information, please use UNC email.",
    "crumbs": [
      "Course Information",
      "Communication"
    ]
  },
  {
    "objectID": "course-communication.html#slack",
    "href": "course-communication.html#slack",
    "title": "Communication",
    "section": "",
    "text": "Slack will be our primary communication hub. An invitation to the course Slack will be shared on the first day of class. Use this place to ask questions, share resources, and engage with your peers, the instructor, and the TA. (For personal or sensitive matters, please use e-mail)\n\n\n\n\n\n\nImportant:\n\n\n\nSlack will not be continuously monitored by the instructor or TA outside of posted office hours. At other times, students are expected to help/engage with one another to foster a shared learning experience. If a question hasn’t been addressed, the TA or Sean will respond but not immediately or even the same day.\nYou may DM Sean or the TA in Slack (but, again, don’t expect a response immediately or even the same day.)\n\n\n\nThe #announcements channel will be used to share course announcements.\nUse the #help channel as a first stop for general questions about the class or for technical assistance.\nUse the #general channel to discuss lab assignments and discuss/share resources related to course topics.\nUse the #interesting-stuff channel to share relevant articles, podcasts, packages, data sources, and anything else relevant that you think your classmates will benefit from.\n\n\n\n\n\n\n\nDon’t post any sensitive or personal information in the class Slack\n\n\n\nSlack is only for general class discussions, announcements, and collaboration. In particular, please do not share or post any of the following:\n\nIndividual or group grades, GPA, or other academic evaluations or records.\nSocial Security Numbers\nPersonal information about tuition payments, financial aid, or scholarships.\nAny information on accommodations or health-related information.\nAny other sensitive information\n\nFor sensitive information, please use UNC email.",
    "crumbs": [
      "Course Information",
      "Communication"
    ]
  },
  {
    "objectID": "course-communication.html#office-hours",
    "href": "course-communication.html#office-hours",
    "title": "Communication",
    "section": "Office Hours",
    "text": "Office Hours\nSean will hold office hours on Wednesdays from 1-2pm. Please book an appointment at least 24 hours ahead of time using this LINK.\nThe TA will hold office hours on…",
    "crumbs": [
      "Course Information",
      "Communication"
    ]
  },
  {
    "objectID": "course-communication.html#email",
    "href": "course-communication.html#email",
    "title": "Communication",
    "section": "Email",
    "text": "Email\nPlease use e-mail only for personal matters. (For anything related to the course material or coding questions, please use Slack.)",
    "crumbs": [
      "Course Information",
      "Communication"
    ]
  },
  {
    "objectID": "labs/lab-5-PeerPAPReview.html",
    "href": "labs/lab-5-PeerPAPReview.html",
    "title": "Lab Assignment: Peer Review of Pre-Analysis Plan",
    "section": "",
    "text": "In this lab assignment, you will conduct a peer review of another student’s Pre-Analysis Plan (PAP) for the “Designing and Analyzing an Experiment in Health Services Research” semester project. The goal of this exercise is to provide constructive feedback to your peer and to critically reflect on effective research design and analysis practices.\nPlease refer to the attached Guide for Peer Review and the provided rubric to structure your feedback clearly and constructively.",
    "crumbs": [
      "Unit 5: Threats to Validity",
      "Lab 5: Peer PAP Review"
    ]
  },
  {
    "objectID": "labs/lab-5-PeerPAPReview.html#introduction",
    "href": "labs/lab-5-PeerPAPReview.html#introduction",
    "title": "Lab Assignment: Peer Review of Pre-Analysis Plan",
    "section": "",
    "text": "In this lab assignment, you will conduct a peer review of another student’s Pre-Analysis Plan (PAP) for the “Designing and Analyzing an Experiment in Health Services Research” semester project. The goal of this exercise is to provide constructive feedback to your peer and to critically reflect on effective research design and analysis practices.\nPlease refer to the attached Guide for Peer Review and the provided rubric to structure your feedback clearly and constructively.",
    "crumbs": [
      "Unit 5: Threats to Validity",
      "Lab 5: Peer PAP Review"
    ]
  },
  {
    "objectID": "labs/lab-5-PeerPAPReview.html#instructions",
    "href": "labs/lab-5-PeerPAPReview.html#instructions",
    "title": "Lab Assignment: Peer Review of Pre-Analysis Plan",
    "section": "Instructions",
    "text": "Instructions\n\nObtain the Peer PAP:\nYou will receive a peer’s Pre-Analysis Plan via a private message in the course Slack channel.\nReview the PAP Carefully:\nCarefully read the document provided by your peer, making notes based on the grading criteria outlined in the rubric and the guidance provided in the Guide for Peer Review.\nAssess the PAP:\nEvaluate your peer’s PAP according to the three main criteria:\n\nClarity and Completeness (40%)\nRigor of Design and Analysis (40%)\nFeasibility and Practicality (20%)\n\nProvide specific, actionable feedback in each area.\nSubmit Your Review:\nWrite your review clearly and respectfully, highlighting strengths and providing constructive suggestions for improvement. Submit your review in Gradescope.",
    "crumbs": [
      "Unit 5: Threats to Validity",
      "Lab 5: Peer PAP Review"
    ]
  },
  {
    "objectID": "labs/lab-5-PeerPAPReview.html#peer-review-submission-template",
    "href": "labs/lab-5-PeerPAPReview.html#peer-review-submission-template",
    "title": "Lab Assignment: Peer Review of Pre-Analysis Plan",
    "section": "Peer Review Submission Template",
    "text": "Peer Review Submission Template\nIn your submission, clearly structure your review using the following headings and prompts. Address each point thoroughly.\nReviewer Name:\nPeer Name (Author of PAP):\nDate of Review:",
    "crumbs": [
      "Unit 5: Threats to Validity",
      "Lab 5: Peer PAP Review"
    ]
  },
  {
    "objectID": "labs/lab-5-PeerPAPReview.html#clarity-and-completeness-40",
    "href": "labs/lab-5-PeerPAPReview.html#clarity-and-completeness-40",
    "title": "Lab Assignment: Peer Review of Pre-Analysis Plan",
    "section": "1. Clarity and Completeness (40%)",
    "text": "1. Clarity and Completeness (40%)\nEvaluate the clarity and completeness of the PAP. Provide comments in each of the following areas:\n\nDescription of Intervention and Control Conditions:\nIs the proposed intervention and control clearly and completely described? What additional details or clarifications would be helpful?\nOutcome Measures Clearly Defined and Justified:\nAre the primary and secondary outcomes clearly defined? Are they justified effectively? Suggest improvements if needed.\nSample and Population Description:\nIs the target population, sampling method, and sample size clearly described and appropriate? What clarifications or adjustments would improve this section?\nData Collection Methods and Timeline:\nAre data collection methods and timelines clearly articulated and realistic? Provide suggestions to improve clarity or completeness.",
    "crumbs": [
      "Unit 5: Threats to Validity",
      "Lab 5: Peer PAP Review"
    ]
  },
  {
    "objectID": "labs/lab-5-PeerPAPReview.html#rigor-of-design-and-analysis-40",
    "href": "labs/lab-5-PeerPAPReview.html#rigor-of-design-and-analysis-40",
    "title": "Lab Assignment: Peer Review of Pre-Analysis Plan",
    "section": "2. Rigor of Design and Analysis (40%)",
    "text": "2. Rigor of Design and Analysis (40%)\nEvaluate the rigor of the experimental design and analytical approach. Provide comments in each of the following areas:\n\nRandomization Strategy:\nHas the randomization approach been clearly described and well-justified? What improvements could strengthen this section?\nStatistical Power and Sample Size Calculations:\nAre power calculations clearly presented, appropriate, and well-justified? Suggest clarifications or improvements.\nAnalytical Approach and Econometric Methods:\nIs the analytical approach clearly defined and appropriate for answering the research questions? Suggest any improvements or additional considerations.\nPlan for Checking Balance and Attrition:\nIs there a clear, feasible plan to check randomization balance and manage attrition? Provide recommendations to improve rigor.",
    "crumbs": [
      "Unit 5: Threats to Validity",
      "Lab 5: Peer PAP Review"
    ]
  },
  {
    "objectID": "labs/lab-5-PeerPAPReview.html#feasibility-and-practicality-20",
    "href": "labs/lab-5-PeerPAPReview.html#feasibility-and-practicality-20",
    "title": "Lab Assignment: Peer Review of Pre-Analysis Plan",
    "section": "3. Feasibility and Practicality (20%)",
    "text": "3. Feasibility and Practicality (20%)\nEvaluate the practicality and feasibility of the PAP. Provide comments in each of the following areas:\n\nPracticality of Intervention Implementation:\nIs the intervention practical and feasible within realistic constraints (time, resources, logistics)? Suggest ways to enhance feasibility.\nFeasibility of Data Collection and Analysis:\nAre data collection and analysis plans realistic and achievable? Suggest any improvements.",
    "crumbs": [
      "Unit 5: Threats to Validity",
      "Lab 5: Peer PAP Review"
    ]
  },
  {
    "objectID": "labs/lab-5-PeerPAPReview.html#summary-and-additional-comments",
    "href": "labs/lab-5-PeerPAPReview.html#summary-and-additional-comments",
    "title": "Lab Assignment: Peer Review of Pre-Analysis Plan",
    "section": "Summary and Additional Comments",
    "text": "Summary and Additional Comments\nProvide a brief summary of the strengths of the PAP and highlight the most important areas for improvement. Include any additional comments or recommendations not covered above.",
    "crumbs": [
      "Unit 5: Threats to Validity",
      "Lab 5: Peer PAP Review"
    ]
  },
  {
    "objectID": "labs/lab-5-PeerPAPReview.html#submission-instructions",
    "href": "labs/lab-5-PeerPAPReview.html#submission-instructions",
    "title": "Lab Assignment: Peer Review of Pre-Analysis Plan",
    "section": "Submission Instructions",
    "text": "Submission Instructions\n\nSubmit your peer review as a pdf file.\nName your file using the following convention: peer-review-[YourName]-for-[Peer PAP ID].pdf\nSubmit via Gradescope by Friday, April 26th, 2025 at 11:59 PM EST.",
    "crumbs": [
      "Unit 5: Threats to Validity",
      "Lab 5: Peer PAP Review"
    ]
  },
  {
    "objectID": "labs/lab-5-PeerPAPReview.html#resources-and-support",
    "href": "labs/lab-5-PeerPAPReview.html#resources-and-support",
    "title": "Lab Assignment: Peer Review of Pre-Analysis Plan",
    "section": "Resources and Support",
    "text": "Resources and Support\n\nRefer to the attached Guide for Peer Review for detailed guidance on providing constructive and effective feedback.\nFor additional examples of PAPs, consult the AEA RCT Registry or the World Bank DIME Wiki.",
    "crumbs": [
      "Unit 5: Threats to Validity",
      "Lab 5: Peer PAP Review"
    ]
  },
  {
    "objectID": "labs/lab-3-design.html",
    "href": "labs/lab-3-design.html",
    "title": "Lab 3",
    "section": "",
    "text": "In this lab, you will apply concepts of optimal experimental design and randomization techniques to a real-world public health challenge: increasing Hepatitis B virus (HBV) vaccination rates. Specifically, you will:\n\nDesign an experiment to test different subsidy strategies for HBV vaccination\nDetermine the optimal allocation of resources given budget constraints\nCreate a simulated dataset for your experiment\nImplement and compare different randomization approaches\n\nTest for covariate balance across treatment groups\nDocument your randomization procedure thoroughly\n\nBy the end of this lab, you will have practical experience with:\n\nCalculating Minimum Detectable Effects (MDEs) for different experimental designs\nMaking informed design trade-offs based on budget constraints\n\nImplementing various randomization techniques including simple, stratified, and matched-pair randomization\nUsing specialized R packages (fabricatr and randomizr) for efficient experimental design",
    "crumbs": [
      "Unit 2: Design of Experiments",
      "Lab 3: Experiment Design"
    ]
  },
  {
    "objectID": "labs/lab-3-design.html#overview-and-learning-objectives",
    "href": "labs/lab-3-design.html#overview-and-learning-objectives",
    "title": "Lab 3",
    "section": "",
    "text": "In this lab, you will apply concepts of optimal experimental design and randomization techniques to a real-world public health challenge: increasing Hepatitis B virus (HBV) vaccination rates. Specifically, you will:\n\nDesign an experiment to test different subsidy strategies for HBV vaccination\nDetermine the optimal allocation of resources given budget constraints\nCreate a simulated dataset for your experiment\nImplement and compare different randomization approaches\n\nTest for covariate balance across treatment groups\nDocument your randomization procedure thoroughly\n\nBy the end of this lab, you will have practical experience with:\n\nCalculating Minimum Detectable Effects (MDEs) for different experimental designs\nMaking informed design trade-offs based on budget constraints\n\nImplementing various randomization techniques including simple, stratified, and matched-pair randomization\nUsing specialized R packages (fabricatr and randomizr) for efficient experimental design",
    "crumbs": [
      "Unit 2: Design of Experiments",
      "Lab 3: Experiment Design"
    ]
  },
  {
    "objectID": "labs/lab-3-design.html#the-hbv-crisis-at-st.-nulls-community",
    "href": "labs/lab-3-design.html#the-hbv-crisis-at-st.-nulls-community",
    "title": "Lab 3",
    "section": "The HBV Crisis at St. Null’s Community",
    "text": "The HBV Crisis at St. Null’s Community\nScene 1: The Foundation’s Dilemma\nThe corridors of St. Null’s Memorial Hospital buzz with unusual energy as the hospital’s charitable foundation prepares to announce a major public health initiative. CEO Barnaby Beta, dressed in his finest suit with a Hepatitis B awareness pin prominently displayed, stands at the head of the conference room.\n“People, we have a crisis on our hands!” he announces dramatically. “Only 26% of adults in our region have received any dose of the Hepatitis B vaccine. With 1.1 million global deaths annually, we simply must act!”\nDr. P-Hacker nods vigorously. “I’ve already run some numbers,” he interjects, pulling out a colorful chart. “If we offer free vaccines to everyone who walks through our doors, we’ll save exactly 371.5 lives per year.”\nNurse Random raises an eyebrow. “That seems… suspiciously precise. And completely ignoring the fact that people need three doses for full protection. The completion rate in similar programs is only 13%.”\n“Also,” adds Dr. Doub R. Obust, who until now had been quietly examining his coffee cup, “our foundation’s budget is finite. Simply offering everyone a free first dose might not optimize our impact. We need a carefully designed experiment to determine the most effective subsidy strategy.”\nCEO Beta brightens. “An experiment! Perfect! I was just reading about those in my ‘Management Buzzwords Monthly’ magazine.”\nDr. P-Hacker frowns. “But I’ve already made the charts for the press release…”\n“What we need,” says Nurse Random firmly, “is to bring in those methodology consultants from the university again. They’ll help us design a proper randomized trial to test different subsidy approaches.”\nAnd that’s where you come in. The foundation has a fixed budget and needs to determine the optimal strategy: Should they subsidize the first, second, or third dose? Or perhaps distribute subsidies differently? Your task is to design an experiment that will provide clear answers within their budget constraints.",
    "crumbs": [
      "Unit 2: Design of Experiments",
      "Lab 3: Experiment Design"
    ]
  },
  {
    "objectID": "labs/lab-3-design.html#part-1-setting-up-the-problem",
    "href": "labs/lab-3-design.html#part-1-setting-up-the-problem",
    "title": "Lab 3",
    "section": "Part 1: Setting Up the Problem",
    "text": "Part 1: Setting Up the Problem\nFirst, let’s establish some key parameters about HBV vaccination in this community:\n\n\nCurrent first-dose uptake rate: 26% of eligible adults\n\nSeries completion rate without intervention: 13% of those who start\n\nProtection rates: 30-55% after one dose, 75% after two doses, &gt;90% after three doses\n\nDose costs: $25 per dose (same for all three doses)\n\nFollow-up costs: $5 per person for reminder/tracking\n\nFoundation budget: $100,000 for the pilot program\n\nThe foundation is considering several possible intervention strategies:\n\n\nFirst Dose Free: Provide the first dose free to everyone\n\nLast Dose Free: Provide the third dose free to those who complete two doses\n\nGraduated Subsidy: Provide increasing subsidies ($10, $15, $25) for each subsequent dose\n\nFull Series Free: Provide all three doses free, but to a smaller population\n\nBefore diving into randomization, let’s install the required packages:\n\n# Install required packages if not already installed\nrequired_packages &lt;- c(\"data.table\", \"ggplot2\", \"fabricatr\", \"randomizr\", \"multiwayvcov\", \"lmtest\")\n\nfor (pkg in required_packages) {\n  if (!require(pkg, character.only = TRUE)) {\n    install.packages(pkg)\n  }\n}\n\n# Load libraries\nlibrary(data.table)\nlibrary(ggplot2)",
    "crumbs": [
      "Unit 2: Design of Experiments",
      "Lab 3: Experiment Design"
    ]
  },
  {
    "objectID": "labs/lab-3-design.html#part-2-optimal-allocation-of-resources",
    "href": "labs/lab-3-design.html#part-2-optimal-allocation-of-resources",
    "title": "Lab 3",
    "section": "Part 2: Optimal Allocation of Resources",
    "text": "Part 2: Optimal Allocation of Resources\nStep 1: Define the parameters\nLet’s start by defining the variables needed for our power calculations and optimal design:\n\n# Cost parameters\ndose_cost &lt;- 25  # Cost per vaccine dose ($)\nfollowup_cost &lt;- 5  # Cost of follow-up per person ($)\ntotal_budget &lt;- 100000  # Total budget ($)\n\n# Expected outcomes for each strategy (completion rates)\nbaseline_completion &lt;- 0.13  # Baseline completion rate\nexpected_effects &lt;- c(\n  first_dose_free = 0.08,   # Expected increase if first dose is free\n  last_dose_free = 0.12,    # Expected increase if third dose is free\n  graduated = 0.15,         # Expected increase if graduated subsidies\n  full_series = 0.20        # Expected increase if all doses free\n)\n\n# Variance parameters (assume equal for now)\noutcome_variance &lt;- 0.2  # Variance of binary outcome (completion)\n\n# Statistical parameters\nalpha &lt;- 0.05  # Significance level\npower &lt;- 0.80  # Desired power\n\nStep 2: Calculate total cost per person for each strategy\nNext, we’ll calculate the cost per person for each intervention strategy:\n\n# Calculate cost per person for each strategy\ncost_per_person &lt;- c(\n  # First dose free: 1 dose cost + followup\n  first_dose_free = dose_cost + followup_cost,\n  \n  # Last dose free: Those who reach third dose (baseline + effect) get subsidy\n  # Everyone gets followup cost\n  last_dose_free = (baseline_completion + expected_effects[\"last_dose_free\"]) * \n                   dose_cost + followup_cost,\n  \n  # Graduated subsidy: $10 + $15 + $25 for completers\n  # Partial subsidy costs for non-completers + followup for all\n  graduated = 10 + \n              0.50 * 15 + # Assume 50% reach second dose\n              (baseline_completion + expected_effects[\"graduated\"]) * 25 + \n              followup_cost,\n  \n  # Full series free: 3 doses for completers, partial for others + followup\n  full_series = 3 * dose_cost + followup_cost\n)\n\n# Display costs\nprint(cost_per_person)\n\nStep 3: Calculate sample size for each arm with equal allocation\nFirst, let’s determine how many people we could include if we divide our budget equally among the strategies:\n\n# Determine total sample size with equal allocation (4 arms)\nn_per_arm_equal &lt;- floor(total_budget / (sum(cost_per_person)))\ntotal_n_equal &lt;- 4 * n_per_arm_equal\n\ncat(\"With equal allocation:\\n\")\ncat(\"Sample size per arm:\", n_per_arm_equal, \"\\n\")\ncat(\"Total sample size:\", total_n_equal, \"\\n\")\n\nStep 4: Calculate the Minimum Detectable Effect (MDE) with equal allocation\nNow, let’s calculate the MDE for each strategy under equal allocation:\n\n# Minimum Detectable Effect calculation\ncalculate_mde &lt;- function(n1, n0, sigma_sq, alpha = 0.05, power = 0.80) {\n  # Critical values\n  z_alpha &lt;- qnorm(1 - alpha/2)\n  z_power &lt;- qnorm(power)\n  \n  # Standard error\n  se &lt;- sqrt(sigma_sq * (1/n1 + 1/n0))\n  \n  # MDE\n  mde &lt;- (z_alpha + z_power) * se\n  \n  return(mde)\n}\n\n# Calculate MDE for each arm vs. control with equal allocation\nmde_equal &lt;- numeric(length(cost_per_person))\nnames(mde_equal) &lt;- names(cost_per_person)\n\nfor (i in seq_along(cost_per_person)) {\n  mde_equal[i] &lt;- calculate_mde(\n    n1 = n_per_arm_equal,\n    n0 = n_per_arm_equal,\n    sigma_sq = outcome_variance\n  )\n}\n\n# Display MDEs\nprint(mde_equal)\n\nStep 5: Calculate optimal allocation based on costs\nNow, let’s determine the optimal allocation of resources based on the cost per person and the expected effect of each strategy:\n\n# Optimal allocation when costs differ\noptimal_allocation &lt;- function(costs, budget, sigma_sq = NULL) {\n  if (is.null(sigma_sq)) {\n    # If no variance provided, assume equal variance\n    sigma_sq &lt;- rep(outcome_variance, length(costs))\n  }\n  \n  # Calculate the optimal proportions\n  proportions &lt;- sqrt(sigma_sq) / sqrt(costs)\n  proportions &lt;- proportions / sum(proportions)\n  \n  # Calculate sample sizes\n  n_per_arm &lt;- floor(budget * proportions / costs)\n  \n  return(n_per_arm)\n}\n\n# Calculate optimal allocation\nn_per_arm_optimal &lt;- optimal_allocation(\n  costs = cost_per_person,\n  budget = total_budget\n)\n\ntotal_n_optimal &lt;- sum(n_per_arm_optimal)\n\ncat(\"With optimal allocation:\\n\")\nfor (i in seq_along(n_per_arm_optimal)) {\n  cat(names(n_per_arm_optimal)[i], \":\", n_per_arm_optimal[i], \"participants\\n\")\n}\ncat(\"Total sample size:\", total_n_optimal, \"\\n\")\n\nStep 6: Calculate the MDE with optimal allocation\nLet’s calculate the MDEs we can expect with the optimal allocation:\n\n# Calculate MDE for each arm vs. control with optimal allocation\nmde_optimal &lt;- numeric(length(cost_per_person))\nnames(mde_optimal) &lt;- names(cost_per_person)\n\n# Control group size (we'll use the smallest treatment group as reference)\nn0 &lt;- min(n_per_arm_optimal)\n\nfor (i in seq_along(cost_per_person)) {\n  mde_optimal[i] &lt;- calculate_mde(\n    n1 = n_per_arm_optimal[i],\n    n0 = n0,\n    sigma_sq = outcome_variance\n  )\n}\n\n# Display MDEs\nprint(mde_optimal)\n\n# Compare to expected effects\nresults_table &lt;- data.frame(\n  Strategy = names(expected_effects),\n  Expected_Effect = expected_effects,\n  MDE_Equal = mde_equal,\n  MDE_Optimal = mde_optimal,\n  Detectable_Equal = expected_effects &gt; mde_equal,\n  Detectable_Optimal = expected_effects &gt; mde_optimal\n)\n\nprint(results_table)\n\nDiscussion Questions\n\nWhich allocation strategy (equal or optimal) allows you to detect more of the expected effects?\nWhat trade-offs are involved in choosing between these allocation strategies?\nBased on the MDEs, which intervention strategies should the foundation prioritize testing?",
    "crumbs": [
      "Unit 2: Design of Experiments",
      "Lab 3: Experiment Design"
    ]
  },
  {
    "objectID": "labs/lab-3-design.html#part-3-creating-a-simulated-dataset",
    "href": "labs/lab-3-design.html#part-3-creating-a-simulated-dataset",
    "title": "Lab 3",
    "section": "Part 3: Creating a Simulated Dataset",
    "text": "Part 3: Creating a Simulated Dataset\nScene 2: The Dataset Debate\nBack at St. Null’s, Dr. P-Hacker is pacing the room impatiently.\n“I still don’t understand why we need to wait for an experiment,” he complains. “I can just use our electronic health records to prove whichever strategy you prefer, CEO Beta.”\nNurse Random crosses her arms. “And that’s exactly the problem. We need a properly randomized prospective study, not a fishing expedition through historical data.”\n“But we have hundreds of thousands of patient records!” Dr. P-Hacker protests.\nDr. Doub R. Obust clears his throat. “Ah, but what we don’t have is the counterfactual. We don’t know what would have happened if Patient X had been offered a different subsidy scheme.”\nCEO Beta looks confused. “Counter… what now?”\n“What people would have done under different circumstances,” Nurse Random explains. “That’s why we need a clean experiment with proper randomization.”\n“Fine,” huffs Dr. P-Hacker. “But how do we even create a dataset for this yet-to-happen experiment?”\nDr. Doub R. Obust smiles. “That’s where our university consultants come in. They’ll simulate the data based on our best knowledge of the population characteristics. Then they’ll help us implement a proper randomization strategy.”\nStep 1: Manual dataset creation\nLet’s create a simulated dataset of potential participants for our HBV vaccination study. We’ll include baseline characteristics that might affect vaccine uptake:\n\n# Set seed for reproducibility\nset.seed(072111)\n\n# Define sample size (from our optimal allocation calculation)\nn_total &lt;- sum(n_per_arm_optimal)\n\n# Create baseline dataset manually\ncreate_baseline_data &lt;- function(n) {\n  # Initialize data table\n  dt &lt;- data.table(\n    id = 1:n,\n    \n    # Demographics\n    age = sample(18:65, n, replace = TRUE),\n    female = rbinom(n, 1, 0.55),\n    education = sample(1:4, n, replace = TRUE, \n                      prob = c(0.15, 0.3, 0.4, 0.15)),\n    \n    # Health factors\n    insurance = rbinom(n, 1, 0.7),\n    prior_vaccines = rbinom(n, 1, 0.4),\n    health_conscious = rbinom(n, 1, 0.3),\n    \n    # Access factors\n    distance_to_clinic = runif(n, 0, 30),\n    works_weekdays = rbinom(n, 1, 0.8)\n  )\n  \n  # Convert categorical variables to factors\n  dt[, education := factor(education, \n                          levels = 1:4, \n                          labels = c(\"Less than HS\", \"HS\", \"Some college\", \"College+\"))]\n  \n  return(dt)\n}\n\n# Create the baseline dataset\nsim_data &lt;- create_baseline_data(n_total)\n\n# View the first few rows\nhead(sim_data)\n\n# Summary statistics\nsummary(sim_data)\n\nStep 2: Using fabricatr for more efficient data generation\nThe fabricatr package provides a more efficient way to generate simulated data:\n\nlibrary(fabricatr)\n\n# Create the same dataset using fabricatr\nsim_data_fab &lt;- fabricate(\n  N = n_total,\n  id = 1:n_total,\n  age = sample(18:65, N, replace = TRUE),\n  female = draw_binary(prob = 0.55),\n  education = draw_categorical(\n    prob = c(0.15, 0.3, 0.4, 0.15),\n    N = N,\n    category_labels = c(\"Less than HS\", \"HS\", \"Some college\", \"College+\")\n  ),\n  insurance = draw_binary(prob = 0.7),\n  prior_vaccines = draw_binary(prob = 0.4),\n  health_conscious = draw_binary(prob = 0.3),\n  distance_to_clinic = runif(N, 0, 30),\n  works_weekdays = draw_binary(prob = 0.8)\n)\n\n# Convert to data.table for consistency\nsim_data_fab &lt;- as.data.table(sim_data_fab)\n\n# View the first few rows\nhead(sim_data_fab)\n\n# Summary statistics\nsummary(sim_data_fab)\n\nStep 3: Add a predicted baseline completion probability\nTo model the heterogeneity in participants’ likelihood of completing the vaccine series, let’s add a baseline predicted probability based on their characteristics:\n\n# Add predicted baseline probability\nsim_data[, baseline_prob := 0.13 +  # Base rate\n         0.05 * (age &gt; 40) +         # Older people more likely\n         0.08 * (insurance == 1) +    # Insurance increases likelihood\n         0.10 * (prior_vaccines == 1) + # Prior vaccination behavior\n         0.07 * (health_conscious == 1) - # Health consciousness\n         0.005 * distance_to_clinic +    # Distance (negative effect)\n         0.03 * (education %in% c(\"Some college\", \"College+\")) # Education effect\n]\n\n# Constrain probabilities between 0 and 1\nsim_data[baseline_prob &lt; 0, baseline_prob := 0]\nsim_data[baseline_prob &gt; 1, baseline_prob := 1]\n\n# Display distribution of baseline probabilities\nhist(sim_data$baseline_prob, \n     main = \"Distribution of Baseline Completion Probabilities\",\n     xlab = \"Probability of Completing Vaccine Series\",\n     col = \"lightblue\",\n     breaks = 20)",
    "crumbs": [
      "Unit 2: Design of Experiments",
      "Lab 3: Experiment Design"
    ]
  },
  {
    "objectID": "labs/lab-3-design.html#part-4-implementing-different-randomization-approaches",
    "href": "labs/lab-3-design.html#part-4-implementing-different-randomization-approaches",
    "title": "Lab 3",
    "section": "Part 4: Implementing Different Randomization Approaches",
    "text": "Part 4: Implementing Different Randomization Approaches\nScene 3: The Randomization Revelation\nIn the hospital’s data center, Dr. P-Hacker is gesturing at his computer screen, where a crude Excel sheet displays his “randomization” plan.\n“I’ve already assigned participants to groups,” he announces proudly. “I just alternated down the list: A, B, C, D, A, B, C, D…”\nNurse Random looks horrified. “That’s not randomization at all! It’s completely predictable, and it doesn’t account for any baseline characteristics.”\n“But it’s so… neat,” Dr. P-Hacker protests.\nDr. Doub R. Obust shakes his head sadly. “A cardinal sin in experimental design. We need proper randomization that gives each participant a known, non-zero probability of assignment to each treatment condition, and that’s independent of potential outcomes.”\n“Exactly,” Nurse Random agrees. “And ideally, we should stratify by key characteristics to ensure balance across treatment arms.”\n“Strati-what now?” CEO Beta asks, glancing up from his phone where he’s been browsing luxury yacht websites.\nDr. Doub R. Obust sighs. “This is why we need the university consultants.”\nStep 1: Simple Randomization\nLet’s implement the simplest form of randomization - Bernoulli trials:\n\n# Define treatment conditions\ntreatment_conditions &lt;- c(\"Control\", \"FirstDoseFree\", \"LastDoseFree\", \"Graduated\")\n\n# Simple randomization (Bernoulli trials)\nsimple_randomize &lt;- function(data, conditions, probabilities = NULL) {\n  n &lt;- nrow(data)\n  k &lt;- length(conditions)\n  \n  # If no probabilities provided, use equal probabilities\n  if (is.null(probabilities)) {\n    probabilities &lt;- rep(1/k, k)\n  }\n  \n  # Generate random assignments\n  assignments &lt;- sample(conditions, n, replace = TRUE, prob = probabilities)\n  \n  return(assignments)\n}\n\n# Apply simple randomization\nset.seed(072111)\nsim_data[, treatment_simple := simple_randomize(sim_data, treatment_conditions)]\n\n# Check distribution\ntable(sim_data$treatment_simple)\n\nStep 2: Complete Randomization\nNow let’s use complete randomization to ensure exact numbers in each group:\n\n# Complete randomization\ncomplete_randomize &lt;- function(data, conditions, ns = NULL) {\n  n &lt;- nrow(data)\n  k &lt;- length(conditions)\n  \n  # If no specific counts provided, divide equally\n  if (is.null(ns)) {\n    ns &lt;- rep(floor(n / k), k)\n    # Distribute any remainder\n    remainder &lt;- n - sum(ns)\n    if (remainder &gt; 0) {\n      ns[1:remainder] &lt;- ns[1:remainder] + 1\n    }\n  }\n  \n  # Check for NA values in ns\n  if (any(is.na(ns))) {\n    stop(\"Group sizes (ns) contain NA values\")\n  }\n  \n  # Ensure all ns are positive integers\n  if (any(ns &lt;= 0) || any(ns != floor(ns))) {\n    stop(\"All group sizes must be positive integers\")\n  }\n  \n  # Ensure sum of ns equals total sample size\n  sum_ns &lt;- sum(ns)\n  if (sum_ns != n) {\n    stop(paste(\"Sum of group sizes (\", sum_ns, \") does not equal total sample size (\", n, \")\", sep = \"\"))\n  }\n  \n  # Create vector of assignments\n  assignments &lt;- rep(conditions, ns)\n  \n  # Randomly shuffle\n  assignments &lt;- sample(assignments, n)\n  \n  return(assignments)\n}\n\n# Calculate desired sample sizes from optimal allocation\n# Adjust to ensure the sum matches the total sample size\nn_total_current &lt;- nrow(sim_data)\n\n# Debug n_per_arm_optimal\nprint(\"n_per_arm_optimal values:\")\nprint(n_per_arm_optimal)\n\n# Check for NA values\nif (any(is.na(n_per_arm_optimal))) {\n  warning(\"n_per_arm_optimal contains NA values, using equal allocation instead\")\n  # Fall back to equal allocation\n  treatment_counts &lt;- rep(floor(n_total_current / 4), 4)\n  names(treatment_counts) &lt;- c(\"Control\", \"FirstDoseFree\", \"LastDoseFree\", \"Graduated\")\n  \n  # Handle remainder\n  remainder &lt;- n_total_current - sum(treatment_counts)\n  if (remainder &gt; 0) {\n    treatment_counts[1:remainder] &lt;- treatment_counts[1:remainder] + 1\n  }\n} else {\n  # Original calculation, but with more robust error handling\n  # Set control size to a reasonable value\n  control_size &lt;- min(n_per_arm_optimal)\n  if (is.na(control_size) || control_size &lt;= 0) {\n    control_size &lt;- floor(n_total_current / 8) # Default to 1/8 of total\n  }\n  \n  # Get treatment arm sizes, replacing any NA values with reasonable defaults\n  get_arm_size &lt;- function(arm_name, default_proportion = 0.25) {\n    size &lt;- n_per_arm_optimal[arm_name]\n    if (is.na(size) || size &lt;= 0) {\n      # Default to 1/4 of what's left after control\n      size &lt;- floor((n_total_current - control_size) * default_proportion)\n    }\n    return(size)\n  }\n  \n  # Total from optimal allocation\n  arms &lt;- c(\n    Control = control_size,\n    FirstDoseFree = get_arm_size(\"first_dose_free\", 1/3),\n    LastDoseFree = get_arm_size(\"last_dose_free\", 1/3),\n    Graduated = get_arm_size(\"graduated\", 1/3)\n  )\n  \n  total_optimal &lt;- sum(arms)\n  \n  # Calculate proportions\n  proportions &lt;- arms / total_optimal\n  \n  # Recalculate group counts based on current sample size\n  treatment_counts &lt;- floor(proportions * n_total_current)\n  \n  # Handle remainder to ensure sum matches exactly\n  remainder &lt;- n_total_current - sum(treatment_counts)\n  if (remainder &gt; 0) {\n    # Add the remainder to groups in order until it's all distributed\n    for (i in 1:remainder) {\n      idx &lt;- (i - 1) %% length(treatment_counts) + 1\n      treatment_counts[idx] &lt;- treatment_counts[idx] + 1\n    }\n  }\n}\n\n# Verify the sum equals the total\ncat(\"Treatment counts:\")\nprint(treatment_counts)\ncat(\"Sum of treatment counts:\", sum(treatment_counts), \"\\n\")\ncat(\"Total sample size:\", n_total_current, \"\\n\")\n\n# Apply complete randomization with adjusted allocation\nset.seed(072111)\ntryCatch({\n  # Verify one more time that counts sum to the correct number\n  if (sum(treatment_counts) != nrow(sim_data)) {\n    stop(\"Treatment counts must sum to the total number of rows in sim_data\")\n  }\n  \n  sim_data[, treatment_complete := complete_randomize(\n    sim_data, \n    names(treatment_counts),\n    ns = treatment_counts\n  )]\n  \n  # Show results of randomization\n  cat(\"\\nTreatment assignment counts:\\n\")\n  print(table(sim_data$treatment_complete))\n}, error = function(e) {\n  # Fall back to simple equal allocation as a last resort\n  message(\"Error in randomization: \", e$message)\n  message(\"Falling back to simple equal allocation\")\n  \n  n &lt;- nrow(sim_data)\n  k &lt;- 4  # Number of treatment groups\n  equal_counts &lt;- rep(floor(n / k), k)\n  remainder &lt;- n - sum(equal_counts)\n  if (remainder &gt; 0) {\n    equal_counts[1:remainder] &lt;- equal_counts[1:remainder] + 1\n  }\n  \n  conditions &lt;- c(\"Control\", \"FirstDoseFree\", \"LastDoseFree\", \"Graduated\")\n  \n  sim_data[, treatment_complete := complete_randomize(\n    sim_data, \n    conditions,\n    ns = equal_counts\n  )]\n})\n\n# Check distribution\ntable(sim_data$treatment_complete)\n\nStep 3: Stratified Randomization\nLet’s stratify by key characteristics that might influence outcomes:\n\n# Stratified randomization\nstratified_randomize &lt;- function(data, conditions, strata_vars, ns = NULL) {\n  # Create strata based on combinations of strata_vars\n  data$strata &lt;- do.call(paste, c(data[, strata_vars, with = FALSE], sep = \"_\"))\n  \n  # Get unique strata\n  unique_strata &lt;- unique(data$strata)\n  \n  # Initialize treatment assignment vector\n  n &lt;- nrow(data)\n  assignments &lt;- rep(NA, n)\n  \n  # Loop through strata\n  for (stratum in unique_strata) {\n    # Get indices for this stratum\n    stratum_indices &lt;- which(data$strata == stratum)\n    stratum_size &lt;- length(stratum_indices)\n    \n    # Determine counts for this stratum (proportional to overall)\n    if (is.null(ns)) {\n      # Equal allocation within stratum\n      stratum_ns &lt;- rep(floor(stratum_size / length(conditions)), length(conditions))\n      remainder &lt;- stratum_size - sum(stratum_ns)\n      if (remainder &gt; 0) {\n        stratum_ns[1:remainder] &lt;- stratum_ns[1:remainder] + 1\n      }\n    } else {\n      # Proportional allocation based on provided ns\n      stratum_props &lt;- ns / sum(ns)\n      stratum_ns &lt;- floor(stratum_size * stratum_props)\n      remainder &lt;- stratum_size - sum(stratum_ns)\n      if (remainder &gt; 0) {\n        # Distribute remainder to largest groups\n        for (i in order(stratum_ns, decreasing = TRUE)[1:remainder]) {\n          stratum_ns[i] &lt;- stratum_ns[i] + 1\n        }\n      }\n    }\n    \n    # Generate assignments for this stratum\n    stratum_assignments &lt;- rep(conditions, stratum_ns)\n    if (length(stratum_assignments) &lt; stratum_size) {\n      # Handle edge case: add one more randomized assignment if needed\n      stratum_assignments &lt;- c(stratum_assignments, \n                              sample(conditions, stratum_size - length(stratum_assignments)))\n    }\n    \n    # Randomly shuffle and assign\n    stratum_assignments &lt;- sample(stratum_assignments, stratum_size)\n    assignments[stratum_indices] &lt;- stratum_assignments\n  }\n  \n  return(assignments)\n}\n\n# Define stratification variables\nstrata_vars &lt;- c(\"insurance\", \"prior_vaccines\")\n\n# Apply stratified randomization\nset.seed(072111)\nsim_data[, treatment_strat := stratified_randomize(\n  sim_data, \n  names(treatment_counts),\n  strata_vars = strata_vars,\n  ns = treatment_counts\n)]\n\n# Check distribution\ntable(sim_data$treatment_strat)\n\n# Check distribution within strata\nwith(sim_data, table(insurance, prior_vaccines, treatment_strat))\n\nStep 4: Using randomizr for efficient randomization\nThe randomizr package provides a more streamlined way to implement these randomization approaches:\n\nlibrary(randomizr)\n\n# Simple randomization with randomizr\nsim_data[, treatment_randomizr_simple := complete_ra(\n  N = nrow(sim_data),\n  num_arms = length(treatment_conditions),\n  conditions = treatment_conditions\n)]\n\n# Complete randomization with randomizr\nsim_data[, treatment_randomizr_complete := complete_ra(\n  N = nrow(sim_data),\n  conditions = names(treatment_counts),\n  prob_each = treatment_counts / sum(treatment_counts)\n)]\n\n# Stratified randomization with randomizr\nsim_data[, strata_combined := paste(insurance, prior_vaccines, sep = \"_\")]\nsim_data[, treatment_randomizr_strat := block_ra(\n  blocks = strata_combined,\n  conditions = names(treatment_counts),\n  prob_each = treatment_counts / sum(treatment_counts)\n)]\n\n# Check distributions\ncat(\"Simple randomization with randomizr:\\n\")\ntable(sim_data$treatment_randomizr_simple)\n\ncat(\"\\nComplete randomization with randomizr:\\n\")\ntable(sim_data$treatment_randomizr_complete)\n\ncat(\"\\nStratified randomization with randomizr:\\n\")\ntable(sim_data$treatment_randomizr_strat)",
    "crumbs": [
      "Unit 2: Design of Experiments",
      "Lab 3: Experiment Design"
    ]
  },
  {
    "objectID": "labs/lab-3-design.html#part-5-assessing-balance-across-treatment-groups",
    "href": "labs/lab-3-design.html#part-5-assessing-balance-across-treatment-groups",
    "title": "Lab 3",
    "section": "Part 5: Assessing Balance Across Treatment Groups",
    "text": "Part 5: Assessing Balance Across Treatment Groups\nScene 4: The Balance Inquiry\nWith the randomization complete, the team at St. Null’s gathers to review the results.\nDr. P-Hacker squints at the printout. “It looks random to me. Can we start the intervention now?”\nNurse Random shakes her head firmly. “Not until we check for balance. We need to make sure our randomization didn’t produce any significant differences in baseline characteristics between groups.”\n“But it’s randomized!” protests Dr. P-Hacker. “Doesn’t that guarantee balance?”\nDr. Doub R. Obust adjusts his glasses. “In expectation, yes. But any single realization of a randomization process can result in imbalances by chance. We need to verify that our treatment groups are comparable at baseline.”\n“Exactly,” Nurse Random agrees. “We should check for balance on key characteristics, especially those that might influence our outcomes.”\n“And if there’s imbalance?” asks CEO Beta, looking worried.\n“Then we might need to re-randomize,” Dr. Doub R. Obust says, “or account for these differences in our analysis.”\nDr. P-Hacker sighs dramatically. “More delays!”\n“Better a delayed but valid study,” Nurse Random counters, “than a quick but meaningless one.”\nStep 1: Creating balance tables\nLet’s assess balance across our treatment groups for key covariates:\n\n# Function to assess balance\nassess_balance &lt;- function(data, treatment_var, covariates) {\n  # Check if treatment variable exists\n  if (!treatment_var %in% names(data)) {\n    stop(paste(\"Treatment variable\", treatment_var, \"not found in dataset\"))\n  }\n  \n  # Check if treatment variable has multiple levels\n  treatment_levels &lt;- unique(data[[treatment_var]])\n  if (length(treatment_levels) &lt; 2) {\n    stop(\"Treatment variable must have at least 2 levels for balance assessment\")\n  }\n  \n  # Initialize results table\n  results &lt;- data.table(\n    Variable = character(),\n    Category = character(),\n    Overall = numeric()\n  )\n  \n  # Add columns for each treatment level\n  for (level in treatment_levels) {\n    results[[as.character(level)]] &lt;- numeric()\n  }\n  \n  # Add columns for p-values\n  results[[\"p.value\"]] &lt;- numeric()\n  \n  # Process each covariate\n  for (var in covariates) {\n    # Check if variable exists\n    if (!var %in% names(data)) {\n      warning(paste(\"Covariate\", var, \"not found in dataset - skipping\"))\n      next\n    }\n    \n    # Determine variable type and handle appropriately\n    if (is.factor(data[[var]]) || is.character(data[[var]]) || length(unique(na.omit(data[[var]]))) &lt;= 5) {\n      # Categorical variable (factor, character, or numeric with few unique values)\n      var_data &lt;- data[[var]]\n      \n      # Ensure factor type for table creation\n      if (!is.factor(var_data)) {\n        var_data &lt;- as.factor(var_data)\n      }\n      \n      # Create contingency table safely\n      var_table &lt;- tryCatch({\n        table(var_data, data[[treatment_var]])\n      }, error = function(e) {\n        warning(paste(\"Error creating table for\", var, \":\", e$message))\n        return(NULL)\n      })\n      \n      # Skip if table creation failed\n      if (is.null(var_table) || any(dim(var_table) == 0)) {\n        warning(paste(\"Could not create valid contingency table for\", var))\n        next\n      }\n      \n      # Calculate proportions\n      var_props &lt;- tryCatch({\n        prop.table(var_table, margin = 2)\n      }, error = function(e) {\n        warning(paste(\"Error calculating proportions for\", var, \":\", e$message))\n        # Create a matrix of NAs with appropriate dimensions\n        props &lt;- matrix(NA, nrow = nrow(var_table), ncol = ncol(var_table))\n        rownames(props) &lt;- rownames(var_table)\n        colnames(props) &lt;- colnames(var_table)\n        return(props)\n      })\n      \n      # Chi-square test (only if enough data)\n      chi_p_value &lt;- tryCatch({\n        if (any(var_table &lt; 5)) {\n          # Use Fisher's exact test for small counts\n          fisher.test(var_table, simulate.p.value = TRUE)$p.value\n        } else {\n          chisq.test(var_table)$p.value\n        }\n      }, error = function(e) {\n        warning(paste(\"Error in statistical test for\", var, \":\", e$message))\n        return(NA)\n      })\n      \n      # Add rows for each category\n      for (cat_idx in 1:nrow(var_props)) {\n        cat &lt;- rownames(var_props)[cat_idx]\n        \n        row &lt;- data.table(\n          Variable = var,\n          Category = as.character(cat),\n          Overall = sum(var_data == cat, na.rm = TRUE) / sum(!is.na(var_data))\n        )\n        \n        # Add proportions for each treatment level\n        for (level in treatment_levels) {\n          level_col &lt;- which(colnames(var_props) == as.character(level))\n          if (length(level_col) &gt; 0) {\n            row[[as.character(level)]] &lt;- var_props[cat_idx, level_col]\n          } else {\n            row[[as.character(level)]] &lt;- NA\n          }\n        }\n        \n        # Add p-value\n        row[[\"p.value\"]] &lt;- chi_p_value\n        \n        # Add to results\n        results &lt;- rbind(results, row)\n      }\n    } else {\n      # Continuous variable\n      # Overall mean\n      row &lt;- data.table(\n        Variable = var,\n        Category = \"Mean\",\n        Overall = mean(data[[var]], na.rm = TRUE)\n      )\n      \n      # Means for each treatment level\n      for (level in treatment_levels) {\n        level_data &lt;- data[data[[treatment_var]] == level, ][[var]]\n        if (length(level_data) &gt; 0) {\n          row[[as.character(level)]] &lt;- mean(level_data, na.rm = TRUE)\n        } else {\n          row[[as.character(level)]] &lt;- NA\n        }\n      }\n      \n      # ANOVA test (catch errors)\n      row[[\"p.value\"]] &lt;- tryCatch({\n        anova_model &lt;- aov(as.formula(paste(var, \"~\", treatment_var)), data = data)\n        anova_summary &lt;- summary(anova_model)\n        anova_summary[[1]]$`Pr(&gt;F)`[1]\n      }, error = function(e) {\n        warning(paste(\"Error in ANOVA for\", var, \":\", e$message))\n        return(NA)\n      })\n      \n      # Add to results\n      results &lt;- rbind(results, row)\n    }\n  }\n  \n  return(results)\n}\n\n\n# Select covariates to check for balance\nbalance_covariates &lt;- c(\"age\", \"female\", \"education\", \"insurance\", \"prior_vaccines\", \"distance_to_clinic\")\n\n# Check balance for complete randomization\nbalance_complete &lt;- assess_balance(\n  sim_data, \n  \"treatment_complete\", \n  balance_covariates\n)\n\n# Check balance for stratified randomization\nbalance_strat &lt;- assess_balance(\n  sim_data, \n  \"treatment_strat\", \n  balance_covariates\n)\n\n# Print balance tables\ncat(\"Balance table for complete randomization:\\n\")\nprint(balance_complete)\n\ncat(\"\\nBalance table for stratified randomization:\\n\")\nprint(balance_strat)\n\n# Compare number of significant imbalances\nsig_complete &lt;- sum(balance_complete$p.value &lt; 0.05)\nsig_strat &lt;- sum(balance_strat$p.value &lt; 0.05)\n\ncat(\"\\nNumber of significant imbalances (p &lt; 0.05):\\n\")\ncat(\"Complete randomization:\", sig_complete, \"\\n\")\ncat(\"Stratified randomization:\", sig_strat, \"\\n\")\n\nStep 2: Visual assessment of balance\nLet’s create visual representations of balance across treatment groups:\n\nlibrary(ggplot2)\n\n# Function to plot standardized mean differences\nplot_balance &lt;- function(data, treatment_var, covariates) {\n  # Initialize results\n  results &lt;- data.frame(\n    Variable = character(),\n    Category = character(),\n    Std_Diff = numeric(),\n    Treatment = character()\n  )\n  \n  # Process each covariate\n  for (var in covariates) {\n    # Overall SD\n    var_sd &lt;- sd(as.numeric(data[[var]]), na.rm = TRUE)\n    \n    # Reference level (Control)\n    ref_mean &lt;- mean(as.numeric(data[data[[treatment_var]] == \"Control\", ][[var]]), na.rm = TRUE)\n    \n    # Other treatment levels\n    treatment_levels &lt;- setdiff(unique(data[[treatment_var]]), \"Control\")\n    \n    for (level in treatment_levels) {\n      # Treatment mean\n      treat_mean &lt;- mean(as.numeric(data[data[[treatment_var]] == level, ][[var]]), na.rm = TRUE)\n      \n      # Standardized difference\n      std_diff &lt;- (treat_mean - ref_mean) / var_sd\n      \n      # Add to results\n      results &lt;- rbind(results, data.frame(\n        Variable = var,\n        Std_Diff = std_diff,\n        Treatment = level\n      ))\n    }\n  }\n  \n  # Create plot\n  ggplot(results, aes(x = Variable, y = Std_Diff, fill = Treatment)) +\n    geom_bar(stat = \"identity\", position = \"dodge\") +\n    geom_hline(yintercept = 0, linetype = \"dashed\") +\n    geom_hline(yintercept = c(-0.1, 0.1), linetype = \"dotted\", color = \"red\") +\n    labs(\n      title = \"Standardized Mean Differences\",\n      subtitle = \"Treatment vs. Control\",\n      x = \"Variable\",\n      y = \"Standardized Difference\"\n    ) +\n    coord_flip() +\n    theme_minimal()\n}\n\n# Create balance plots\nbalance_plot_complete &lt;- plot_balance(\n  sim_data, \n  \"treatment_complete\", \n  c(\"age\", \"female\", \"insurance\", \"prior_vaccines\", \"distance_to_clinic\")\n)\n\nbalance_plot_strat &lt;- plot_balance(\n  sim_data, \n  \"treatment_strat\", \n  c(\"age\", \"female\", \"insurance\", \"prior_vaccines\", \"distance_to_clinic\")\n)\n\n# Display plots\nbalance_plot_complete\nbalance_plot_strat",
    "crumbs": [
      "Unit 2: Design of Experiments",
      "Lab 3: Experiment Design"
    ]
  },
  {
    "objectID": "labs/lab-3-design.html#part-6-documenting-the-randomization-procedure",
    "href": "labs/lab-3-design.html#part-6-documenting-the-randomization-procedure",
    "title": "Lab 3",
    "section": "Part 6: Documenting the Randomization Procedure",
    "text": "Part 6: Documenting the Randomization Procedure\nScene 5: Documentation Day\nAs the team finalizes their randomization approach, Nurse Random insists on proper documentation.\n“We need to document every step of our randomization procedure,” she explains. “Not just for transparency, but for reproducibility.”\nDr. P-Hacker looks puzzled. “Can’t we just say ‘we randomized participants’ and leave it at that?”\nDr. Doub R. Obust chuckles. “That’s like saying ‘we did surgery’ without specifying the procedure, instruments, or technique. Details matter in science.”\n“Exactly,” Nurse Random agrees. “We need to record the randomization method, the seed used, the stratification variables, and verification of balance.”\n“But why?” whines Dr. P-Hacker.\n“Because,” Dr. Doub R. Obust explains patiently, “someday someone might need to replicate our study. Or we might need to defend our methods. Good documentation is good science.”\nCEO Beta nods decisively. “Makes sense to me. If I’m going to put the hospital’s name on this project, I want it to be bulletproof.”\nStep 1: Creating a randomization log\nLet’s create a comprehensive randomization log:\n\n# Create randomization log\nrandomization_log &lt;- data.table(\n  study_name = \"HBV Vaccination Subsidy Trial\",\n  randomization_date = Sys.Date(),\n  randomization_method = \"Stratified randomization\",\n  seed_value = 072111,\n  stratification_variables = paste(strata_vars, collapse = \", \"),\n  treatment_conditions = paste(names(treatment_counts), collapse = \", \"),\n  sample_size_total = nrow(sim_data),\n  sample_sizes_by_arm = paste(paste(names(treatment_counts), treatment_counts, sep = \": \"), \n                             collapse = \"; \"),\n  balance_assessment = paste0(sig_strat, \" significant imbalances out of \", \n                             length(balance_covariates), \" covariates\")\n)\n\n# Save randomization log\nwrite.csv(randomization_log, \"randomization_log.csv\", row.names = FALSE)\n\n# Save treatment assignments\ntreatment_assignments &lt;- sim_data[, .(id, treatment = treatment_strat)]\nwrite.csv(treatment_assignments, \"treatment_assignments.csv\", row.names = FALSE)\n\n# Save balance assessment\nwrite.csv(balance_strat, \"balance_assessment.csv\", row.names = FALSE)\n\n# Display randomization log\nprint(randomization_log)\n\nStep 2: Finalizing the allocation approch\nBased on our analyses, let’s make a final recommendation for the experimental design:\n\n# Extract key results\nfinal_allocation &lt;- treatment_counts\nmdes &lt;- mde_optimal\nexpected_completion &lt;- baseline_completion + expected_effects\n\n# Create summary table\nfinal_summary &lt;- data.table(\n  Strategy = names(final_allocation),\n  Sample_Size = final_allocation,\n  Proportion = final_allocation / sum(final_allocation),\n  Expected_Completion = c(baseline_completion, expected_completion[-4]),  # Exclude full_series\n  MDE = c(NA, mdes[-4]),  # NA for control, exclude full_series\n  Powered = c(NA, expected_effects[-4] &gt; mdes[-4])  # NA for control, exclude full_series\n)\n\n# Display final recommendation\nprint(final_summary)",
    "crumbs": [
      "Unit 2: Design of Experiments",
      "Lab 3: Experiment Design"
    ]
  },
  {
    "objectID": "labs/lab-3-design.html#discussion-and-conclusion",
    "href": "labs/lab-3-design.html#discussion-and-conclusion",
    "title": "Lab 3",
    "section": "Discussion and Conclusion",
    "text": "Discussion and Conclusion\nFinal Questions to Consider\nAs you complete this lab, consider the following questions:\n\nHow does the optimal allocation differ from an equal allocation approach? What factors drive this difference?\nWhich randomization approach provided the best balance across treatment groups, and why?\n\nWhat are the trade-offs between the different subsidy strategies in terms of:\n\nCost-effectiveness\nExpected impact on vaccine completion rates\nStatistical power to detect effects\n\n\nIf the foundation asked you to recommend just one subsidy strategy to test against a control group (rather than testing all three), which would you choose and why?\n\nHow might the results change if:\n\nThe ICC (intraclass correlation) between patients within clinics was high?\nThe baseline completion rate was much higher (e.g., 30% instead of 13%)?\nThe cost of one dose was much higher than the others?\n\n\nSubmission Instructions\n\nMake sure your .qmd file knits successfully to HTML.\nInclude your answers to the discussion questions.\nSubmit your completed lab to Gradescope by the deadline.\n\nRemember that good experimental design balances statistical power, practical constraints, and the focus of inquiry. In the real world, there’s rarely a perfect design—only thoughtful trade-offs guided by clear priorities.",
    "crumbs": [
      "Unit 2: Design of Experiments",
      "Lab 3: Experiment Design"
    ]
  },
  {
    "objectID": "labs/lab-2-Power-sols.html",
    "href": "labs/lab-2-Power-sols.html",
    "title": "Lab 2: Power by Simulation",
    "section": "",
    "text": "In this lab, you will explore the concept of statistical conclusion validity by conducting power calculations via simulation. Specifically, you will:\n\nConduct a power simulation for a simple randomized experiment without clustering (i.e., ignoring the fact that participants may be grouped within clinics).\nExtend that simulation to account for clustering (i.e., clinics as the unit of randomization).\nExamine the impact of sample size, effect size, and clustering on statistical power, and visualize how minimum detectable effect sizes (MDE) change with sample size.\n\nBy the end of this lab, you will have a deeper understanding of:\n\nHow sources of uncertainty (sampling variation, variance in potential outcomes across participants, and measurement error) affect your study’s outcomes.\nWhy we must beware of Type I (false positive) and Type II (false negative) errors—especially if Dr. P-Hacker is anywhere near our data.\nHow p-values should (and should not!) be interpreted.\nHow to simulate data that include cluster-level effects and adjust for these effects in your analysis."
  },
  {
    "objectID": "labs/lab-2-Power-sols.html#overview-and-learning-objectives",
    "href": "labs/lab-2-Power-sols.html#overview-and-learning-objectives",
    "title": "Lab 2: Power by Simulation",
    "section": "",
    "text": "In this lab, you will explore the concept of statistical conclusion validity by conducting power calculations via simulation. Specifically, you will:\n\nConduct a power simulation for a simple randomized experiment without clustering (i.e., ignoring the fact that participants may be grouped within clinics).\nExtend that simulation to account for clustering (i.e., clinics as the unit of randomization).\nExamine the impact of sample size, effect size, and clustering on statistical power, and visualize how minimum detectable effect sizes (MDE) change with sample size.\n\nBy the end of this lab, you will have a deeper understanding of:\n\nHow sources of uncertainty (sampling variation, variance in potential outcomes across participants, and measurement error) affect your study’s outcomes.\nWhy we must beware of Type I (false positive) and Type II (false negative) errors—especially if Dr. P-Hacker is anywhere near our data.\nHow p-values should (and should not!) be interpreted.\nHow to simulate data that include cluster-level effects and adjust for these effects in your analysis."
  },
  {
    "objectID": "labs/lab-2-Power-sols.html#the-hospital-drama-begins",
    "href": "labs/lab-2-Power-sols.html#the-hospital-drama-begins",
    "title": "Lab 2: Power by Simulation",
    "section": "The Hospital Drama Begins",
    "text": "The Hospital Drama Begins\nWelcome to St. Null’s Memorial Hospital, where a brand-new quality improvement intervention is about to be tested. The hospital’s network of clinics, collectively called The Null Distribution, is famous for its dedication to meticulous data collection—and also for some questionable statistical practices performed by a rather infamous staff member.\n\n\nCEO Barnaby Beta has championed a new, cost-intensive intervention aimed at improving patient satisfaction.\n\nNurse Random insists on conducting a proper randomized control trial (RCT), but quickly realizes Dr. P-Hacker might have already meddled with the initial power calculations.\n\nDr. P-Hacker is known to declare victory (“Significant at the 5% level!”) before even cleaning the data, and is rumored to own a golden “p &lt; 0.05” sign that he waves in staff meetings.\n\nDr. Doub R. Obust is the voice of reason, reminding everyone of fundamental statistical principles.\n\nIn this lab, you (the consultants) have been summoned to salvage the situation. Let’s see how this plays out."
  },
  {
    "objectID": "labs/lab-2-Power-sols.html#the-plot-thickens-power-calculations-without-clustering",
    "href": "labs/lab-2-Power-sols.html#the-plot-thickens-power-calculations-without-clustering",
    "title": "Lab 2: Power by Simulation",
    "section": "The Plot Thickens: Power Calculations Without Clustering",
    "text": "The Plot Thickens: Power Calculations Without Clustering\nScene 1: The Mysterious Spreadsheet\nNurse Random bursts into the conference room, clutching a color-coded spreadsheet.\n\nNurse Random: “Dr. P-Hacker, your power calculations look suspiciously high. Did you account for the fact that we’re randomizing by clinic, not by individual patient?”\n\n\nDr. P-Hacker: “Of course not, Nurse Random. Look, a random patient is a random patient—no matter which clinic they come from! Besides, I love high power. It makes me feel like my study can conquer the world!”\n\n\nDr. Doub R. Obust (rolling eyes): “We’re going to need to run a simulation that properly reflects how the intervention is assigned at the clinic level, not just among individual patients. Otherwise, your calculations will be as overconfident as your new P-value neon sign.”\n\nNonetheless, Dr. P-Hacker shares his “method” with you first. Let’s start by replicating his approach (ignoring clustering). This will be our baseline—what not to do if clinics are truly the unit of randomization.\nStep 1: Flawed Power Simulation (Ignoring Clustering)\nWe’ll simulate a dataset as if individual patients are randomly assigned to treatment or control. We’ll compute the statistical power for detecting a true treatment effect of a specified size, ignoring any clinic-level differences.\n\n\n\n\n\n\nTask 1: Flawed Simulation Setup\n\n\n\nFill in the code below to acheive a power of around 0.8 (may not be exact, but get as close as you can):\n\nSet a sample size for the total number of patients.\n\nSpecify a treatment effect (effect).\n\nDecide on the proportion of participants to receive treatment (prop).\n\nSelect a significance level (t_alpha).\n\nRun multiple simulations (sim.size).\n\nThen, run a linear regression on each simulated dataset, gather the p-values, and compute how often the null hypothesis is rejected (i.e., estimate power).\nNote: The code below is not executable. You need to change the eval: false to eval: true to make it work and fill in the blanks.\n\n\n\nlibrary(data.table)\nset.seed(123456)\n\n# Define parameters\nsample_size &lt;- 790     # e.g. 500\neffect &lt;- 0.2          # e.g. 0.2\nprop &lt;- 0.5            # e.g. 0.5\nt_alpha &lt;- 0.05        # e.g. 0.05\nsim.size &lt;- 2000       # e.g. 2000\n\n# Initialize storage for results\nreject_t &lt;- numeric(sim.size)\n\n# Simulation loop\nfor (i in 1:sim.size) {\n  dt &lt;- data.table(id = 1:sample_size)\n  \n  # Assign treatment individually (incorrect for cluster randomization!)\n  dt[, treatment := rbinom(.N, 1, prop)]\n  \n  # Simulate outcome (10 is baseline, 'effect' is added if treatment=1)\n  dt[, outcome := 10 + effect * treatment + rnorm(.N, mean = 0, sd = 1)]\n  \n  # Estimate the effect\n  fit &lt;- lm(outcome ~ treatment, data = dt)\n  p_value &lt;- summary(fit)$coefficients[2,4]\n  \n  # Check if we reject H0: (p-value &lt; alpha)\n  reject_t[i] &lt;- ifelse(p_value &lt; t_alpha, 1, 0)\n}\n\n# Compute estimated power\npower_flawed &lt;- mean(reject_t)\ncat(\"Estimated Power (Ignoring Clustering):\", power_flawed, \"\\n\")\n\nEstimated Power (Ignoring Clustering): 0.8035 \n\n\nDiscussion of Step 1\n\n\nNurse Random sighs: “We got an estimated power of 0.8 (yours might be slightly different). But do we trust this number?”\n\nDr. Doub R. Obust chimes in: “Nope. We’re ignoring that patients within the same clinic might be more similar to each other than to patients in other clinics. Our standard errors are artificially small.”\n\nAt this point, CEO Barnaby Beta perks up: “Artificially small standard errors? That means we’re basically claiming more precision than we actually have, right?”\nYes, indeed. If we disregard the clustering of patients, we risk making a Type I error more often than we realize. Dr. P-Hacker, in typical fashion, responds:\n\nDr. P-Hacker: “Type I error? Isn’t that just when we see something interesting that’s obviously there?!”Dr. Doub R. Obust (groaning): “No. A Type I error is a false positive—we conclude there is an effect even though, in truth, there isn’t. Like thinking you’ve discovered a rare golden banana flavor at the cafeteria soda machine when really it’s just seltzer water with a weird label!”"
  },
  {
    "objectID": "labs/lab-2-Power-sols.html#understanding-the-sources-of-uncertainty",
    "href": "labs/lab-2-Power-sols.html#understanding-the-sources-of-uncertainty",
    "title": "Lab 2: Power by Simulation",
    "section": "Understanding the Sources of Uncertainty",
    "text": "Understanding the Sources of Uncertainty\nBefore we fix our simulation, Dr. Doub R. Obust insists that you reflect on why ignoring clinic-level clustering is a problem. He ticks off sources of variability that a real experiment faces:\n\n\nSampling Variation: Even if you have a large population of patients, the sample you select is just one draw from a bigger population. Different samples might give different estimates.\n\n\nMeasurement Error: If your measurement tool for patient well-being is noisy (e.g., patients often mis-report how they feel, or staff record data incorrectly), it introduces extra variability that can muddy your effect estimates.\n\nVariance in Potential Outcomes: Not all patients respond to treatments in the same way. Some might be strongly affected, some not at all—leading to variation in the outcomes. This includes clustering: Patients in the same clinic share certain characteristics, environmental factors, or staff practices that can affect their potential outcomes. This correlation must be accounted for in the design and analysis.\n\n\nNurse Random: “So if we ignore that last point—clustering—our estimate of the variability (and thus the standard error) is off, and we might incorrectly claim significance. That’s basically p-hacking 101!”"
  },
  {
    "objectID": "labs/lab-2-Power-sols.html#power-calculations-with-clustering",
    "href": "labs/lab-2-Power-sols.html#power-calculations-with-clustering",
    "title": "Lab 2: Power by Simulation",
    "section": "Power Calculations With Clustering",
    "text": "Power Calculations With Clustering\nScene 2: A (Cluster-)Randomized Trial\nNow we come to the correct approach: our unit of randomization is the clinic, not the individual. That means each clinic either receives the intervention or does not, and all patients in a clinic share the same treatment assignment.\nStep 2: Incorporating Clustering\nWe’ll model:\n\n\nnum_clusters = number of clinics.\n\n\ncluster_size = number of patients per clinic.\n\n\nicc (Intraclass Correlation Coefficient): A measure of how strongly patients in the same clinic resemble each other. High ICC means patients within a clinic are more correlated.\n\nWe’ll generate clinic-level “random effects” and individual-level error terms to reflect both measurement error and the variability in potential outcomes across individuals.\n\n\n\n\n\n\n\nTask 2: Correcting the Simulation for Clustering\n\n\n\nFill in the code below to acheive a power of 0.8:\n\nDefine the number of clinics and the number of patients per clinic.\n\nAssign each entire clinic to treatment or control.\n\nIncorporate cluster-level random effects.\n\nFit the model but adjust standard errors for clustering.\n\n\n\n\n\nlibrary(multiwayvcov)  # for cluster-robust standard errors\nlibrary(lmtest)        # for coeftest\n\nLoading required package: zoo\n\n\n\nAttaching package: 'zoo'\n\n\nThe following objects are masked from 'package:data.table':\n\n    yearmon, yearqtr\n\n\nThe following objects are masked from 'package:base':\n\n    as.Date, as.Date.numeric\n\n# Define cluster parameters\nnum_clusters &lt;- 241    # e.g. 50\ncluster_size &lt;- 20     # e.g. 10\ntotal_sample &lt;- num_clusters * cluster_size\nicc &lt;- 0.2             # e.g. 0.4\n\n# Variances\nind_err_var &lt;- 1\n# cluster_err_var is derived from the intraclass correlation coefficient\ncluster_err_var &lt;- (icc * ind_err_var) / (1 - icc)\n\nsim.size &lt;- 2000        # e.g. 2000\neffect &lt;- 0.2           # e.g. 0.2\nprop &lt;- 0.5             # e.g. 0.5\nt_alpha &lt;- 0.05         # e.g. 0.05\n\nreject_t &lt;- numeric(sim.size)\n\nset.seed(654321)\nfor (i in 1:sim.size) {\n  \n  # Create a data table with one row per cluster, repeated for each patient\n  clusters &lt;- data.table(\n    cluster = rep(1:num_clusters, each = cluster_size)\n  )\n  \n  # Generate a cluster-level random effect\n  clusters[, cluster_error := rep(\n    rnorm(num_clusters, mean = 0, sd = sqrt(cluster_err_var)),\n    each = cluster_size\n  )]\n  \n  # Randomize at the clinic level\n  clusters[, treatment := rep(\n    rbinom(num_clusters, 1, prop),\n    each = cluster_size\n  )]\n  \n  # Add an individual-level error\n  clusters[, individual_error := rnorm(.N, mean = 0, sd = sqrt(ind_err_var))]\n  \n  # Final outcome for each individual\n  clusters[, outcome := 10 + effect * treatment + cluster_error + individual_error]\n  \n  # Fit a naive linear model (ignoring clustering in standard errors)\n  fit &lt;- lm(outcome ~ treatment, data = clusters)\n  \n  # Adjust standard errors for clustering\n  robust_SE &lt;- cluster.vcov(fit, clusters$cluster, df_correction = TRUE)\n  robust_coef &lt;- coeftest(fit, robust_SE)\n  \n  # Get the p-value for the treatment coefficient\n  p_value &lt;- robust_coef[2,4]\n  reject_t[i] &lt;- ifelse(p_value &lt; t_alpha, 1, 0)\n}\n\n# Compute estimated power with clustering\npower_clustered &lt;- mean(reject_t)\ncat(\"Estimated Power (Clustered):\", power_clustered, \"\\n\")\n\nEstimated Power (Clustered): 0.82 \n\n\nDiscussion of Step 2\nDr. Doub R. Obust looks at the new power estimate and remarks:\n\n“As you can see, once we account for clustering, the power is (usually) lower than in the flawed approach. That’s because those cluster-level similarities effectively reduce the amount of independent information we have. Our standard errors are bigger, so it’s harder to find significance unless the effect size or sample size is larger.”\n\nCEO Barnaby Beta shakes his head: “But that means our study might be underpowered if we stick to our current budget!”\nNurse Random replies with a grin: “Don’t worry, we can plan more carefully. Otherwise, if we run a smaller study, we risk a Type II error—failing to reject the null hypothesis when there really is an effect. You know, like leaving the gold standard intervention on the shelf because we didn’t gather enough data to prove it works.”\nDr. P-Hacker interjects:\n\n“And there’s always p &lt; 0.10, right? We can move our threshold for significance to get more ‘positive’ results!”Nurse Random (scolding): “That’s literally p-hacking. Please step away from the analysis, Doctor.”"
  },
  {
    "objectID": "labs/lab-2-Power-sols.html#the-truth-about-p-values",
    "href": "labs/lab-2-Power-sols.html#the-truth-about-p-values",
    "title": "Lab 2: Power by Simulation",
    "section": "The Truth About p-Values",
    "text": "The Truth About p-Values\nA quick comedic break for a lesson on p-values:\n\n\nDr. P-Hacker keeps shouting that p &lt; 0.05 means there’s a 5% chance the null hypothesis is true.\n\n\nNurse Random corrects him: “No, no, no. A p-value is the probability of observing a result at least as extreme as ours if the null is true. It’s NOT the probability the null is true. You can’t interpret it that way.”\n\n\nDr. P-Hacker: “I was so sure it was the probability that I was right.”Dr. Doub R. Obust: “We live and learn, Doctor.”"
  },
  {
    "objectID": "labs/lab-2-Power-sols.html#plotting-the-minimum-detectable-effect-mde",
    "href": "labs/lab-2-Power-sols.html#plotting-the-minimum-detectable-effect-mde",
    "title": "Lab 2: Power by Simulation",
    "section": "Plotting the Minimum Detectable Effect (MDE)",
    "text": "Plotting the Minimum Detectable Effect (MDE)\nBecause CEO Barnaby Beta wants to know how large an effect must be to have a reasonable chance of detection, you might want to simulate across a range of effect sizes and/or sample sizes. You can then plot the resulting power curves to see what the Minimum Detectable Effect (MDE) is for a given power requirement.\n\n\n\n\n\n\nTask 3: Create an MDE Plot\n\n\n\n\nLoop over a grid of effect sizes (or sample sizes).\n\nFor each value, compute power using the cluster-based simulation approach.\n\nPlot the effect size on the x-axis and the corresponding power on the y-axis.\n\n\n\n\nlibrary(ggplot2)\n\n# Simulation parameters for clustering\nnum_clusters &lt;- 241    # Number of clinics\ncluster_size &lt;- 20     # Patients per clinic\ntotal_sample &lt;- num_clusters * cluster_size\nicc &lt;- 0.2             # Intraclass correlation coefficient\n\n# Variance parameters\nind_err_var &lt;- 1\ncluster_err_var &lt;- (icc * ind_err_var) / (1 - icc)\n\nsim.size &lt;- 2000       # Number of simulation iterations per effect size\nprop &lt;- 0.5            # Proportion of clinics assigned to treatment\nt_alpha &lt;- 0.05        # Significance level\n\n# Define a grid of effect sizes\neffect_sizes &lt;- seq(0, 0.5, by = 0.05) \nresults &lt;- data.table(effect = effect_sizes, power = NA_real_)\n\nset.seed(072111)  # For reproducibility\n\n# Loop over each effect size\nfor (e in seq_along(effect_sizes)) {\n  current_effect &lt;- effect_sizes[e]\n  reject_t &lt;- numeric(sim.size)\n  \n  for (i in 1:sim.size) {\n    # Create a data table with one row per patient, with clinic identifier\n    clusters &lt;- data.table(cluster = rep(1:num_clusters, each = cluster_size))\n    \n    # Generate cluster-level random effects\n    clusters[, cluster_error := rep(rnorm(num_clusters, mean = 0, \n                                            sd = sqrt(cluster_err_var)), \n                                     each = cluster_size)]\n    \n    # Randomize treatment at the clinic level (all patients in a clinic get the same assignment)\n    clusters[, treatment := rep(rbinom(num_clusters, 1, prop), each = cluster_size)]\n    \n    # Add individual-level random error\n    clusters[, individual_error := rnorm(.N, mean = 0, sd = sqrt(ind_err_var))]\n    \n    # Generate the outcome variable: baseline of 10 plus treatment effect and both errors\n    clusters[, outcome := 10 + current_effect * treatment + cluster_error + individual_error]\n    \n    # Fit the linear model (naively, without clustering adjustments)\n    fit &lt;- lm(outcome ~ treatment, data = clusters)\n    \n    # Adjust standard errors for clustering\n    robust_SE &lt;- cluster.vcov(fit, clusters$cluster, df_correction = TRUE)\n    robust_coef &lt;- coeftest(fit, robust_SE)\n    \n    # Check if the p-value for the treatment coefficient is below the threshold\n    p_value &lt;- robust_coef[2, 4]\n    reject_t[i] &lt;- ifelse(p_value &lt; t_alpha, 1, 0)\n  }\n  \n  # Compute the estimated power (proportion of rejections) for this effect size\n  results$power[e] &lt;- mean(reject_t)\n}\n\n# Create the plot: Power vs. Effect Size\nggplot(results, aes(x = effect, y = power)) +\n  geom_line() +\n  geom_point() +\n  labs(\n    title = \"Power vs. Effect Size\",\n    x = \"Effect Size\",\n    y = \"Power\"\n  ) +\n  theme_minimal()"
  },
  {
    "objectID": "labs/lab-2-Power-sols.html#final-words-from-the-hospital-staff",
    "href": "labs/lab-2-Power-sols.html#final-words-from-the-hospital-staff",
    "title": "Lab 2: Power by Simulation",
    "section": "Final Words from the Hospital Staff",
    "text": "Final Words from the Hospital Staff\nCEO Barnaby Beta: “Alright, so if we need to ensure we have enough clinics and participants to achieve our desired power, we may need a bigger budget than expected. Let’s not forget that ignoring clustering would have given us a false sense of security in our power. Now we know better.”\nNurse Random: “And no more Type I or Type II error confusion, Dr. P-Hacker. We must keep our definitions straight if we’re to have any credibility around here!”\nDr. P-Hacker (sighing): “Fine, fine. Guess I’ll tone down the p-value hype. But I’m keeping my neon sign.”\nDr. Doub R. Obust (with a grin): “We’ll allow the neon sign, as long as you promise to interpret it correctly.”"
  },
  {
    "objectID": "labs/lab-2-Power-sols.html#submission-instructions",
    "href": "labs/lab-2-Power-sols.html#submission-instructions",
    "title": "Lab 2: Power by Simulation",
    "section": "Submission Instructions",
    "text": "Submission Instructions\n\nMake sure your .qmd file knits successfully (to .pdf or .html).\n\nUpload your compiled document to Gradescope.\n\n\nInclude your discussion of the results, your code, and your responses to the callout sections.\n\nRemember, the moral of the story: Always check who (or what) is being randomized, and account for clustering when necessary! Otherwise, your study conclusions might be as random as Dr. P-Hacker’s next dinner choice."
  },
  {
    "objectID": "labs/lab-1-InternalValidityPO_sols.html",
    "href": "labs/lab-1-InternalValidityPO_sols.html",
    "title": "Lab 1 Solutions",
    "section": "",
    "text": "Results may differ\n\n\n\nThis is only one of many possible ways to complete this lab. Your final code may look different, which is fine! In fact, it is good practice to experiment with different approaches."
  },
  {
    "objectID": "labs/lab-1-InternalValidityPO_sols.html#overview-and-learning-objectives",
    "href": "labs/lab-1-InternalValidityPO_sols.html#overview-and-learning-objectives",
    "title": "Lab 1 Solutions",
    "section": "Overview and Learning Objectives",
    "text": "Overview and Learning Objectives\nIn this lab, we will explore internal validity and the potential outcomes framework using simulated health data from the endlessly eventful St. Null’s Memorial Hospital. Specifically, we will recreate a scenario where a new intervention (putting patients on ventilators) may or may not reduce patient mortality. As you’ll discover, chaos at the hospital has made it far from straightforward to identify causal effects.\nBy the end of this lab, you will be able to:\n\nUnderstand the concept of potential outcomes and causal effects.\nApply randomization inference to estimate treatment effects.\nIdentify threats to internal validity and explore possible solutions.\nImplement basic difference-in-means estimation in R.\nUse the WebR package to interactively run and modify code."
  },
  {
    "objectID": "labs/lab-1-InternalValidityPO_sols.html#instructions",
    "href": "labs/lab-1-InternalValidityPO_sols.html#instructions",
    "title": "Lab 1 Solutions",
    "section": "Instructions",
    "text": "Instructions\n\nOpen this .qmd file in RStudio or another Quarto-supported editor.\nFollow the guided coding prompts below, completing the missing code blocks.\nSubmit your completed lab on Gradescope [Insert Link Here] by [Insert Deadline Here]."
  },
  {
    "objectID": "labs/lab-1-InternalValidityPO_sols.html#scenario",
    "href": "labs/lab-1-InternalValidityPO_sols.html#scenario",
    "title": "Lab 1 Solutions",
    "section": "Scenario",
    "text": "Scenario\nThe Hospital of Uncertain Outcomes\nWelcome to St. Null’s Memorial Hospital—an institution where the only constant is confusion. The hospital board—led by the well-meaning but trend-obsessed CEO, Barnaby Beta—changes policies so often that nobody knows what’s going on.\nWorse yet is Dr. P-Hacker, a “data guru” who prefers p-values to patients. He mines the electronic health records (EHR) until something (anything!) is “significant.” Meanwhile, Nurse Random tries to keep everything on track, pointing out that good causal methods can be more important than good vibes. Lastly, Dr. Doub R. Obust lurks in the background, waiting for a chance to champion doubly robust methods that might someday save everyone’s sanity.\n\n\n\n\nYou and your team of budding methodologists are the new consultants hired to impose some order on this bedlam. In each module, you’ll tackle another fiasco at St. Null’s—from overfitted AI catastrophes to weird missing-data mishaps—and attempt to restore some semblance of methodological rigor. Good luck!"
  },
  {
    "objectID": "labs/lab-1-InternalValidityPO_sols.html#your-mission",
    "href": "labs/lab-1-InternalValidityPO_sols.html#your-mission",
    "title": "Lab 1 Solutions",
    "section": "Your Mission",
    "text": "Your Mission\nThe Pandemic Mystery at St. Null’s\nA mysterious respiratory illness has swept through St. Null’s, leaving every ward scrambling. The question at hand is whether putting these patients on ventilators prolongs their lifespans. CEO Barnaby Beta wants quick answers (“If TikTok can do it, so can we!”). Dr. P-Hacker gleefully promises “instant significance,” claiming all he needs is the hospital’s EHR from the past week.\nBut Nurse Random, unimpressed, insists that the hospital’s chaotic, ad hoc ventilator assignments will cloud any conclusions. Dr. Doub R. Obust nods knowingly. They call in your team for an unbiased, data-driven approach.\nIn this lab, you’ll simulate a dataset of 100,000 patients that captures both “potential outcomes” (i.e., what would happen if a patient were ventilated vs. not ventilated). This magical glimpse at parallel universes is impossible in real-world data, but here it will let us see exactly how different analytic approaches fare in the face of selection bias."
  },
  {
    "objectID": "labs/lab-1-InternalValidityPO_sols.html#step-1-simulating-the-dataset",
    "href": "labs/lab-1-InternalValidityPO_sols.html#step-1-simulating-the-dataset",
    "title": "Lab 1 Solutions",
    "section": "Step 1: Simulating the Dataset",
    "text": "Step 1: Simulating the Dataset\nBecause the EHR system at St. Null’s is, in a word, “unreliable” (or, in two words, “utterly broken”), we’ll create our own dataset in R.\nRun the following code to generate 100,000 patient records along with potential outcomes (y0 if no ventilator, y1 if ventilated). Each outcome is the patient’s lifespan (in some made-up units). Note that lifespans below zero are set to zero—any negative numbers would just be an artifact of Dr. P-Hacker’s bizarre data extraction methods.\n\nlibrary(fixest)\nlibrary(data.table)\nset.seed(072121)\n\n# 100,000 people with differing levels of covid symptoms\nN_people = 100000\ndf = data.table(person = 1:N_people)\n\n# Potential outcomes (Y0): life-span if no vent\ndf[, y0 := rnorm(N_people, 9.4, 4)]\ndf[y0 &lt; 0, y0 := 0]\n\n# Potential outcomes (Y1): life-span if assigned to vents\ndf[, y1 := rnorm(N_people, 10, 4)]\ndf[y1 &lt; 0, y1 := 0]\n\n\n\n\n\n\n\nExplanation:\n\n\n\n\nThe necessary packages (fixest and data.table) are loaded.\nThe set.seed(072121) ensures reproducibility, meaning the random numbers generated will be the same every time the code is run.\nA dataset df is created using data.table with 100,000 individuals (each represented by a row).\n\nSimulating Potential Outcomes:\n\n\ny0 represents the expected lifespan (in years) if no ventilator treatment is given. It is drawn from a normal distribution with:\n\nMean: 9.4 years\nStandard deviation: 4 years\n\n\nAny y0 values below 0 (i.e., negative lifespans) are set to zero.\ny1 represents the expected lifespan if assigned to a ventilator. It is similarly drawn from a normal distribution but with a slightly higher mean of 10 years.\nAgain, negative lifespan values are replaced with 0.\n\n\n\n\n\nWe also define the individual treatment effect for each patient. Dr. Doub R. Obust is thrilled, because for once, we have both y0 and y1 simultaneously—an impossible dream in real life!\n\n# Define individual treatment effect\ndf[, delta := y1 - y0]\n\n\n\n\n\n\n\nExplanation:\n\n\n\n\nA new column delta is created, which represents the individual treatment effect (ITE) for each person.\nThe ITE is calculated as the difference between the potential outcome under treatment (y1) and the potential outcome under control (y0).\nThis measures how much additional lifespan (if any) the ventilator treatment provides.\n\n\n\n\n\n\n\n\n\nAlternative Approach\n\n\n\n\n\nThe column can be created using mutate() from dplyr if using tidyverse:\nlibrary(dplyr)\ndf &lt;- df %&gt;% mutate(delta = y1 - y0)"
  },
  {
    "objectID": "labs/lab-1-InternalValidityPO_sols.html#step-2-the-two-doctors",
    "href": "labs/lab-1-InternalValidityPO_sols.html#step-2-the-two-doctors",
    "title": "Lab 1 Solutions",
    "section": "Step 2: The Two Doctors",
    "text": "Step 2: The Two Doctors\nSt. Null’s has two very different doctors assigning ventilators:\n\n\nDr. Perfect: A mystical being who can see into each patient’s future and only gives ventilators to those who would benefit from them.\n\nDr. Bad: The name says it all. This doctor assigns ventilators randomly—could be beneficial, could not be. Who knows?\n\nStep 2a: Assigning Doctors\nFirst, we randomly assign each patient to one of these two doctors. (No wonder this hospital is chaotic…)\n\n# Assign doctors randomly\ndf[, doctor := sample(c(\"perfect\", \"bad\"), N_people, replace = TRUE)]\n\n\n\n\n\n\n\nExplanation\n\n\n\n\n\nsample(c(\"perfect\", \"bad\"), N_people, replace = TRUE)\n\nsample(x, size, replace) randomly selects size elements from the vector x.\nHere, x = c(\"perfect\", \"bad\"), meaning we are choosing between \"perfect\" and \"bad\".\nsize = N_people ensures that we assign a doctor to all N_people individuals.\nreplace = TRUE allows values to be selected independently, meaning each person is assigned a doctor without affecting others.\n\n\n\ndf[, doctor := ...] (Data.table Syntax)\n\ndf[, column_name := value] is data.table’s syntax for adding or modifying a column.\nHere, doctor is created and populated with \"perfect\" or \"bad\" based on the sample() function.\n\n\n\n\n\n\n\n\n\n\n\nAlternative Approach:\n\n\n\n\n\nUsing mutate() from dplyr:\ndf &lt;- df %&gt;%\n  mutate(doctor = sample(c(\"perfect\", \"bad\"), N_people, replace = TRUE))\n\n\nInstead of using sample(), one could use runif() :\ndf[, doctor := ifelse(runif(N_people) &gt; 0.5, \"perfect\", \"bad\")]\nThis approach provides more flexibility if you want to adjust the probability of assigning each type of doctor.\n\n\n\n\nStep 2b: Assigning Ventilators\nNext, each doctor does what they do best. Dr. Perfect uses clairvoyance to treat only those who stand to gain (delta &gt; 0). Dr. Bad flips a metaphorical coin:\n\n# Perfect doctor assigns vents only to those who benefit\ndf[doctor == \"perfect\", vents := (delta &gt; 0)]\n\n# Random doctor assigns vents randomly\ndf[doctor == \"bad\", vents := sample(c(TRUE, FALSE), .N, replace = TRUE)]\n\n\n\n\n\n\n\nExplanation:\n\n\n\n\n\ndf[doctor == \"perfect\", vents := (delta &gt; 0)]\n\ndf[...] is data.table’s way of selecting rows where doctor == \"perfect\".\nvents := (delta &gt; 0) assigns TRUE if delta &gt; 0, meaning treatment is given if the patient benefits.\nThe := operator modifies the column in place, making it more memory-efficient than base R.\n\n\n\ndf[doctor == \"bad\", vents := sample(c(TRUE, FALSE), .N, replace = TRUE)]\n\n.N represents the number of rows in the subset (doctor == \"bad\"), ensuring the right number of values is generated.\nsample(c(TRUE, FALSE), .N, replace = TRUE) randomly assigns TRUE (ventilator given) or FALSE (no ventilator) to these individuals.\nEach person is assigned independently due to replace = TRUE.\n\n\n\n\n\nIt’s not exactly a model of ethical clarity, but it certainly demonstrates the complications of “treatment assignment” in the real world (or the real St. Null’s world)."
  },
  {
    "objectID": "labs/lab-1-InternalValidityPO_sols.html#step-3-computing-causal-parameters",
    "href": "labs/lab-1-InternalValidityPO_sols.html#step-3-computing-causal-parameters",
    "title": "Lab 1 Solutions",
    "section": "Step 3: Computing Causal Parameters",
    "text": "Step 3: Computing Causal Parameters\nNow, let’s calculate the key causal parameters:\n\n\nAverage Treatment Effect (ATE): The overall difference in outcomes if everyone were ventilated vs. if no one were ventilated.\n\nAverage Treatment Effect on the Treated (ATT): The effect of ventilation on those who actually received ventilation.\n\nAverage Treatment Effect on the Untreated (ATU): The effect of ventilation on those who were not ventilated.\n\n\n# Calculate all aggregate Causal Parameters (ATE, ATT, ATU)\nate = df[, mean(delta)]\natt = df[vents == TRUE, mean(delta)]\natu = df[vents == FALSE, mean(delta)]\n\ncat(sprintf(\"ATE = %.03f\n\", ate))\n\nATE = 0.602\n\ncat(sprintf(\"ATT = %.03f\n\", att))\n\nATT = 2.748\n\ncat(sprintf(\"ATU = %.03f\n\", atu))\n\nATU = -1.711\n\n\n\n\n\n\n\n\nExplanation:\n\n\n\n\n\ndf[, mean(delta)]\n\nmean(delta) computes the average of delta, which represents the Average Treatment Effect (ATE).\nSince no filtering is applied, it considers all individuals in the dataset.\n\n\n\ndf[vents == TRUE, mean(delta)]\n\ndf[...] selects rows where vents == TRUE (patients who received ventilation).\nmean(delta) then computes the Average Treatment Effect on the Treated (ATT).\n\n\n\ndf[vents == FALSE, mean(delta)]\n\nThis selects individuals who were not treated (vents == FALSE).\nmean(delta) calculates the Average Treatment Effect on the Untreated (ATU).\n\n\n\ncat(sprintf(\"ATE = %.03f\\n\", ate))\n\nsprintf(\"ATE = %.03f\\n\", ate) formats ate to 3 decimal places.\ncat() prints the formatted result to the console.\n\n\n\n\n\nDr. P-Hacker would stop right here and rejoice: “We have all the significance we need!” But hold your celebratory balloon drop—there’s more to do."
  },
  {
    "objectID": "labs/lab-1-InternalValidityPO_sols.html#step-4-selection-bias-and-realized-outcomes",
    "href": "labs/lab-1-InternalValidityPO_sols.html#step-4-selection-bias-and-realized-outcomes",
    "title": "Lab 1 Solutions",
    "section": "Step 4: Selection Bias and Realized Outcomes",
    "text": "Step 4: Selection Bias and Realized Outcomes\nAlthough the dataset has both y0 and y1, in the real world, a patient’s outcome is observed under only one condition (treated or untreated). Let’s capture which outcome we’d actually see based on the ventilator assignment:\n\n# Use the switching equation to select realized outcomes from potential outcomes based on treatment assignment\ndf[, y := vents * y1 + (1 - vents) * y0]\n\n\n\n\n\n\n\nExplanation :\n\n\n\n\n\nvents * y1 + (1 - vents) * y0\n\nThis applies the switching equation, which determines the observed outcome (y) based on whether an individual received treatment.\nIf vents == TRUE (1), the observed outcome is y1 (the treated potential outcome).\nIf vents == FALSE (0), the observed outcome is y0 (the untreated potential outcome).\n\nMathematically, this follows:\n\ny=vents×y1+(1−vents)×y0\n\n\n\n\n\ndf[, y := ...]\n\nThis is data.table syntax for creating or modifying a column in place.\nThe new column y represents the observed lifespan for each individual.\n\n\n\n\n\nSelection Bias Calculation\nWe’ll see if there is selection bias by comparing the expected lifespan of ventilated patients had they not been ventilated to the expected lifespan of non-ventilated patients.\n\n# Calculate EY0 for vent group and no vent group\ney01 = df[vents == TRUE, mean(y0)]  \ney00 = df[vents == FALSE, mean(y0)] \n\n# Calculate selection bias\nselection_bias = (ey01 - ey00)\n\ncat(sprintf(\n  \"Selection Bias = %.03f - %.03f = %.03f \n\", \n  ey01, ey00, selection_bias\n))\n\nSelection Bias = 8.334 - 10.574 = -2.240 \n\n\n\n\n\n\n\n\nDetailed Explanation of Commands:\n\n\n\n\n\ndf[vents == TRUE, mean(y0)]\n\nThis calculates the expected y0 (lifespan without ventilation) for people who actually received a ventilator.\nIt measures the counterfactual lifespan if the treated group had not received treatment.\n\n\n\ndf[vents == FALSE, mean(y0)]\n\nThis calculates the expected y0 for people who did not receive a ventilator.\nIt represents their actual untreated lifespan.\n\n\n\nSelection Bias Calculation\n\nselection_bias = (ey01 - ey00) compares these two means.\nIf selection into treatment is not random, then the untreated potential outcome (y0) may differ between the treated and untreated groups.\n\n\n\nFormatted Output Using sprintf()\n\nsprintf() formats the output to three decimal places.\ncat() prints the formatted text to the console.\n\n\n\n\n\nIf Dr. Perfect is involved, we’d expect some big differences here. Dr. P-Hacker would probably ignore that and claim victory anyway. (He likes ignoring inconvenient truths.)"
  },
  {
    "objectID": "labs/lab-1-InternalValidityPO_sols.html#step-5-comparing-outcomes-between-groups",
    "href": "labs/lab-1-InternalValidityPO_sols.html#step-5-comparing-outcomes-between-groups",
    "title": "Lab 1 Solutions",
    "section": "Step 5: Comparing Outcomes Between Groups",
    "text": "Step 5: Comparing Outcomes Between Groups\nOne of the simplest ways to estimate the treatment effect is to look at the Simple Difference in Outcomes (SDO)—the difference in the observed mean outcome between those who got the treatment (ventilators) and those who did not.\n\n# Calculate the share of units treated with vents (pi)\npi = mean(df$vents)\n\n# Manually calculate the simple difference in mean health outcomes\ney1 = df[vents == TRUE, mean(y)]\ney0 = df[vents == FALSE, mean(y)]\nsdo = ey1 - ey0\n\ncat(sprintf(\n  \"Simple Difference-in-Outcomes = %.03f - %.03f = %.03f \n\", \n  ey1, ey0, sdo\n))\n\nSimple Difference-in-Outcomes = 11.082 - 10.574 = 0.509 \n\n\n\n\n\n\n\n\nExplanation:\n\n\n\n\n\npi = mean(df$vents)\n\ndf$vents extracts the vents column as a vector.\nmean(df$vents) computes the proportion of individuals who received ventilation (TRUE is treated as 1, FALSE as 0).\nThis provides π (pi), the treatment probability.\n\n\n\ndf[vents == TRUE, mean(y)]\n\nCalculates the mean observed outcome y (lifespan) for the treated group.\n\n\n\ndf[vents == FALSE, mean(y)]\n\nCalculates the mean observed outcome y for the untreated group.\n\n\n\nSimple Difference in Outcomes (SDO)\n\nsdo = ey1 - ey0 computes the naive difference in means.\nThis is an unadjusted estimate of the treatment effect, which may be biased if treatment selection was non-random.\n\n\n\n\n\nDr. P-Hacker would run around shouting: “Aha! This difference proves the intervention works!” or “Aha! It’s not significant!” depending on the p-value. Let’s see if we can do better."
  },
  {
    "objectID": "labs/lab-1-InternalValidityPO_sols.html#step-6-estimating-the-effect-with-regression",
    "href": "labs/lab-1-InternalValidityPO_sols.html#step-6-estimating-the-effect-with-regression",
    "title": "Lab 1 Solutions",
    "section": "Step 6: Estimating the Effect with Regression",
    "text": "Step 6: Estimating the Effect with Regression\nTo confirm our findings, let’s do an Ordinary Least Squares (OLS) regression of the realized outcome on the ventilator indicator:\n\n# Estimate the treatment effect using OLS\nreg = feols(\n  y ~ vents, data = df, \n  vcov = \"hc1\"\n)\n\ncat(\"\n\")\nprint(reg)\n\nOLS estimation, Dep. Var.: y\nObservations: 100,000\nStandard-errors: Heteroskedasticity-robust \n             Estimate Std. Error  t value  Pr(&gt;|t|)    \n(Intercept) 10.573703   0.017534 603.0511 &lt; 2.2e-16 ***\nventsTRUE    0.508554   0.024209  21.0072 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 3.82331   Adj. R2: 0.004388\n\n\n\n\n\n\n\n\nExplanation :\n\n\n\n\n\nfeols(y ~ vents, data = df, vcov = \"hc1\")\n\nfeols() from the fixest package estimates an Ordinary Least Squares (OLS) regression.\nThe formula y ~ vents models the observed outcome (y) as a function of treatment (vents).\nThis provides an estimate of the average treatment effect (ATE) under the assumption of unconfoundedness (i.e., no omitted variables affecting both treatment and outcome).\nvcov = \"hc1\" specifies robust standard errors (Huber-White correction), which adjust for heteroskedasticity in the residuals.\n\n\n\n\n\n\n\n\n\n\n\nAlternative Approach:\n\n\n\n\n\nUsing tidyverse with broom:\nlibrary(broom)\nreg %&gt;% tidy()\n\n\nIf clustering standard errors is needed:\nreg = feols(y ~ vents, data = df, vcov = \"cluster\")\n\n\n\n\nThis approach, while straightforward, is still subject to any biases from non-random assignment—like, say, Dr. Perfect or Dr. Bad being in charge."
  },
  {
    "objectID": "labs/lab-1-InternalValidityPO_sols.html#step-7-validating-the-decomposition",
    "href": "labs/lab-1-InternalValidityPO_sols.html#step-7-validating-the-decomposition",
    "title": "Lab 1 Solutions",
    "section": "Step 7: Validating the Decomposition",
    "text": "Step 7: Validating the Decomposition\nAt this point, we’d like to see if the Simple Difference in Outcomes (SDO) can be broken down into our measured parameters. You’ll fill in a table comparing Dr. Perfect to Dr. Bad, computing the ATE, ATT, ATU, selection bias, SDO, and so on.\nBelow is a quick consistency check to see if the SDO lines up with our theoretical decomposition:\n\n# Decomposition check\nsdo_check = ate + selection_bias + (1 - pi) * (att - atu)\n\ncat(sprintf(\"SDO Check = %.03f \n\", sdo_check))\n\nSDO Check = 0.509 \n\n\n\n\n\n\n\n\nDetailed Explanation of Commands:\n\n\n\n\n\nDecomposition Formula:\n\n\nThe simple difference in outcomes (SDO) can be decomposed into:\n = + + (1 - ) ( - )\n\nThis checks whether our earlier calculations are consistent with theoretical expectations.\n\n\n\nsdo_check = ate + selection_bias + (1 - pi) * (att - atu)\n\nate: The overall average treatment effect.\nselection_bias: The difference in untreated potential outcomes between treated and untreated individuals.\n(1 - pi) * (att - atu): Adjusts for differences in treatment assignment proportions.\n\n\n\n\n\nWhen you fill out the table, you should include:\n\n\nPerfect Doctor\nBad Doctor\nCausal Parameter\n\n\n\nATE\n\n\n\n\nATT\n\n\n\n\nATU\n\n\n\n\nSelection bias terms\n\n\n\n\nE[Y0 | D=1]\n\n\n\n\nE[Y0 | D=0]\n\n\n\n\nSelection bias\n\n\n\n\nCalculations\n\n\n\n\nPi (share on vents)\n\n\n\n\nSDO manual\n\n\n\n\nSDO OLS\n\n\n\n\nSDO Decomposition\n\n\n\n\nObs\n\n\n\n\n\nReflection\nFinally, reflect on these questions:\n\nIs the Simple Difference in Outcomes (SDO) enough to identify the ATE, ATT, or ATU?\nHow does selection bias play into interpreting the SDO?\nWhat lessons would Nurse Random want Dr. P-Hacker to learn about data and design?\n\n(Extra credit if you can imagine the dramatic showdown when Dr. Doub R. Obust finally unveils a doubly robust method that saves the day—but that’s a story for a future lab!)\nThat’s it! You’ve run the gauntlet of the Hospital of Uncertain Outcomes and lived to tell the tale. Now submit your work, and good luck diagnosing more of St. Null’s methodological maladies!"
  },
  {
    "objectID": "labs/lab-1-InternalValidityPO.html",
    "href": "labs/lab-1-InternalValidityPO.html",
    "title": "Lab 1",
    "section": "",
    "text": "In this lab, we will explore internal validity and the potential outcomes framework using simulated health data from the endlessly eventful St. Null’s Memorial Hospital. Specifically, we will recreate a scenario where a new intervention (putting patients on ventilators) may or may not reduce patient mortality. As you’ll discover, chaos at the hospital has made it far from straightforward to identify causal effects.\nBy the end of this lab, you will be able to:\n\nUnderstand the concept of potential outcomes and causal effects.\nApply randomization inference to estimate treatment effects.\nIdentify threats to internal validity and explore possible solutions.\nImplement basic difference-in-means estimation in R.\nUse the WebR package to interactively run and modify code.\n\n\nOpen this .qmd file in RStudio or another Quarto-supported editor.\nFollow the guided coding prompts below, completing the missing code blocks.\nSubmit your completed lab on Gradescope by Feb 3 at 11:59pm.\n\n\nWelcome to St. Null’s Memorial Hospital—an institution where the only constant is confusion. The hospital board—led by the well-meaning but trend-obsessed CEO, Barnaby Beta—changes policies so often that nobody knows what’s going on.\nWorse yet is Dr. P-Hacker, a “data guru” who prefers p-values to patients. He mines the electronic health records (EHR) until something (anything!) is “significant.” Meanwhile, Nurse Random tries to keep everything on track, pointing out that good causal methods can be more important than good vibes. Lastly, Dr. Doub R. Obust lurks in the background, waiting for a chance to champion doubly robust methods that might someday save everyone’s sanity.\n\n\n\n\nYou and your team of budding methodologists are the new consultants hired to impose some order on this bedlam. In each module, you’ll tackle another fiasco at St. Null’s—from overfitted AI catastrophes to weird missing-data mishaps—and attempt to restore some semblance of methodological rigor. Good luck!\n\n\nA mysterious respiratory illness has swept through St. Null’s, leaving every ward scrambling. The question at hand is whether putting these patients on ventilators prolongs their lifespans. CEO Barnaby Beta wants quick answers (“If TikTok can do it, so can we!”). Dr. P-Hacker gleefully promises “instant significance,” claiming all he needs is the hospital’s EHR from the past week.\nBut Nurse Random, unimpressed, insists that the hospital’s chaotic, ad hoc ventilator assignments will cloud any conclusions. Dr. Doub R. Obust nods knowingly. They call in your team for an unbiased, data-driven approach.\nIn this lab, you’ll simulate a dataset of 100,000 patients that captures both “potential outcomes” (i.e., what would happen if a patient were ventilated vs. not ventilated). This magical glimpse at parallel universes is impossible in real-world data, but here it will let us see exactly how different analytic approaches fare in the face of selection bias.",
    "crumbs": [
      "Unit 1: Internal Validity",
      "Lab 1: The Hospital of Uncertain Outcomes"
    ]
  },
  {
    "objectID": "labs/lab-1-InternalValidityPO.html#overview-and-learning-objectives",
    "href": "labs/lab-1-InternalValidityPO.html#overview-and-learning-objectives",
    "title": "Lab 1",
    "section": "",
    "text": "In this lab, we will explore internal validity and the potential outcomes framework using simulated health data from the endlessly eventful St. Null’s Memorial Hospital. Specifically, we will recreate a scenario where a new intervention (putting patients on ventilators) may or may not reduce patient mortality. As you’ll discover, chaos at the hospital has made it far from straightforward to identify causal effects.\nBy the end of this lab, you will be able to:\n\nUnderstand the concept of potential outcomes and causal effects.\nApply randomization inference to estimate treatment effects.\nIdentify threats to internal validity and explore possible solutions.\nImplement basic difference-in-means estimation in R.\nUse the WebR package to interactively run and modify code.",
    "crumbs": [
      "Unit 1: Internal Validity",
      "Lab 1: The Hospital of Uncertain Outcomes"
    ]
  },
  {
    "objectID": "labs/lab-1-InternalValidityPO.html#instructions",
    "href": "labs/lab-1-InternalValidityPO.html#instructions",
    "title": "Lab 1",
    "section": "",
    "text": "Open this .qmd file in RStudio or another Quarto-supported editor.\nFollow the guided coding prompts below, completing the missing code blocks.\nSubmit your completed lab on Gradescope by Feb 3 at 11:59pm.",
    "crumbs": [
      "Unit 1: Internal Validity",
      "Lab 1: The Hospital of Uncertain Outcomes"
    ]
  },
  {
    "objectID": "labs/lab-1-InternalValidityPO.html#scenario",
    "href": "labs/lab-1-InternalValidityPO.html#scenario",
    "title": "Lab 1",
    "section": "",
    "text": "Welcome to St. Null’s Memorial Hospital—an institution where the only constant is confusion. The hospital board—led by the well-meaning but trend-obsessed CEO, Barnaby Beta—changes policies so often that nobody knows what’s going on.\nWorse yet is Dr. P-Hacker, a “data guru” who prefers p-values to patients. He mines the electronic health records (EHR) until something (anything!) is “significant.” Meanwhile, Nurse Random tries to keep everything on track, pointing out that good causal methods can be more important than good vibes. Lastly, Dr. Doub R. Obust lurks in the background, waiting for a chance to champion doubly robust methods that might someday save everyone’s sanity.\n\n\n\n\nYou and your team of budding methodologists are the new consultants hired to impose some order on this bedlam. In each module, you’ll tackle another fiasco at St. Null’s—from overfitted AI catastrophes to weird missing-data mishaps—and attempt to restore some semblance of methodological rigor. Good luck!",
    "crumbs": [
      "Unit 1: Internal Validity",
      "Lab 1: The Hospital of Uncertain Outcomes"
    ]
  },
  {
    "objectID": "labs/lab-1-InternalValidityPO.html#your-mission",
    "href": "labs/lab-1-InternalValidityPO.html#your-mission",
    "title": "Lab 1",
    "section": "",
    "text": "A mysterious respiratory illness has swept through St. Null’s, leaving every ward scrambling. The question at hand is whether putting these patients on ventilators prolongs their lifespans. CEO Barnaby Beta wants quick answers (“If TikTok can do it, so can we!”). Dr. P-Hacker gleefully promises “instant significance,” claiming all he needs is the hospital’s EHR from the past week.\nBut Nurse Random, unimpressed, insists that the hospital’s chaotic, ad hoc ventilator assignments will cloud any conclusions. Dr. Doub R. Obust nods knowingly. They call in your team for an unbiased, data-driven approach.\nIn this lab, you’ll simulate a dataset of 100,000 patients that captures both “potential outcomes” (i.e., what would happen if a patient were ventilated vs. not ventilated). This magical glimpse at parallel universes is impossible in real-world data, but here it will let us see exactly how different analytic approaches fare in the face of selection bias.",
    "crumbs": [
      "Unit 1: Internal Validity",
      "Lab 1: The Hospital of Uncertain Outcomes"
    ]
  },
  {
    "objectID": "labs/lab-1-InternalValidityPO.html#step-1-simulating-the-dataset",
    "href": "labs/lab-1-InternalValidityPO.html#step-1-simulating-the-dataset",
    "title": "Lab 1",
    "section": "Step 1: Simulating the Dataset",
    "text": "Step 1: Simulating the Dataset\nBecause the EHR system at St. Null’s is, in a word, “unreliable” (or, in two words, “utterly broken”), we’ll create our own dataset in R.\nRun the following code to generate 100,000 patient records along with potential outcomes (y0 if no ventilator, y1 if ventilated). Each outcome is the patient’s lifespan (in some made-up units). Note that lifespans below zero are set to zero—any negative numbers would just be an artifact of Dr. P-Hacker’s bizarre data extraction methods.\n\nlibrary(fixest)\nlibrary(data.table)\nset.seed(20200403)\n\n# 100,000 people with differing levels of covid symptoms\nN_people = 100000\ndf = data.table(person = 1:N_people)\n\n# Potential outcomes (Y0): life-span if no vent\ndf[, y0 := rnorm(N_people, 9.4, 4)]\ndf[y0 &lt; 0, y0 := 0]\n\n# Potential outcomes (Y1): life-span if assigned to vents\ndf[, y1 := rnorm(N_people, 10, 4)]\ndf[y1 &lt; 0, y1 := 0]\n\nWe also define the individual treatment effect for each patient. Dr. Doub R. Obust is thrilled, because for once, we have both y0 and y1 simultaneously—an impossible dream in real life!\n\n# Define individual treatment effect",
    "crumbs": [
      "Unit 1: Internal Validity",
      "Lab 1: The Hospital of Uncertain Outcomes"
    ]
  },
  {
    "objectID": "labs/lab-1-InternalValidityPO.html#step-2-the-two-doctors",
    "href": "labs/lab-1-InternalValidityPO.html#step-2-the-two-doctors",
    "title": "Lab 1",
    "section": "Step 2: The Two Doctors",
    "text": "Step 2: The Two Doctors\nSt. Null’s has two very different doctors assigning ventilators:\n\n\nDr. Perfect: A mystical being who can see into each patient’s future and only gives ventilators to those who would benefit from them.\n\nDr. Bad: The name says it all. This doctor assigns ventilators randomly—could be beneficial, could not be. Who knows?\n\nStep 2a: Assigning Doctors\nFirst, we randomly assign each patient to one of these two doctors. (No wonder this hospital is chaotic…)\n\n# Assign doctors randomly\n\nStep 2b: Assigning Ventilators\nNext, each doctor does what they do best. Dr. Perfect uses clairvoyance to treat only those who stand to gain (delta &gt; 0). Dr. Bad flips a metaphorical coin:\n\n# Perfect doctor assigns vents only to those who benefit\n\n\n# Random doctor assigns vents randomly\n\nIt’s not exactly a model of ethical clarity, but it certainly demonstrates the complications of “treatment assignment” in the real world (or the real St. Null’s world).",
    "crumbs": [
      "Unit 1: Internal Validity",
      "Lab 1: The Hospital of Uncertain Outcomes"
    ]
  },
  {
    "objectID": "labs/lab-1-InternalValidityPO.html#step-3-computing-causal-parameters",
    "href": "labs/lab-1-InternalValidityPO.html#step-3-computing-causal-parameters",
    "title": "Lab 1",
    "section": "Step 3: Computing Causal Parameters",
    "text": "Step 3: Computing Causal Parameters\nNow, let’s calculate the key causal parameters:\n\n\nAverage Treatment Effect (ATE): The overall difference in outcomes if everyone were ventilated vs. if no one were ventilated.\n\nAverage Treatment Effect on the Treated (ATT): The effect of ventilation on those who actually received ventilation.\n\nAverage Treatment Effect on the Untreated (ATU): The effect of ventilation on those who were not ventilated.\n\n\n# Calculate all aggregate Causal Parameters (ATE, ATT, ATU)\n\nDr. P-Hacker would stop right here and rejoice: “We have all the significance we need!” But hold your celebratory balloon drop—there’s more to do.",
    "crumbs": [
      "Unit 1: Internal Validity",
      "Lab 1: The Hospital of Uncertain Outcomes"
    ]
  },
  {
    "objectID": "labs/lab-1-InternalValidityPO.html#step-4-selection-bias-and-realized-outcomes",
    "href": "labs/lab-1-InternalValidityPO.html#step-4-selection-bias-and-realized-outcomes",
    "title": "Lab 1",
    "section": "Step 4: Selection Bias and Realized Outcomes",
    "text": "Step 4: Selection Bias and Realized Outcomes\nAlthough the dataset has both y0 and y1, in the real world, a patient’s outcome is observed under only one condition (treated or untreated). Let’s capture which outcome we’d actually see based on the ventilator assignment:\n\n# Use the switching equation to select realized outcomes from potential outcomes based on treatment assignment\n\nSelection Bias Calculation\nWe’ll see if there is selection bias by comparing the expected lifespan of ventilated patients had they not been ventilated to the expected lifespan of non-ventilated patients.\n\n# Calculate EY0 for vent group and no vent group\n\nIf Dr. Perfect is involved, we’d expect some big differences here. Dr. P-Hacker would probably ignore that and claim victory anyway. (He likes ignoring inconvenient truths.)",
    "crumbs": [
      "Unit 1: Internal Validity",
      "Lab 1: The Hospital of Uncertain Outcomes"
    ]
  },
  {
    "objectID": "labs/lab-1-InternalValidityPO.html#step-5-comparing-outcomes-between-groups",
    "href": "labs/lab-1-InternalValidityPO.html#step-5-comparing-outcomes-between-groups",
    "title": "Lab 1",
    "section": "Step 5: Comparing Outcomes Between Groups",
    "text": "Step 5: Comparing Outcomes Between Groups\nOne of the simplest ways to estimate the treatment effect is to look at the Simple Difference in Outcomes (SDO)—the difference in the observed mean outcome between those who got the treatment (ventilators) and those who did not.\n\n# Calculate the share of units treated with vents (pi)\n\n\n# Manually calculate the simple difference in mean health outcomes\n\nDr. P-Hacker would run around shouting: “Aha! This difference proves the intervention works!” or “Aha! It’s not significant!” depending on the p-value. Let’s see if we can do better.",
    "crumbs": [
      "Unit 1: Internal Validity",
      "Lab 1: The Hospital of Uncertain Outcomes"
    ]
  },
  {
    "objectID": "labs/lab-1-InternalValidityPO.html#step-6-estimating-the-effect-with-regression",
    "href": "labs/lab-1-InternalValidityPO.html#step-6-estimating-the-effect-with-regression",
    "title": "Lab 1",
    "section": "Step 6: Estimating the Effect with Regression",
    "text": "Step 6: Estimating the Effect with Regression\nTo confirm our findings, let’s do an Ordinary Least Squares (OLS) regression of the realized outcome on the ventilator indicator:\n\n# Estimate the treatment effect using OLS\n\nThis approach, while straightforward, is still subject to any biases from non-random assignment—like, say, Dr. Perfect or Dr. Bad being in charge.",
    "crumbs": [
      "Unit 1: Internal Validity",
      "Lab 1: The Hospital of Uncertain Outcomes"
    ]
  },
  {
    "objectID": "labs/lab-1-InternalValidityPO.html#step-7-validating-the-decomposition",
    "href": "labs/lab-1-InternalValidityPO.html#step-7-validating-the-decomposition",
    "title": "Lab 1",
    "section": "Step 7: Validating the Decomposition",
    "text": "Step 7: Validating the Decomposition\nAt this point, we’d like to see if the Simple Difference in Outcomes (SDO) can be broken down into our measured parameters. You’ll fill in a table comparing Dr. Perfect to Dr. Bad, computing the ATE, ATT, ATU, selection bias, SDO, and so on.\nBelow is a quick consistency check to see if the SDO lines up with our theoretical decomposition:\n\n# Decomposition check\n\nWhen you fill out the table, you should include:\n\n\nPerfect Doctor\nBad Doctor\nCausal Parameter\n\n\n\nATE\n\n\n\n\nATT\n\n\n\n\nATU\n\n\n\n\nSelection bias terms\n\n\n\n\nE[Y0 | D=1]\n\n\n\n\nE[Y0 | D=0]\n\n\n\n\nSelection bias\n\n\n\n\nCalculations\n\n\n\n\nPi (share on vents)\n\n\n\n\nSDO manual\n\n\n\n\nSDO OLS\n\n\n\n\nSDO Decomposition\n\n\n\n\nObs\n\n\n\n\n\nReflection\nFinally, reflect on these questions:\n\nIs the Simple Difference in Outcomes (SDO) enough to identify the ATE, ATT, or ATU?\nHow does selection bias play into interpreting the SDO?\nWhat lessons would Nurse Random want Dr. P-Hacker to learn about data and design?\n\n(Extra credit if you can imagine the dramatic showdown when Dr. Doub R. Obust finally unveils a doubly robust method that saves the day—but that’s a story for a future lab!)\nThat’s it! You’ve run the gauntlet of the Hospital of Uncertain Outcomes and lived to tell the tale. Now submit your work, and good luck diagnosing more of St. Null’s methodological maladies!",
    "crumbs": [
      "Unit 1: Internal Validity",
      "Lab 1: The Hospital of Uncertain Outcomes"
    ]
  },
  {
    "objectID": "labs/lab-2-Power.html",
    "href": "labs/lab-2-Power.html",
    "title": "Lab 2",
    "section": "",
    "text": "In this lab, you will explore the concept of statistical conclusion validity by conducting power calculations via simulation. Specifically, you will:\n\nConduct a power simulation for a simple randomized experiment without clustering (i.e., ignoring the fact that participants may be grouped within clinics).\nExtend that simulation to account for clustering (i.e., clinics as the unit of randomization).\nExamine the impact of sample size, effect size, and clustering on statistical power, and visualize how minimum detectable effect sizes (MDE) change with sample size.\n\nBy the end of this lab, you will have a deeper understanding of:\n\nHow sources of uncertainty (sampling variation, variance in potential outcomes across participants, and measurement error) affect your study’s outcomes.\nWhy we must beware of Type I (false positive) and Type II (false negative) errors—especially if Dr. P-Hacker is anywhere near our data.\nHow p-values should (and should not!) be interpreted.\nHow to simulate data that include cluster-level effects and adjust for these effects in your analysis.",
    "crumbs": [
      "Unit 1: Internal Validity",
      "Lab 2: Power by Simulation"
    ]
  },
  {
    "objectID": "labs/lab-2-Power.html#overview-and-learning-objectives",
    "href": "labs/lab-2-Power.html#overview-and-learning-objectives",
    "title": "Lab 2",
    "section": "",
    "text": "In this lab, you will explore the concept of statistical conclusion validity by conducting power calculations via simulation. Specifically, you will:\n\nConduct a power simulation for a simple randomized experiment without clustering (i.e., ignoring the fact that participants may be grouped within clinics).\nExtend that simulation to account for clustering (i.e., clinics as the unit of randomization).\nExamine the impact of sample size, effect size, and clustering on statistical power, and visualize how minimum detectable effect sizes (MDE) change with sample size.\n\nBy the end of this lab, you will have a deeper understanding of:\n\nHow sources of uncertainty (sampling variation, variance in potential outcomes across participants, and measurement error) affect your study’s outcomes.\nWhy we must beware of Type I (false positive) and Type II (false negative) errors—especially if Dr. P-Hacker is anywhere near our data.\nHow p-values should (and should not!) be interpreted.\nHow to simulate data that include cluster-level effects and adjust for these effects in your analysis.",
    "crumbs": [
      "Unit 1: Internal Validity",
      "Lab 2: Power by Simulation"
    ]
  },
  {
    "objectID": "labs/lab-2-Power.html#the-hospital-drama-begins",
    "href": "labs/lab-2-Power.html#the-hospital-drama-begins",
    "title": "Lab 2",
    "section": "The Hospital Drama Begins",
    "text": "The Hospital Drama Begins\nWelcome to St. Null’s Memorial Hospital, where a brand-new quality improvement intervention is about to be tested. The hospital’s network of clinics, collectively called The Null Distribution, is famous for its dedication to meticulous data collection—and also for some questionable statistical practices performed by a rather infamous staff member.\n\n\nCEO Barnaby Beta has championed a new, cost-intensive intervention aimed at improving patient satisfaction.\n\nNurse Random insists on conducting a proper randomized control trial (RCT), but quickly realizes Dr. P-Hacker might have already meddled with the initial power calculations.\n\nDr. P-Hacker is known to declare victory (“Significant at the 5% level!”) before even cleaning the data, and is rumored to own a golden “p &lt; 0.05” sign that he waves in staff meetings.\n\nDr. Doub R. Obust is the voice of reason, reminding everyone of fundamental statistical principles.\n\nIn this lab, you (the consultants) have been summoned to salvage the situation. Let’s see how this plays out.",
    "crumbs": [
      "Unit 1: Internal Validity",
      "Lab 2: Power by Simulation"
    ]
  },
  {
    "objectID": "labs/lab-2-Power.html#the-plot-thickens-power-calculations-without-clustering",
    "href": "labs/lab-2-Power.html#the-plot-thickens-power-calculations-without-clustering",
    "title": "Lab 2",
    "section": "The Plot Thickens: Power Calculations Without Clustering",
    "text": "The Plot Thickens: Power Calculations Without Clustering\nScene 1: The Mysterious Spreadsheet\nNurse Random bursts into the conference room, clutching a color-coded spreadsheet.\n\nNurse Random: “Dr. P-Hacker, your power calculations look suspiciously high. Did you account for the fact that we’re randomizing by clinic, not by individual patient?”\n\n\nDr. P-Hacker: “Of course not, Nurse Random. Look, a random patient is a random patient—no matter which clinic they come from! Besides, I love high power. It makes me feel like my study can conquer the world!”\n\n\nDr. Doub R. Obust (rolling eyes): “We’re going to need to run a simulation that properly reflects how the intervention is assigned at the clinic level, not just among individual patients. Otherwise, your calculations will be as overconfident as your new P-value neon sign.”\n\nNonetheless, Dr. P-Hacker shares his “method” with you first. Let’s start by replicating his approach (ignoring clustering). This will be our baseline—what not to do if clinics are truly the unit of randomization.\nStep 1: Flawed Power Simulation (Ignoring Clustering)\nWe’ll simulate a dataset as if individual patients are randomly assigned to treatment or control. We’ll compute the statistical power for detecting a true treatment effect of a specified size, ignoring any clinic-level differences.\n\n\n\n\n\n\nTask 1: Flawed Simulation Setup\n\n\n\nFill in the code below to acheive a power of around 0.8 (may not be exact, but get as close as you can):\n\nSet a sample size for the total number of patients.\n\nSpecify a treatment effect (effect).\n\nDecide on the proportion of participants to receive treatment (prop).\n\nSelect a significance level (t_alpha).\n\nRun multiple simulations (sim.size).\n\nThen, run a linear regression on each simulated dataset, gather the p-values, and compute how often the null hypothesis is rejected (i.e., estimate power).\nNote: The code below is not executable. You need to change the eval: false to eval: true to make it work and fill in the blanks.\n\n\n\nlibrary(data.table)\nset.seed(123456)\n\n# Define parameters\nsample_size &lt;-     # e.g. 500\neffect &lt;-          # e.g. 0.2\nprop &lt;-            # e.g. 0.5\nt_alpha &lt;-         # e.g. 0.05\nsim.size &lt;-        # e.g. 2000\n\n# Initialize storage for results\nreject_t &lt;- numeric(sim.size)\n\n# Simulation loop\nfor (i in 1:sim.size) {\n  dt &lt;- data.table(id = 1:sample_size)\n  \n  # Assign treatment individually (incorrect for cluster randomization!)\n  dt[, treatment := rbinom(.N, 1, prop)]\n  \n  # Simulate outcome (10 is baseline, 'effect' is added if treatment=1)\n  dt[, outcome := 10 + effect * treatment + rnorm(.N, mean = 0, sd = 1)]\n  \n  # Estimate the effect\n  fit &lt;- lm(outcome ~ treatment, data = dt)\n  p_value &lt;- summary(fit)$coefficients[2,4]\n  \n  # Check if we reject H0: (p-value &lt; alpha)\n  reject_t[i] &lt;- ifelse(p_value &lt; t_alpha, 1, 0)\n}\n\n# Compute estimated power\npower_flawed &lt;- mean(reject_t)\ncat(\"Estimated Power (Ignoring Clustering):\", power_flawed, \"\\n\")\n\nDiscussion of Step 1\n\n\nNurse Random sighs: “We got an estimated power of 0.8 (yours might be slightly different). But do we trust this number?”\n\nDr. Doub R. Obust chimes in: “Nope. We’re ignoring that patients within the same clinic might be more similar to each other than to patients in other clinics. Our standard errors are artificially small.”\n\nAt this point, CEO Barnaby Beta perks up: “Artificially small standard errors? That means we’re basically claiming more precision than we actually have, right?”\nYes, indeed. If we disregard the clustering of patients, we risk making a Type I error more often than we realize. Dr. P-Hacker, in typical fashion, responds:\n\nDr. P-Hacker: “Type I error? Isn’t that just when we see something interesting that’s obviously there?!”Dr. Doub R. Obust (groaning): “No. A Type I error is a false positive—we conclude there is an effect even though, in truth, there isn’t. Like thinking you’ve discovered a rare golden banana flavor at the cafeteria soda machine when really it’s just seltzer water with a weird label!”",
    "crumbs": [
      "Unit 1: Internal Validity",
      "Lab 2: Power by Simulation"
    ]
  },
  {
    "objectID": "labs/lab-2-Power.html#understanding-the-sources-of-uncertainty",
    "href": "labs/lab-2-Power.html#understanding-the-sources-of-uncertainty",
    "title": "Lab 2",
    "section": "Understanding the Sources of Uncertainty",
    "text": "Understanding the Sources of Uncertainty\nBefore we fix our simulation, Dr. Doub R. Obust insists that you reflect on why ignoring clinic-level clustering is a problem. He ticks off sources of variability that a real experiment faces:\n\n\nSampling Variation: Even if you have a large population of patients, the sample you select is just one draw from a bigger population. Different samples might give different estimates.\n\n\nMeasurement Error: If your measurement tool for patient well-being is noisy (e.g., patients often mis-report how they feel, or staff record data incorrectly), it introduces extra variability that can muddy your effect estimates.\n\nVariance in Potential Outcomes: Not all patients respond to treatments in the same way. Some might be strongly affected, some not at all—leading to variation in the outcomes. This includes clustering: Patients in the same clinic share certain characteristics, environmental factors, or staff practices that can affect their potential outcomes. This correlation must be accounted for in the design and analysis.\n\n\nNurse Random: “So if we ignore that last point—clustering—our estimate of the variability (and thus the standard error) is off, and we might incorrectly claim significance. That’s basically p-hacking 101!”",
    "crumbs": [
      "Unit 1: Internal Validity",
      "Lab 2: Power by Simulation"
    ]
  },
  {
    "objectID": "labs/lab-2-Power.html#power-calculations-with-clustering",
    "href": "labs/lab-2-Power.html#power-calculations-with-clustering",
    "title": "Lab 2",
    "section": "Power Calculations With Clustering",
    "text": "Power Calculations With Clustering\nScene 2: A (Cluster-)Randomized Trial\nNow we come to the correct approach: our unit of randomization is the clinic, not the individual. That means each clinic either receives the intervention or does not, and all patients in a clinic share the same treatment assignment.\nStep 2: Incorporating Clustering\nWe’ll model:\n\n\nnum_clusters = number of clinics.\n\n\ncluster_size = number of patients per clinic.\n\n\nicc (Intraclass Correlation Coefficient): A measure of how strongly patients in the same clinic resemble each other. High ICC means patients within a clinic are more correlated.\n\nWe’ll generate clinic-level “random effects” and individual-level error terms to reflect both measurement error and the variability in potential outcomes across individuals.\n\n\n\n\n\n\n\nTask 2: Correcting the Simulation for Clustering\n\n\n\nFill in the code below to acheive a power of 0.8:\n\nDefine the number of clinics and the number of patients per clinic.\n\nAssign each entire clinic to treatment or control.\n\nIncorporate cluster-level random effects.\n\nFit the model but adjust standard errors for clustering.\n\n\n\n\n\nlibrary(multiwayvcov)  # for cluster-robust standard errors\n\n# Define cluster parameters\nnum_clusters &lt;-     # e.g. 50\ncluster_size &lt;-     # e.g. 10\ntotal_sample &lt;- num_clusters * cluster_size\nicc &lt;-              # e.g. 0.4\n\n# Variances\nind_err_var &lt;- 1\n# cluster_err_var is derived from the intraclass correlation coefficient\ncluster_err_var &lt;- (icc * ind_err_var) / (1 - icc)\n\nsim.size &lt;-         # e.g. 2000\neffect &lt;-           # e.g. 0.2\nprop &lt;-             # e.g. 0.5\nt_alpha &lt;-          # e.g. 0.05\n\nreject_t &lt;- numeric(sim.size)\n\nset.seed(654321)\nfor (i in 1:sim.size) {\n  \n  # Create a data table with one row per cluster, repeated for each patient\n  clusters &lt;- data.table(\n    cluster = rep(1:num_clusters, each = cluster_size)\n  )\n  \n  # Generate a cluster-level random effect\n  clusters[, cluster_error := rep(\n    rnorm(num_clusters, mean = 0, sd = sqrt(cluster_err_var)),\n    each = cluster_size\n  )]\n  \n  # Randomize at the clinic level\n  clusters[, treatment := rep(\n    rbinom(num_clusters, 1, prop),\n    each = cluster_size\n  )]\n  \n  # Add an individual-level error\n  clusters[, individual_error := rnorm(.N, mean = 0, sd = sqrt(ind_err_var))]\n  \n  # Final outcome for each individual\n  clusters[, outcome := 10 + effect * treatment + cluster_error + individual_error]\n  \n  # Fit a naive linear model (ignoring clustering in standard errors)\n  fit &lt;- lm(outcome ~ treatment, data = clusters)\n  \n  # Adjust standard errors for clustering\n  robust_SE &lt;- cluster.vcov(fit, clusters$cluster, df_correction = TRUE)\n  robust_coef &lt;- coeftest(fit, robust_SE)\n  \n  # Get the p-value for the treatment coefficient\n  p_value &lt;- robust_coef[2,4]\n  reject_t[i] &lt;- ifelse(p_value &lt; t_alpha, 1, 0)\n}\n\n# Compute estimated power with clustering\npower_clustered &lt;- mean(reject_t)\ncat(\"Estimated Power (Clustered):\", power_clustered, \"\\n\")\n\nDiscussion of Step 2\nDr. Doub R. Obust looks at the new power estimate and remarks:\n\n“As you can see, once we account for clustering, the power is (usually) lower than in the flawed approach. That’s because those cluster-level similarities effectively reduce the amount of independent information we have. Our standard errors are bigger, so it’s harder to find significance unless the effect size or sample size is larger.”\n\nCEO Barnaby Beta shakes his head: “But that means our study might be underpowered if we stick to our current budget!”\nNurse Random replies with a grin: “Don’t worry, we can plan more carefully. Otherwise, if we run a smaller study, we risk a Type II error—failing to reject the null hypothesis when there really is an effect. You know, like leaving the gold standard intervention on the shelf because we didn’t gather enough data to prove it works.”\nDr. P-Hacker interjects:\n\n“And there’s always p &lt; 0.10, right? We can move our threshold for significance to get more ‘positive’ results!”Nurse Random (scolding): “That’s literally p-hacking. Please step away from the analysis, Doctor.”",
    "crumbs": [
      "Unit 1: Internal Validity",
      "Lab 2: Power by Simulation"
    ]
  },
  {
    "objectID": "labs/lab-2-Power.html#the-truth-about-p-values",
    "href": "labs/lab-2-Power.html#the-truth-about-p-values",
    "title": "Lab 2",
    "section": "The Truth About p-Values",
    "text": "The Truth About p-Values\nA quick comedic break for a lesson on p-values:\n\n\nDr. P-Hacker keeps shouting that p &lt; 0.05 means there’s a 5% chance the null hypothesis is true.\n\n\nNurse Random corrects him: “No, no, no. A p-value is the probability of observing a result at least as extreme as ours if the null is true. It’s NOT the probability the null is true. You can’t interpret it that way.”\n\n\nDr. P-Hacker: “I was so sure it was the probability that I was right.”Dr. Doub R. Obust: “We live and learn, Doctor.”",
    "crumbs": [
      "Unit 1: Internal Validity",
      "Lab 2: Power by Simulation"
    ]
  },
  {
    "objectID": "labs/lab-2-Power.html#plotting-the-minimum-detectable-effect-mde",
    "href": "labs/lab-2-Power.html#plotting-the-minimum-detectable-effect-mde",
    "title": "Lab 2",
    "section": "Plotting the Minimum Detectable Effect (MDE)",
    "text": "Plotting the Minimum Detectable Effect (MDE)\nBecause CEO Barnaby Beta wants to know how large an effect must be to have a reasonable chance of detection, you might want to simulate across a range of effect sizes and/or sample sizes. You can then plot the resulting power curves to see what the Minimum Detectable Effect (MDE) is for a given power requirement.\n\n\n\n\n\n\nTask 3: Create an MDE Plot\n\n\n\n\nLoop over a grid of effect sizes (or sample sizes).\n\nFor each value, compute power using the cluster-based simulation approach.\n\nPlot the effect size on the x-axis and the corresponding power on the y-axis.\n\n\n\n\n# Example code snippet (feel free to modify)\nlibrary(ggplot2)\n\neffect_sizes &lt;- seq(0, 0.5, by = 0.05) \nresults &lt;- data.table(effect = effect_sizes, power = NA_real_)\n\nfor (e in seq_along(effect_sizes)) {\n  # Repeat your cluster simulation steps but with effect = effect_sizes[e]\n  # ...\n  # store the average of reject_t in results$power[e]\n  # ...\n}\n\nggplot(results, aes(x = effect, y = power)) +\n  geom_line() +\n  geom_point() +\n  labs(\n    title = \"Power vs. Effect Size\",\n    x = \"Effect Size\",\n    y = \"Power\"\n  ) +\n  theme_minimal()",
    "crumbs": [
      "Unit 1: Internal Validity",
      "Lab 2: Power by Simulation"
    ]
  },
  {
    "objectID": "labs/lab-2-Power.html#final-words-from-the-hospital-staff",
    "href": "labs/lab-2-Power.html#final-words-from-the-hospital-staff",
    "title": "Lab 2",
    "section": "Final Words from the Hospital Staff",
    "text": "Final Words from the Hospital Staff\nCEO Barnaby Beta: “Alright, so if we need to ensure we have enough clinics and participants to achieve our desired power, we may need a bigger budget than expected. Let’s not forget that ignoring clustering would have given us a false sense of security in our power. Now we know better.”\nNurse Random: “And no more Type I or Type II error confusion, Dr. P-Hacker. We must keep our definitions straight if we’re to have any credibility around here!”\nDr. P-Hacker (sighing): “Fine, fine. Guess I’ll tone down the p-value hype. But I’m keeping my neon sign.”\nDr. Doub R. Obust (with a grin): “We’ll allow the neon sign, as long as you promise to interpret it correctly.”",
    "crumbs": [
      "Unit 1: Internal Validity",
      "Lab 2: Power by Simulation"
    ]
  },
  {
    "objectID": "labs/lab-2-Power.html#submission-instructions",
    "href": "labs/lab-2-Power.html#submission-instructions",
    "title": "Lab 2",
    "section": "Submission Instructions",
    "text": "Submission Instructions\n\nMake sure your .qmd file knits successfully (to .pdf or .html).\n\nUpload your compiled document to Gradescope.\n\n\nInclude your discussion of the results, your code, and your responses to the callout sections.\n\nRemember, the moral of the story: Always check who (or what) is being randomized, and account for clustering when necessary! Otherwise, your study conclusions might be as random as Dr. P-Hacker’s next dinner choice.",
    "crumbs": [
      "Unit 1: Internal Validity",
      "Lab 2: Power by Simulation"
    ]
  },
  {
    "objectID": "instructors.html",
    "href": "instructors.html",
    "title": "Instructors",
    "section": "",
    "text": "Sean Sylvia is an Associate Professor in the Department of Health Policy and Management in the Gillings School of Global Public Health at UNC Chapel Hill.\nOffice Hours: Dr. Sylvia will hold office hours on Wednesdays from 1-2pm. Please book an appointment at least 24 hours ahead of time using this LINK.",
    "crumbs": [
      "Course Information",
      "Instructors"
    ]
  },
  {
    "objectID": "instructors.html#instructor-sean-sylvia-ph.d.",
    "href": "instructors.html#instructor-sean-sylvia-ph.d.",
    "title": "Instructors",
    "section": "",
    "text": "Sean Sylvia is an Associate Professor in the Department of Health Policy and Management in the Gillings School of Global Public Health at UNC Chapel Hill.\nOffice Hours: Dr. Sylvia will hold office hours on Wednesdays from 1-2pm. Please book an appointment at least 24 hours ahead of time using this LINK.",
    "crumbs": [
      "Course Information",
      "Instructors"
    ]
  },
  {
    "objectID": "instructors.html#teaching-assistant-yumeng-du",
    "href": "instructors.html#teaching-assistant-yumeng-du",
    "title": "Instructors",
    "section": "Teaching Assistant: Yumeng Du",
    "text": "Teaching Assistant: Yumeng Du\nYumeng Du is a Ph.D. student in the Department of Health Policy and Management in the Gillings School of Global Public Health at UNC Chapel Hill.\nOffice Hours: Yumeng will hold office hours on Mondays from 1-2pm. Please book an appointment at least 24 hours ahead of time using this LINK.",
    "crumbs": [
      "Course Information",
      "Instructors"
    ]
  },
  {
    "objectID": "Resources/Pre-AnalysisPlanTemplate.html",
    "href": "Resources/Pre-AnalysisPlanTemplate.html",
    "title": "Pre-Analysis Plan: Title of the Study",
    "section": "",
    "text": "Note\n\n\n\nTemplate for a Pre-Analysis Plan (PAP) for a randomized experiment. This is modified from an original template created by Alejandro Ganimian, available here.\nOther Helpful Resources:\nFor guidance on pre-analysis plans, refer to\n\nthe World Bank’s DIME Wiki: Pre-Analysis Plan - DIME Wiki\nThe J-Pal Research Resources Website: J-Pal Research Resources\n\nFor examples of pre-analysis plans, explore the AEA’s RCT Registry: AEA RCT Registry. Here are some of mine:\n\nPay by Design Trial\nAnemia P4P Trial"
  },
  {
    "objectID": "Resources/Pre-AnalysisPlanTemplate.html#abstract",
    "href": "Resources/Pre-AnalysisPlanTemplate.html#abstract",
    "title": "Pre-Analysis Plan: Title of the Study",
    "section": "Abstract",
    "text": "Abstract\n\nIn 1-2 sentences, what does the study entail?\nIn 1-2 sentences, why is this study important/relevant?"
  },
  {
    "objectID": "Resources/Pre-AnalysisPlanTemplate.html#motivation",
    "href": "Resources/Pre-AnalysisPlanTemplate.html#motivation",
    "title": "Pre-Analysis Plan: Title of the Study",
    "section": "Motivation",
    "text": "Motivation\n\nWhat is the main problem/question motivating the study?\nHow has this problem/question been addressed thus far?\nHow is this study different from prior research on this problem/question?\nWhy is the context that you have chosen for this study appropriate?"
  },
  {
    "objectID": "Resources/Pre-AnalysisPlanTemplate.html#research-questions",
    "href": "Resources/Pre-AnalysisPlanTemplate.html#research-questions",
    "title": "Pre-Analysis Plan: Title of the Study",
    "section": "Research Questions",
    "text": "Research Questions\n\nWhat are the main research questions the study seeks to answer?"
  },
  {
    "objectID": "Resources/Pre-AnalysisPlanTemplate.html#sampling",
    "href": "Resources/Pre-AnalysisPlanTemplate.html#sampling",
    "title": "Pre-Analysis Plan: Title of the Study",
    "section": "Sampling",
    "text": "Sampling\n\nSampling Frame\n\nWhat is the eligible population for the study?\n\nWhat are the main characteristics of this population?\n\nWhat is the expected sample for the study?\n\nWhat is the expected sample size?\nHow does the expected sample differ from the population?\n\n\n\n\nStatistical Power\n\nWhat is the effect size you will be able to detect?\n\nWhat are your assumptions about your alpha-level?\nWhat are your assumptions about your statistical power?\nWhat are your assumptions about variability in your effect size?\nHow many sites will you have?\nHow many people will you have in each site?\nWhat share of the variance do you expect to predict with your covariates?\n\nHow sensitive is your effect size to changes in your parameters?\n\n\n\nAssignment to Treatment\n\nHow will individuals be assigned to treatment and control conditions?\nWhat is the source of exogenous variation in your study?\n\n\n\nAttrition from the Sample\n\nDo you anticipate any form of attrition from the sample?\n\nIf so, what share of the sample do you anticipate will attrit?\nOn what evidence are you basing your expectations about attrition?\nHow realistic are your expectations about attrition?\n\nWhat can you do to prevent/remedy sample attrition?\nHow does expected attrition change your power calculations?"
  },
  {
    "objectID": "Resources/Pre-AnalysisPlanTemplate.html#fieldwork",
    "href": "Resources/Pre-AnalysisPlanTemplate.html#fieldwork",
    "title": "Pre-Analysis Plan: Title of the Study",
    "section": "Fieldwork",
    "text": "Fieldwork\n\nInstruments\n\nWhat data collection instruments will you employ?\n\nWhat (groups of) indicators will each instrument cover?\nHow was each instrument developed?\nHave each instrument been used before?\nIf so, by whom? If not, are you piloting it?\nWhat are the main advantages/disadvantages of each instrument?\n\n\n\n\nData Collection\n\nHow long will the entire data collection process take from start to finish?\nWhat does the data collection entail?\nWhat steps will be taken to keep the data collected confidential at this stage?\n\n\n\nData Processing\n\nHow long will data processing take from start to finish?\nWhat does the data processing entail?\nWhat steps will be taken to keep the processed data confidential?\nWho has ownership over the processed data?\nHow will the data be used/stored after the study at this stage?"
  },
  {
    "objectID": "Resources/Pre-AnalysisPlanTemplate.html#variables",
    "href": "Resources/Pre-AnalysisPlanTemplate.html#variables",
    "title": "Pre-Analysis Plan: Title of the Study",
    "section": "Variables",
    "text": "Variables\n\nWhat are the main variables of interest in your study?\n\nHow is each of them defined in your dataset?"
  },
  {
    "objectID": "Resources/Pre-AnalysisPlanTemplate.html#balancing-checks",
    "href": "Resources/Pre-AnalysisPlanTemplate.html#balancing-checks",
    "title": "Pre-Analysis Plan: Title of the Study",
    "section": "Balancing Checks",
    "text": "Balancing Checks\n\nHow will you check balance between treatment and control groups?\n\nWhat is the specification that you will run?\nWhat variables will you include in these balancing checks?\n\nHow will you check balance between attritors and non-attritors?\n\nWhat is the specification that you will run?\nWhat variables will you include in these balancing checks?"
  },
  {
    "objectID": "Resources/Pre-AnalysisPlanTemplate.html#treatment-effects",
    "href": "Resources/Pre-AnalysisPlanTemplate.html#treatment-effects",
    "title": "Pre-Analysis Plan: Title of the Study",
    "section": "Treatment Effects",
    "text": "Treatment Effects\n\nIntent to Treat\n\nHow will you estimate the (causal) effect of the offer of the treatment?\n\nWhat is the specification that you will run?\nWhat controls will you include in your specification?\n\n\n\n\nTreatment on the Treated\n\nHow will you estimate the (causal) effect of the receipt of the treatment?\n\nWhat is the specification that you will run?\nWhat controls will you include in your specification?"
  },
  {
    "objectID": "Resources/Pre-AnalysisPlanTemplate.html#heterogeneous-effects",
    "href": "Resources/Pre-AnalysisPlanTemplate.html#heterogeneous-effects",
    "title": "Pre-Analysis Plan: Title of the Study",
    "section": "Heterogeneous Effects",
    "text": "Heterogeneous Effects\n\nWhich groups do you anticipate will display heterogeneous effects?\nWhat is the broad theory of action that leads you to anticipate these effects?\n\n\nIntent to Treat\n\nHow will you estimate the heterogeneous effects of the offer of the treatment?\n\nWhat are the specifications that you will run?\nWhat controls will you include in your specification?\n\n\n\n\nTreatment on the Treated\n\nHow will you estimate the heterogeneous effects of the receipt of the treatment?\n\nWhat are the specifications that you will run?\nWhat controls will you include in your specification?"
  },
  {
    "objectID": "Resources/Pre-AnalysisPlanTemplate.html#standard-error-adjustments",
    "href": "Resources/Pre-AnalysisPlanTemplate.html#standard-error-adjustments",
    "title": "Pre-Analysis Plan: Title of the Study",
    "section": "Standard Error Adjustments",
    "text": "Standard Error Adjustments\n\nHow will you account for clustering in your data?\nHow will you address false positives from multiple hypothesis testing?\n\nIf you plan to adjust your standard errors, what adjustment procedure will you use? (e.g., Family Wise Error Rate, False Discovery Rates, etc.)\nIf you plan to aggregate multiple variables into an index, which variables will you aggregate and how?\nHow will you deal with outcomes with limited variation?"
  },
  {
    "objectID": "unit-0/lec-0-3.html",
    "href": "unit-0/lec-0-3.html",
    "title": "Foundations: Reproducible Research",
    "section": "",
    "text": "Reproducible research is, quite frankly, one of the most important aspects of scientific work—and also one of the most neglected. Whether you’re a grad student wrangling code after midnight or a seasoned PI rediscovering a half-forgotten analysis, reproducibility ensures your brilliant findings don’t vanish into the digital abyss. Below, we’ll review why you should care, and how you can implement reproducibility so thoroughly that even a time-traveling version of yourself from five years ago could decipher your code."
  },
  {
    "objectID": "unit-0/lec-0-3.html#a.-why-reproducibility-matters",
    "href": "unit-0/lec-0-3.html#a.-why-reproducibility-matters",
    "title": "Foundations: Reproducible Research",
    "section": "A. Why Reproducibility Matters",
    "text": "A. Why Reproducibility Matters\n\nDefinition\n\nIn quantitative research, reproducibility means another researcher—or your future self with only half a memory of what you did—can take the same data and code, then arrive at the same results. No secret incantations, arcane folder structures, or chanting under the full moon required.\n\nMotivations\n\n\nTransparency: Ever heard the phrase “Trust me, I’m a scientist”? Yeah, that doesn’t fly so well if your code is locked away in a cryptically named folder like “analysis_final_FINAL_noReally.r”. By exposing how you clean, analyze, and report data, you bolster confidence in your results.\nCollaboration: If multiple people have to dig through your code to figure out why you did something, at least make it a pleasant excavation. A reproducible workflow means you can share your project with colleagues, and they’ll only have mild confusion instead of sheer panic.\nEfficiency: Imagine re-running your entire analysis with a single command. Now contrast that with rummaging through 57 scripts, each called “analysis2_v2_copy.R”. Reproducibility helps you avoid the second scenario—and possibly a meltdown.\nError Detection: Because let’s face it, even the best of us occasionally type mean when we mean median. With reproducible code, errors can be spotted quickly and corrected before your findings end up on the front page of the Journal of Irreproducible Results.\n\nReproducibility, in short, saves your sanity and safeguards your scientific honor."
  },
  {
    "objectID": "unit-0/lec-0-3.html#b.-core-tools-and-concepts",
    "href": "unit-0/lec-0-3.html#b.-core-tools-and-concepts",
    "title": "Foundations: Reproducible Research",
    "section": "B. Core Tools and Concepts",
    "text": "B. Core Tools and Concepts\nSo how do we make all this happen? We bring in the cavalry: version control, literate programming, environment management, a logical project structure, and tidy documentation. Let’s get to it.\n1. Version Control (Git + GitHub)\nVersion control is like a well-ordered diary for your project: it tracks every change—no matter how small—across time, preserving your code’s evolutionary history in tidy “commits.”\n\nTracks Changes: Every minor tweak, even the moment you changed a comma to a semicolon and saved your entire analysis from ruin.\nFacilitates Collaboration: Gone are the days of emailing zip files named “Project_Latest.zip” back and forth. Now, you can break things collaboratively on GitHub and blame it on the merge conflict.\nCommit History: “Who changed my code last Thursday?” Git knows. Git always knows.\n\nThink of GitHub as your code’s remote spa retreat—safe, relaxing, and occasionally throwing small hissy fits called “merge conflicts.”\n2. Literate Programming (Quarto, R Markdown, knitr)\nSome folks write code in one file, then paste results into a Word document, and so on. Then they wonder why their final paper includes the wrong p-values. Enter literate programming.\n\nIntegrated Code + Narrative: Quarto, R Markdown, knitr, and friends let you keep code, text, and figures in one place, so you never again get lost in the labyrinth of “Oh wait, which file made that plot?”\nMinimizes Copy-Paste Errors: If your results are automatically woven into your final report, you can’t “forget to update Table 3.” All you do is re-run the document, and—voilà!—your latest analysis magically appears.\nSeamless Feedback Loop: Make a change, watch the effect ripple through the entire document. It’s like having the world’s fastest (and least sarcastic) collaborator.\n3. Environment Management (renv, or other)\nPicture this: You wrote a brilliant script six months ago, but now it refuses to run because some obscure package updated. Environment management tools let you freeze your code in time.\n\nCaptures R Package Versions: Tools like renv let you record all the packages (and their versions) you used, so re-installing them later won’t feel like Jumanji in dependency land.\nConsistency Over Time: If you want your analyses to work in five years—or on your advisor’s laptop next Tuesday—document your environment so it can be replicated exactly.\nFewer “It Worked On My Machine” Excuses: Because telling your collaborators to “just figure it out” is not a good look.\n4. Project Structure\nIf your current method involves flinging scripts and data into a single folder named “Stuff,” we might have a gentle suggestion: adopt a standardized project structure.\n\nData Folder: A sanctuary for raw files, processed data, or your deep, dark secrets—just keep it consistent.\nR Scripts or Quarto Documents: Store code for data cleaning, analysis, and visualization in separate scripts or well-labeled notebooks. It’s okay to have multiple files—just label them in a way that future you can actually decipher.\nOutput Folder: All those shiny plots and tables? Keep them in one place. Resist the urge to edit them manually; they should be generated by your scripts, not conjured by hand.\nREADME: The wise old gatekeeper that explains your entire project to anyone who stops by (including you, after you forget everything).\n\nA tidy folder structure is a gift to yourself. And your future co-authors. And your future self’s sanity.\n5. Documentation\nDocumentation is the special sauce that holds everything together—or, in some cases, the glaring absence that leads to frantic Slack messages at 2 a.m.\n\nCommit Messages: If your commit message says “Update stuff,” your collaborators may weep softly. Be descriptive—like “Fix bug in logistic regression loop that caused meltdown.”\nCode Comments: Add them wherever something might confuse your labmates or your brain on a Monday morning. The more complicated the code, the more we need small notes to guide us.\nREADME or docs/ Folder: Provide an at-a-glance explanation of the project’s purpose, the dataset, how to replicate results, and any known issues. It’s your public service announcement to the world.\n\nIf you’re feeling particularly poetic, documentation is the short story you write about your data, so that others can appreciate its plot—without rummaging through all the raw scripts to figure out who the villain is (spoiler: it’s usually missing data).\nTakeaway\nReproducible research is about ensuring that data and code aren’t locked in the dusty attic of your personal computer—half-labeled and wholly misunderstood. By relying on tools like Git, Quarto, and renv, and by maintaining a disciplined project structure and documentation process, you guarantee that your results are both credible and easy to revisit."
  },
  {
    "objectID": "unit-0/lec-0-3.html#hands-on-pratice",
    "href": "unit-0/lec-0-3.html#hands-on-pratice",
    "title": "Foundations: Reproducible Research",
    "section": "Hands-on Pratice",
    "text": "Hands-on Pratice\nNow let’s get our hands dirty with some of the tools for reproducibility. We’ll be applying reproducible research practices to a simulated RCT dataset. You’ll learn to clone a GitHub repository, explore the project structure, run an analysis using Quarto, and commit changes back to GitHub."
  },
  {
    "objectID": "unit-0/lec-0-3.html#guided-computer-setup",
    "href": "unit-0/lec-0-3.html#guided-computer-setup",
    "title": "Foundations: Reproducible Research",
    "section": "0. Guided Computer Setup",
    "text": "0. Guided Computer Setup\n\nComputing Setup\nIntro to Github"
  },
  {
    "objectID": "unit-0/lec-0-3.html#github-template-repository",
    "href": "unit-0/lec-0-3.html#github-template-repository",
    "title": "Foundations: Reproducible Research",
    "section": "1. GitHub Template Repository",
    "text": "1. GitHub Template Repository\n\n\nClone the Repository\n\nGo to the provided URL or use git clone https://github.com/unc-hpm-quant/rct-analysis-template in the terminal.\nIn RStudio: “File” &gt; “New Project” &gt; “Version Control” &gt; “Git” and paste the repo URL.\n\n\nReview Project Structure\n\n   rct-analysis-template/\n   ├── README.md\n   ├── .gitignore\n   ├── renv.lock\n   ├── data/\n   │   └── rct_sim_data.csv\n   ├── analysis/\n   │   ├── analysis.qmd\n   │   └── helpers.R\n   └── output/\n\n\nInstall Dependencies\n\nIf renv is used, run renv::restore() in the project to sync package versions."
  },
  {
    "objectID": "unit-0/lec-0-3.html#analyzing-the-simulated-rct-data",
    "href": "unit-0/lec-0-3.html#analyzing-the-simulated-rct-data",
    "title": "Foundations: Reproducible Research",
    "section": "2. Analyzing the Simulated RCT Data",
    "text": "2. Analyzing the Simulated RCT Data\n\n\nData: rct_sim_data.csv includes columns\n\nsubject_id\ntreatment\noutcome\nage\ngender, etc.\n\n\n\nGoal: Estimate the average treatment effect, create summaray tables, and visualize distributions.\n\n2.1 Open analysis.qmd\n1.  Look at the top YAML and code chunks.\n2.  Notice how code and text are interwoven.\n2.2 Render the Document\n\nIn RStudio, click “Render” or run:\n\n\nquarto::quarto_render(\"analysis/analysis.qmd\")\n\nThe output (HTML, PDF, etc.) will appear in your output folder or in the same directory."
  },
  {
    "objectID": "unit-0/lec-0-3.html#modify-and-commit",
    "href": "unit-0/lec-0-3.html#modify-and-commit",
    "title": "Foundations: Reproducible Research",
    "section": "3. Modify and Commit",
    "text": "3. Modify and Commit\n\nMake a Small Change\n\nFor example, add a simple plot of outcome vs. treatment:\n\nggplot(rct_data, aes(x = factor(treatment), y = outcome)) +\n  geom_boxplot()\n\n\nRe-run the document to see the new figure in the rendered output.\nCommit and Push\n\n\nStage your changes:\n\n\ngit add analysis/analysis.qmd\ngit commit -m \"Added treatment-outcome boxplot\"\ngit push origin main\n\n\nView on GitHub: Confirm your commit is visible and see the updated code."
  },
  {
    "objectID": "unit-0/lec-0-3.html#branching-and-pull-requests-for-bigger-or-test-changes",
    "href": "unit-0/lec-0-3.html#branching-and-pull-requests-for-bigger-or-test-changes",
    "title": "Foundations: Reproducible Research",
    "section": "4. Branching and Pull Requests for bigger or test changes",
    "text": "4. Branching and Pull Requests for bigger or test changes\n\nCreate a new branch\n\n\ngit checkout -b new-plot\n\n\nAdd Covariates: Update the analysis to adjust for age and gender in a regression model."
  },
  {
    "objectID": "unit-0/lec-0-3.html#wrap-up",
    "href": "unit-0/lec-0-3.html#wrap-up",
    "title": "Foundations: Reproducible Research",
    "section": "5. Wrap-Up",
    "text": "5. Wrap-Up\nReflect: How did version control and Quarto documents streamline your workflow?\nNext Steps: If you want to dig further into reproducible data analysis practices, you can explore advanced topics like Docker, code review workflows, and CI/CD (continuous integration and deployment)."
  },
  {
    "objectID": "unit-0/unit-0-foundations-3.html",
    "href": "unit-0/unit-0-foundations-3.html",
    "title": "Unit 0: Foundations",
    "section": "",
    "text": "Learn importance of reproducible research\nLearn key principles of reproducible research\nLearn how to design and implement a reproducible research workflow\n\nVersion control using git and github\nLiterate programming using Quarto\nPackage managemenet using renv",
    "crumbs": [
      "Unit 0: Foundations",
      "Unit 0.2: Computing for Reproducible Research"
    ]
  },
  {
    "objectID": "unit-0/unit-0-foundations-3.html#learning-objectives",
    "href": "unit-0/unit-0-foundations-3.html#learning-objectives",
    "title": "Unit 0: Foundations",
    "section": "",
    "text": "Learn importance of reproducible research\nLearn key principles of reproducible research\nLearn how to design and implement a reproducible research workflow\n\nVersion control using git and github\nLiterate programming using Quarto\nPackage managemenet using renv",
    "crumbs": [
      "Unit 0: Foundations",
      "Unit 0.2: Computing for Reproducible Research"
    ]
  },
  {
    "objectID": "unit-0/unit-0-foundations-3.html#class-will-be-remote-only",
    "href": "unit-0/unit-0-foundations-3.html#class-will-be-remote-only",
    "title": "Unit 0: Foundations",
    "section": "Class will be remote-only",
    "text": "Class will be remote-only\nJoin from the comfort of your own home!\nZoom link: https://unc.zoom.us/j/98379053109",
    "crumbs": [
      "Unit 0: Foundations",
      "Unit 0.2: Computing for Reproducible Research"
    ]
  },
  {
    "objectID": "unit-0/unit-0-foundations-3.html#preparation",
    "href": "unit-0/unit-0-foundations-3.html#preparation",
    "title": "Unit 0: Foundations",
    "section": "Preparation",
    "text": "Preparation\n\n1. Read/Watch/Listen:\n\nReview Computing page\nSkim: Reproducibility Lecture\n\n\n\n2. Computing Setup:\nTo ensure a smooth experience during class, please follow these steps:\n\nInstall R (v4.x or later)\n\nCRAN Download or via your preferred package manager.\n\nInstall RStudio or a similar IDE\n\nRStudio Download\n\nInstall Quarto\n\nQuarto Installation – ensures you can render .qmd files.\n\nInstall Git\n\nConfirm Git is accessible from the command line:\ngit --version\nWindows: Git for Windows\n\nmacOS: Use Xcode Command Line Tools or Homebrew.\n\nCreate a GitHub Account\n\nGitHub Signup – used for version control and code sharing.\n\nSet Up Credentials (SSH keys or PAT)\n\nHelps push/pull to GitHub without repeated logins.\n\nGitHub Docs: Connecting to GitHub with SSH\n\nInstall Required R Packages\n\nIn R/RStudio, run:\ninstall.packages(c(\"tidyverse\",\n                   \"renv\",\n                   \"devtools\",\n                   \"broom\",\n                   \"infer\",\n                   \"rmarkdown\",\n                   \"quarto\"))\n\nOptional but Recommended: Set Up Renv\n\nWe will use renv to manage R package versions.\n\nRenv Documentation\n\n\nOnce you have completed the above steps, you should be ready for our in-class activities.",
    "crumbs": [
      "Unit 0: Foundations",
      "Unit 0.2: Computing for Reproducible Research"
    ]
  },
  {
    "objectID": "unit-0/unit-0-foundations-3.html#in-class",
    "href": "unit-0/unit-0-foundations-3.html#in-class",
    "title": "Unit 0: Foundations",
    "section": "In Class",
    "text": "In Class\n\nReproducibility Lecture\nZoom Recording\nRCT Analysis Template Repository",
    "crumbs": [
      "Unit 0: Foundations",
      "Unit 0.2: Computing for Reproducible Research"
    ]
  },
  {
    "objectID": "unit-0/unit-0-foundations-3.html#additional-resources",
    "href": "unit-0/unit-0-foundations-3.html#additional-resources",
    "title": "Unit 0: Foundations",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nHappy with Git and GitHub for the UseR\nBryan, Jennifer, 2018. Excuse Me, Do You Have a Moment to Talk About Version Control?, The American Statistician.",
    "crumbs": [
      "Unit 0: Foundations",
      "Unit 0.2: Computing for Reproducible Research"
    ]
  },
  {
    "objectID": "unit-0/unit-0-foundations-1.html",
    "href": "unit-0/unit-0-foundations-1.html",
    "title": "Unit 0: Foundations",
    "section": "",
    "text": "This unit will provide a foundation for the rest of the course. It will cover the following topics:\n\nWhy experiments and ML for Causal Inference\nCourse structure and tools\nA field experiment in health services research\nComputing for Reproducible Research",
    "crumbs": [
      "Unit 0: Foundations",
      "Unit 0.0: Introduction"
    ]
  },
  {
    "objectID": "unit-0/unit-0-foundations-1.html#unit-overview",
    "href": "unit-0/unit-0-foundations-1.html#unit-overview",
    "title": "Unit 0: Foundations",
    "section": "",
    "text": "This unit will provide a foundation for the rest of the course. It will cover the following topics:\n\nWhy experiments and ML for Causal Inference\nCourse structure and tools\nA field experiment in health services research\nComputing for Reproducible Research",
    "crumbs": [
      "Unit 0: Foundations",
      "Unit 0.0: Introduction"
    ]
  },
  {
    "objectID": "unit-0/unit-0-foundations-1.html#class-1-course-introduction",
    "href": "unit-0/unit-0-foundations-1.html#class-1-course-introduction",
    "title": "Unit 0: Foundations",
    "section": "Class 1: Course Introduction",
    "text": "Class 1: Course Introduction\n\nPreparation\n\nRead: Harrison and List (2004)\n\nPerusall Link: Harrison and List (2004)\n\n\n\n\n\n\n\n\nPerusall\n\n\n\nPerusall is a free online platform that allows you to collaboratively annotate content with your classmates. Here is a link to the Perusall page for this course: HPM 883. If needed, the class enrollment code is SYLVIA-ZXTWH.\n\nAlthough it is a good way to engage with your classmates around the material, it is not required that you comment on Perusall. If you wish, you can just download the readings directly.\n\n\n\nACI Chapters 0 and 2\n\n\n\nIn Class\n\nLecture slides\n\n\n\nLab/Homework\n\n10-15 min Introductory Class Survey (Due Jan 15)\n\n\n\nAdditional Materials",
    "crumbs": [
      "Unit 0: Foundations",
      "Unit 0.0: Introduction"
    ]
  },
  {
    "objectID": "course-overview.html",
    "href": "course-overview.html",
    "title": "HPM 883Spring 2025",
    "section": "",
    "text": "HPM 883 is an advanced graduate-level course designed to equip PhD students in the Health Policy and Management program with sophisticated quantitative research skills. This course represents the third installment in the quantitative methods sequence, building on the foundational knowledge acquired in HPM 881 and HPM 882.\nThroughout the term, students will delve into advanced quantitative methods pertinent to health services research with a focus on the use of experimental methods and machine learning for causal inference. The course will provide a thorough introduction to machine learning techniques and explore the burgeoning domain of causal machine learning.",
    "crumbs": [
      "Course Information",
      "Overview"
    ]
  },
  {
    "objectID": "course-overview.html#advanced-quantitative-methods-for-health-policy-and-management",
    "href": "course-overview.html#advanced-quantitative-methods-for-health-policy-and-management",
    "title": "HPM 883Spring 2025",
    "section": "",
    "text": "HPM 883 is an advanced graduate-level course designed to equip PhD students in the Health Policy and Management program with sophisticated quantitative research skills. This course represents the third installment in the quantitative methods sequence, building on the foundational knowledge acquired in HPM 881 and HPM 882.\nThroughout the term, students will delve into advanced quantitative methods pertinent to health services research with a focus on the use of experimental methods and machine learning for causal inference. The course will provide a thorough introduction to machine learning techniques and explore the burgeoning domain of causal machine learning.",
    "crumbs": [
      "Course Information",
      "Overview"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Course Schedule",
    "section": "",
    "text": "This page contains the schedule, topics, content and assigments for the semester.\n\n\n\n\n\n\nNote\n\n\n\nThis schedule will be updated as the semester progresses.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nweek\ndow\ndate\nsession\nunit\ntopic\nprepare\nclass\nlab\nlab_sa\ndue\n\n\n\n1\nW\n8 January\nLec\nFoundations\nWelcome; Key Concepts\n\n\n\n\n\n\n\n\n\n2\nM\n13 January\nLec\nFoundations\nGuest Lecture: Running a Randomized Trial\n\n\n\n\n\nIntroductory Survey\n\n\n\nW\n15 January\n-\nFoundations\nReproducible Research\n\n\n\n\n\n\n\n\n\n3\nM\n20 January\n-\nNo Class: MLK Day\n\n\n\n\n\n\n\n\n\nW\n22 January\n-\nInternal Validity\n\n\n\n\n\nClass Canceled\n\n\n4\nM\n27 January\nLec\nInternal Validity\nGeneralized Potential Outcomes Framework; Exclusion Restrictions\n\n\n\n\n\n\n\n\n\n\nW\n29 January\nLab\nInternal Validity\nLab 1: Hospital of Uncertain Outcomes\n\n\n\n\n\n\n\n\n\n5\nM\n3 February\nLab\nInternal Validity\nLab 1: Hospital of Uncertain Outcomes, Solutions\n\n\n\n\n\n\nLab 1 Report\n\n\n\nW\n5 February\nLec\nInternal Validity\nStatistical Conclusion Validity; Power\n\n\n\n\n\n\n\n\n\n6\nM\n10 February\n-\nNo Class: Well-being Day\n\n\n\n\n\n\n\n\n\nW\n12 February\nLab\nInternal Validity\nLab 2: Power by Simulation\n\n\n\n\n\n\n\n\n\n7\nM\n17 February\n-\nInternal Validity\n\n\n\n\n\n\nLab 2 Report\n\n\n\nW\n19 February\nLec\nDesign of Expeirments\n\n\n\n\n\n\n\n\n\n8\nM\n24 February\nLec\nDesign of Expeirments\nOptimal Design of Experiments\n\n\n\n\n\n\n\n\n\n\nW\n26 February\nLec\nDesign of Expeirments\nRandomization Techniques;\n\n\n\n\n\n\n\n\n\n9\nM\n3 March\nLec\nDesign of Expeirments\nLab 3, Midterm QA\n\n\n\n\n\n\n\n\n\nW\n5 March\n-\nMIDTERM 1\nExam will cover all material up to and including Feb 26\n\n\n\n\n\n\n\n10\nM\n10 March\n-\nSpring Break\nHave fun!\n\n\n\n\n\n\n\n\nW\n12 March\n-\nSpring Break\nHave fun!\n\n\n\n\n\n\n\n11\nM\n17 March\nLec\nIntroduction to Machine Learning\nLasso; Penalized Regression; Cross-Validation\n\n\n\n\n\nLab 3 Report\n\n\n\nW\n19 March\nLec\nIntroduction to Machine Learning\nLasso; Penalized Regression; Cross-Validation\n\n\n\n\n\n\n\n\n12\nM\n24 March\nLab\nIntroduction to Machine Learning\nLab: Lasso - Complementarity in Early Childhood\n\n\n\n\n\n\n\n\n\n\nW\n26 March\nLec\nIntroduction to Machine Learning\nTree-based Methods\n\n\n\n\n\n\n\n\n\n13\nM\n31 March\nLec\nPre-Analysis Plans\nPre-Analysis Plans, Building a Thoery of Change\n\n\n\n\n\n\n\n\n\n\nW\n2 April\nLec\nHeterogeneity and Moderation\nHeterogeneous Treatment Effects\n\n\n\n\n\n\n\n\n\n14\nM\n7 April\nLab\nHeterogeneity and Moderation\nLab 4: HTE and Causal Forests\n\n\n\n\n\n\n\n\n\n\nW\n9 April\nLec\nHeterogeneity and Moderation\nHeterogeneous Treatment Effects\n\n\n\n\n\n\n\n\n\n15\nM\n14 April\n-\nMIDTERM 2\n\n\n\n\n\n\n\n\n\nW\n16 April\nLec\nViolations of Internal Validity\nSUTVA and Observability\n\n\n\n\n\n\nProject Pre-Analysis Plan\n\n\n16\nM\n21 April\nLec\nViolations of Internal Validity\nOne and Two-sided Compliance; TOT (LATE)\n\n\n\n\n\n\nLab 4 Report\n\n\n\nW\n23 April\nLab\nViolations of Internal Validity\nOne and Two-sided Compliance; TOT (LATE)\n\n\n\n\n\n\nLab 5 (Peer PAP Review)\n\n\n17\nM\n28 April\nLec\nExternal Validity & Wrap-up\nGeneralizability\n\n\n\n\n\n\n\n\n\n\nW\n30 April\n\nFINAL EXAM: 4-7pm\nOPTIONAL\n\n\n\n\nFinal Project Manuscript & Replication Repository",
    "crumbs": [
      "Course Schedule"
    ]
  },
  {
    "objectID": "unit-1/unit-1-internal-1.html",
    "href": "unit-1/unit-1-internal-1.html",
    "title": "Unit 1.1: Internal Validity and Potential Outcomes",
    "section": "",
    "text": "Reviewws the Potential Outcomes Framework\nAddresses key aspects of experimental design, including:\n\nRandomization\nExclusion restrictions\n\nThese elements are essential for establishing causality.\nWe will also review the notation that will be used throughout the class.\n\n\n\n\nRead:\n\nApplied Causal Inference Powered by ML and AI, Chapter 2\nCausal Inference Mixtape, Chapter 4\n\nListen: Internal Validity Podcast\n\n\n\n\n\nGroup Assignments\n\n\n\n\nGroup Assignments\n\n\n\nSelected Additional Topics via Borda count:\n\n\n\n\nTopic Selection\n\n\n\nSemester Project Description: Semester Project\nLecture: Internal Validity\n\n\n\n\n\nLab 1: The Hospital of Uncertain Outcomes\n\nSolutions",
    "crumbs": [
      "Unit 1: Internal Validity",
      "Unit 1.0: Internal Validity"
    ]
  },
  {
    "objectID": "unit-1/unit-1-internal-1.html#unit-overview",
    "href": "unit-1/unit-1-internal-1.html#unit-overview",
    "title": "Unit 1.1: Internal Validity and Potential Outcomes",
    "section": "",
    "text": "Reviewws the Potential Outcomes Framework\nAddresses key aspects of experimental design, including:\n\nRandomization\nExclusion restrictions\n\nThese elements are essential for establishing causality.\nWe will also review the notation that will be used throughout the class.\n\n\n\n\nRead:\n\nApplied Causal Inference Powered by ML and AI, Chapter 2\nCausal Inference Mixtape, Chapter 4\n\nListen: Internal Validity Podcast\n\n\n\n\n\nGroup Assignments\n\n\n\n\nGroup Assignments\n\n\n\nSelected Additional Topics via Borda count:\n\n\n\n\nTopic Selection\n\n\n\nSemester Project Description: Semester Project\nLecture: Internal Validity\n\n\n\n\n\nLab 1: The Hospital of Uncertain Outcomes\n\nSolutions",
    "crumbs": [
      "Unit 1: Internal Validity",
      "Unit 1.0: Internal Validity"
    ]
  },
  {
    "objectID": "unit-1/unit-1-internal-2.html",
    "href": "unit-1/unit-1-internal-2.html",
    "title": "Unit 1.2: Statistical Conclusion Validity",
    "section": "",
    "text": "Introduce statistical conclusion validity\nExplore sources of uncertainty in experiments\nIntroduce hypothesis testing\nExamine the role of statistical power in determining sample size\nUse simulation techniques for power calculations and design assessments\nDiscuss multiple hypothesis testing and its implications\n\n\n\n\nRead:\n\nWager Chapter 1\nDeclare Design Book: 5.1, 8.2, 10\n\nListen:\n\nExperimental Design and Multiple Hypothesis Testing Podcast\n\n\n\n\n\n\nLecture: Statistical Conclusion Validity\n\n\n\n\n\nLab 2: Power Calculation by Simulation\n\nDue by 11:59pm on Monday, February 17\nSubmit on Gradescope\nSolutions: lab-2-Power-sols.qmd",
    "crumbs": [
      "Unit 1: Internal Validity",
      "Unit 1.1: Statistical Conclusion Validity"
    ]
  },
  {
    "objectID": "unit-1/unit-1-internal-2.html#unit-overview",
    "href": "unit-1/unit-1-internal-2.html#unit-overview",
    "title": "Unit 1.2: Statistical Conclusion Validity",
    "section": "",
    "text": "Introduce statistical conclusion validity\nExplore sources of uncertainty in experiments\nIntroduce hypothesis testing\nExamine the role of statistical power in determining sample size\nUse simulation techniques for power calculations and design assessments\nDiscuss multiple hypothesis testing and its implications\n\n\n\n\nRead:\n\nWager Chapter 1\nDeclare Design Book: 5.1, 8.2, 10\n\nListen:\n\nExperimental Design and Multiple Hypothesis Testing Podcast\n\n\n\n\n\n\nLecture: Statistical Conclusion Validity\n\n\n\n\n\nLab 2: Power Calculation by Simulation\n\nDue by 11:59pm on Monday, February 17\nSubmit on Gradescope\nSolutions: lab-2-Power-sols.qmd",
    "crumbs": [
      "Unit 1: Internal Validity",
      "Unit 1.1: Statistical Conclusion Validity"
    ]
  },
  {
    "objectID": "unit-1/lec-1-2.html",
    "href": "unit-1/lec-1-2.html",
    "title": "Unit 1-2: Statistical Conclusion Validity",
    "section": "",
    "text": "This lecture delves into statistical conclusion validity, exploring how we can assess the reliability and accuracy of estimated treatment effects in experimental research. As we have seen, due to the fundamental nature of potential outcomes, it is inherently impossible to recover individual treatment effects since each unit reveals only one potential outcome. However, under specific assumptions and with an appropriate assignment mechanism—such as random assignment—we can consistently estimate an average treatment effect (ATE). This lecture provides a brief, selective statistical background on how to estimate the ATE and quantify the uncertainty in these estimates, ensuring statistical conclusion validity. To streamline the presentation, we focus on settings with a binary treatment and largely ignore the role of covariates."
  },
  {
    "objectID": "unit-1/lec-1-2.html#what-is-uncertainty",
    "href": "unit-1/lec-1-2.html#what-is-uncertainty",
    "title": "Unit 1-2: Statistical Conclusion Validity",
    "section": "What is Uncertainty?",
    "text": "What is Uncertainty?\nUncertainty in empirical research refers to the inherent imprecision that arises when we attempt to infer quantities that cannot be directly observed. Whether we are engaged in descriptive, causal, or generalization inference, our estimates come with uncertainty that must be quantified and communicated.\nTwo primary frameworks exist for this purpose: the Bayesian and the frequentist approaches.1\nBayesian Approach\nThe Bayesian framework uses Bayes’ rule to combine prior beliefs with the observed data, resulting in a posterior probability distribution over the parameter of interest, \\(\\theta\\). Mathematically, this is expressed as:\n\\[\n\\Pr(\\theta = \\theta' \\mid d = d') = \\frac{\\Pr(d = d' \\mid \\theta = \\theta')\\, \\Pr(\\theta = \\theta')}{\\sum_{\\theta''} \\Pr(d = d' \\mid \\theta = \\theta'')\\, \\Pr(\\theta = \\theta'')},\n\\]\nwhere:\n\n\\(d\\) represents data, and \\(d'\\) represents the observed data (or an “observed realization of the data”)\n\\(\\theta'\\) and \\(\\theta''\\) represent particular values of the parameter \\(\\theta\\).\n\nFrom this posterior distribution, the posterior mean serves as our best estimate of \\(\\theta\\), and the posterior variance quantifies the uncertainty associated with that estimate.\nBy applying Bayes’ rule over different values of \\(\\theta\\), we construct a complete probability distribution that represents all possible answers. This posterior distribution simultaneously provides our best estimate—often summarized by the posterior mean—and quantifies our uncertainty via the posterior variance.\nWhile intuitive, a challenge is that specifying prior uncertainty (\\(\\Pr(\\theta = \\theta')\\)) is often a subjective choice, and the posterior distribution is often difficult to interpret and communicate.\nFrequentist Approach\nIn contrast, the frequentist approach avoids specifying prior beliefs and focuses on the likelihood function, \\(\\Pr(d = d' \\mid \\theta = \\theta')\\), which describes the probability of observing the data \\(d'\\) given a specific value of \\(\\theta\\).2 I.e. instead of thinking of the strength of beliefs, we consider that \\(\\theta\\) generates the actual probability distriubtion over possible data \\(d\\).\nThis approach yields useful quantities:\nP-value: The p-value for a null hypothesis, \\(\\theta = \\theta_0\\), is defined as the probability of observing data as extreme as \\(d_{m*}\\) under the null hypothesis, or\n\\[\n\\Pr(d = d_{m*} \\mid \\theta = \\theta_0).\n\\]\nwhere \\(d_{m*}\\) is the test statistic.\nConfidence Interval: A 95% confidence interval is constructed such that, if the experiment were repeated many times, 95% of the intervals would contain the true parameter value. This approach provides a framework to rule out parameter values that are inconsistent with the observed data, or \\(Pr(d = d' \\mid \\theta = \\theta') \\leq 0.05\\)."
  },
  {
    "objectID": "unit-1/lec-1-2.html#where-does-uncertainty-come-from-in-an-experimental-study",
    "href": "unit-1/lec-1-2.html#where-does-uncertainty-come-from-in-an-experimental-study",
    "title": "Unit 1-2: Statistical Conclusion Validity",
    "section": "Where Does Uncertainty Come From in an Experimental Study?",
    "text": "Where Does Uncertainty Come From in an Experimental Study?\nBefore getting into estimation and the uncertainty of the estimate, we need to be precise about the source of uncertainty. In experimental studies, uncertainty is an inherent part of the inference process, arising from several key sources. Recognizing these sources is critical to designing robust experiments and correctly interpreting the results. The main contributors to uncertainty include:\n\nSampling Variation:\nUncertainty due to sampling variation stems from the fact that any sample drawn from a population is just one of many possible samples. Consequently, the same treatment might yield different results if applied to a different sample, reflecting random fluctuations in the selection process.\nVariance in Potential Outcomes:\nThe natural variability in the potential outcomes (i.e., the outcomes that would be observed under different treatment conditions) can lead to uncertainty. High variance makes it more challenging to detect a true treatment effect because the noise in the data can obscure the signal, thereby reducing the study’s power to reject a false null hypothesis.\nMeasurement Error:\nMeasurement error occurs when there are inaccuracies in recording or assessing the potential outcomes. Such errors introduce additional variability and can bias the estimated treatment effect, further contributing to uncertainty in the experimental results.\n\nTo vizualize this, the diagram below shows a “directed acyclic graph” (DAG) representation of the “data strategy” framework discussed in Chapter 8 of Research Design in the Social Sciences.\n\n\nData Strategy DAG, Source: Research Design in the Social Sciences\n\n\nIn Figure 8.1, we illustrate these three elements of data strategies: sampling (S), treatment assignment (Z), and measurement (Q). These nodes are highlighted by blue boxes to emphasize that they are in the control of the researcher. No arrows go into the S, Z, or Q nodes; they are set by the researcher. In each case, the strategy selected by the researcher affects a corresponding endogenous variable. The sampling procedure causes changes in the endogenous response (R), which represents whether participants provide outcome data, for example responding to survey questions. R is not under the full control of the researchers: it is affected by S, the sampling procedure, but also by the idiosyncratic choices of participants who have higher and lower interest and ability to respond and participate in the study (U). Similarly, the endogenous variable treatment D represents whether participants actually receive the treatment, regardless of their assignment Z. D is affected by the treatment assignment procedure (Z) of course. But except in cases when Z fully determines D (no noncompliance), we are concerned that it will be affected by unobserved idiosyncratic features of individuals U. The third researcher node is Q, the measurement procedure. Q affects Y, the observed outcome, measured by the researcher. Y is also affected by a latent variable Y*, which cannot be directly observed. The measurement procedure provides an imperfect measurement of that latent variable, which is (potentially) affected by treatment D and unobserved heterogeneity U. In the robustness section at the end of the chapter, we explore further variations of this DAG that incorporate threats to inference from noncompliance, attrition, excludability violations, and interference.\n\n\n\n\n\n\n\nTip\n\n\n\nCan you draw four arrows representing the four exclusion restrictions?"
  },
  {
    "objectID": "unit-1/lec-1-2.html#statistical-conclusion-validity",
    "href": "unit-1/lec-1-2.html#statistical-conclusion-validity",
    "title": "Unit 1-2: Statistical Conclusion Validity",
    "section": "Statistical Conclusion Validity",
    "text": "Statistical Conclusion Validity\nGiven that our exclusion restrictions are satisfied, we can estimate the average treatment effect (ATE). The question is then: How can we ensure valid statistical conclusions from our estimate of the ATE?\nWe need to consider the uncertainty in our estimate, and whether we can reject the null hypothesis that the treatment has no effect."
  },
  {
    "objectID": "unit-1/lec-1-2.html#the-super-population-approach",
    "href": "unit-1/lec-1-2.html#the-super-population-approach",
    "title": "Unit 1-2: Statistical Conclusion Validity",
    "section": "The Super-Population Approach",
    "text": "The Super-Population Approach\nThe super-population approach assumes that the study sample is drawn from a larger, hypothetical infinite population represented by a probability distribution, \\(Q\\). This perspective views potential outcomes—both with and without treatment—as stochastic variables drawn from \\(Q\\). The primary goal in this framework is to estimate a feature of this distribution, typically the expected treatment effect:\n\\[\nE[Y(1) - Y(0)]\n\\]\nwhere \\(Y(1)\\) and \\(Y(0)\\) denote the potential outcomes under treatment and control conditions, respectively. Under this approach, each sample is considered an independent and identically distributed (i.i.d.) draw from the distribution \\(Q\\), meaning the researcher is interested in making generalizable inferences beyond the study sample.\nA key implication of this framework is that two sources of variance affect our estimation of treatment effects: 1. Sampling variance—arising from differences between one sample and another. 2. Assignment mechanism variance—introduced by the randomness in treatment assignment.\nThe super-population approach is useful when researchers aim to extend their findings to a broader population, such as in policy recommendations or clinical trials. However, it requires strong assumptions about how well the study sample represents the population."
  },
  {
    "objectID": "unit-1/lec-1-2.html#the-finite-population-approach",
    "href": "unit-1/lec-1-2.html#the-finite-population-approach",
    "title": "Unit 1-2: Statistical Conclusion Validity",
    "section": "The Finite-Population Approach",
    "text": "The Finite-Population Approach\nIn contrast, the finite-population approach considers the sample as a fixed, well-defined group rather than a subset of an infinite population. Here, the researcher is not making inferences beyond the observed sample but instead treating the units as the entire relevant population. This approach is common in evaluations of specific interventions where the focus is on estimating the finite-population average treatment effect (ATE) (also called the sample average treatemnt effect, or SATE):3\n\\[\n\\tau_{fp} = \\frac{1}{N} \\sum_{i=1}^{N} \\left[Y_i(1) - Y_i(0)\\right]\n\\]\nwhere \\(N\\) is the total number of units in the study. Unlike in the super-population approach, the potential outcomes in a finite population framework are fixed, not random. The treatment effect is then viewed as an empirical quantity to be estimated within the sample, rather than a parameter of an underlying distribution.\nA practical distinction between the two approaches is in their implications for statistical inference:\n\nIn the super-population approach, standard errors reflect both sampling variability and randomization-induced variation.\nIn the finite-population approach, standard errors are based only on the variation within the observed sample, without assuming a broader distribution.\n\nThis framework is particularly relevant when researchers are concerned with internal validity over generalizability, such as in program evaluations or field experiments."
  },
  {
    "objectID": "unit-1/lec-1-2.html#subpopulations-in-the-super-population-framework",
    "href": "unit-1/lec-1-2.html#subpopulations-in-the-super-population-framework",
    "title": "Unit 1-2: Statistical Conclusion Validity",
    "section": "Subpopulations in the Super-Population Framework",
    "text": "Subpopulations in the Super-Population Framework\nWithin the super-population framework, researchers often refine their analysis by considering subpopulations to account for heterogeneous treatment effects. One important example is the Conditional Average Treatment Effect (CATE):\n\\[\nE[Y(1, X) - Y(0, X) | X]\n\\]\nwhere \\(X\\) represents observed covariates that influence treatment effects. This approach allows for differentiated insights across groups, such as demographic segments in public health interventions."
  },
  {
    "objectID": "unit-1/lec-1-2.html#choosing-between-the-two-approaches",
    "href": "unit-1/lec-1-2.html#choosing-between-the-two-approaches",
    "title": "Unit 1-2: Statistical Conclusion Validity",
    "section": "Choosing Between the Two Approaches",
    "text": "Choosing Between the Two Approaches\nThe choice between these sampling frameworks depends on the research question:\n\nIf the goal is to make generalizable claims about a broader population, the super-population approach is preferred.\nIf the study focuses on a specific, finite group of units, the finite-population approach is more appropriate.\n\nBoth perspectives provide valuable insights, and many empirical studies incorporate elements of both frameworks, particularly when considering external validity and policy relevance."
  },
  {
    "objectID": "unit-1/lec-1-2.html#statistical-significance-and-the-t-statistic",
    "href": "unit-1/lec-1-2.html#statistical-significance-and-the-t-statistic",
    "title": "Unit 1-2: Statistical Conclusion Validity",
    "section": "Statistical Significance and the t-Statistic",
    "text": "Statistical Significance and the t-Statistic\nTo formally test the null hypothesis, we construct a t-statistic:\n\\[\nt = \\frac{\\hat{\\tau}}{\\text{SE}(\\hat{\\tau})}\n\\]\nwhere SE(\\(\\hat{\\tau}\\)) represents the standard error of the difference-in-means estimator. As the sample size grows, this t-statistic follows a standard normal distribution (or Student’s t-distribution for small samples). A large absolute value of $ t $ provides evidence against the null hypothesis.\nTo determine whether the result is statistically significant, we compare the t-statistic to a critical value determined by our chosen significance level (\\(\\alpha\\), commonly set at 0.05). If the absolute value of the t-statistic exceeds this threshold, we reject the null hypothesis."
  },
  {
    "objectID": "unit-1/lec-1-2.html#type-i-and-type-ii-errors",
    "href": "unit-1/lec-1-2.html#type-i-and-type-ii-errors",
    "title": "Unit 1-2: Statistical Conclusion Validity",
    "section": "Type I and Type II Errors",
    "text": "Type I and Type II Errors\nWhile hypothesis testing provides a structured approach to evaluating treatment effects, errors can still occur:\n\n\nType I Error (\\(\\alpha\\)): Rejecting the null hypothesis when it is actually true (false positive).\n\nControlled by setting the significance level (\\(\\alpha\\)), which determines the probability of mistakenly rejecting \\(H_0\\).\nLower \\(\\alpha\\) reduces false positives but increases the risk of missing real effects.\n\n\n\nType II Error (\\(\\beta\\)): Failing to reject the null hypothesis when it is actually false (false negative).\n\nRelated to statistical power, which is the probability of detecting an effect when it truly exists.\n\n\n\n\n\nType 1 and Type 2 Errors"
  },
  {
    "objectID": "unit-1/lec-1-2.html#power-calculation-ensuring-detectability",
    "href": "unit-1/lec-1-2.html#power-calculation-ensuring-detectability",
    "title": "Unit 1-2: Statistical Conclusion Validity",
    "section": "Power Calculation: Ensuring Detectability",
    "text": "Power Calculation: Ensuring Detectability\nStatistical power refers to the ability of a test to correctly reject the null hypothesis when a true effect exists. Mathematically:\n\\[\n\\text{Power} = 1 - \\beta\n\\]\nFactors influencing power:\n\n\nEffect size (\\(\\tau\\)): Larger effects are easier to detect.\n\nSample size (\\(N\\)): Larger samples reduce variability, increasing power.\n\nSignificance level (\\(\\alpha\\)): Lowering \\(\\alpha\\) increases the risk of missing true effects.\n\nStandard deviation of outcomes: Higher variability in outcomes reduces power.\n\nTo achieve a well-powered experiment, researchers conduct power calculations before data collection to determine the minimum sample size required to detect an effect with reasonable confidence.\nA Simple Example\nLet’s consider a simple example of a power simulation using a simple random assignment to a treatment and control group, where the estimate is the difference-in-means estimator.\n\nlibrary(data.table)\n\nStep 1: Define Simulation Parameters\n\nLet’s assume we have a total sample size of \\(N = 200\\) individuals.\nThe treatment group receives an intervention, while the control group does not.\nThe true treatment effect is set to \\(\\tau = 2\\).\nThe outcome variable follows a normal distribution with mean 10 and standard deviation 4.\n\n\nset.seed(072111)  # Ensures reproducibility\n\n# Define parameters\nN &lt;- 200  # Total sample size\np &lt;- 0.5  # Probability of assignment to treatment\ntrue_tau &lt;- 2  # True treatment effect\nsigma &lt;- 4  # Standard deviation of outcome\n\nStep 2: Simulate Data\nNow we can simulate the data using data.table:\n\n# Simulate data using data.table\ndt &lt;- data.table(id = 1:N)\ndt[, treatment := rbinom(.N, 1, p)]\ndt[, outcome := 10 + true_tau * treatment + rnorm(.N, mean = 0, sd = sigma)]\n\n#display first few rows\nhead(dt)\n\n      id treatment  outcome\n   &lt;int&gt;     &lt;int&gt;    &lt;num&gt;\n1:     1         0 10.39758\n2:     2         1 17.78058\n3:     3         0 11.46925\n4:     4         0 14.56752\n5:     5         1 10.55347\n6:     6         1 14.99933\n\n\nStep 3: Estimate Treatment Effect\n\ndiff_means &lt;- dt[treatment == 1, mean(outcome)] - dt[treatment == 0, mean(outcome)]\nSE &lt;- sqrt(dt[treatment == 1, var(outcome)] / dt[treatment == 1, .N] +\n           dt[treatment == 0, var(outcome)] / dt[treatment == 0, .N])\n\nt_stat &lt;- diff_means / SE  # Compute t-statistic\np_value &lt;- 2 * (1 - pt(abs(t_stat), df = N - 2))  # Two-tailed test\n\n# Display results\ncat(\"Estimated Treatment Effect:\", diff_means, \"\\n\")\n\nEstimated Treatment Effect: 2.453868 \n\ncat(\"p-value:\", p_value, \"\\n\")\n\np-value: 1.088986e-05 \n\n\nStep 4: Power Simulation\nFirst, let’s consider how we will interpret the results:\n\nIf the p-value is less than 0.05, we reject the null hypothesis and conclude that the treatment has a significant effect.\nIf the p-value is greater than 0.05, we fail to reject the null, meaning we do not have enough evidence to confirm a treatment effect.\n\nNow, let’s simulate a power simulation:\nFirst, define the power simulation function:\n\nsimulate_power &lt;- function(N, true_tau, sigma, p, alpha, reps = 1000) {\n  rejections &lt;- 0\n  \n  for (i in 1:reps) {\n    dt &lt;- data.table(id = 1:N)\n    dt[, treatment := rbinom(.N, 1, p)]\n    dt[, outcome := 10 + true_tau * treatment + rnorm(.N, mean = 0, sd = sigma)]\n    diff_means &lt;- dt[treatment == 1, mean(outcome)] - dt[treatment == 0, mean(outcome)]\n    SE &lt;- sqrt(dt[treatment == 1, var(outcome)] / dt[treatment == 1, .N] +\n               dt[treatment == 0, var(outcome)] / dt[treatment == 0, .N])\n    t_stat &lt;- diff_means / SE\n    p_value &lt;- 2 * (1 - pt(abs(t_stat), df = N - 2))\n    \n    if (p_value &lt; alpha) {\n      rejections &lt;- rejections + 1\n    }\n  }\n  return(rejections / reps)\n}\n\nNow, simulate this experiment 1,000 times:\n\n# Run power simulation\npower &lt;- simulate_power(N = 200, true_tau = 2, sigma = 4, p = 0.5, alpha = 0.05, reps = 1000)\ncat(\"Estimated Power:\", power, \"\\n\")\n\nEstimated Power: 0.938 \n\n\nStep 5: Interpreting Power Calculation\n\nThe power of the test is the proportion of simulations in which we correctly reject the null hypothesis when the treatment effect is truly \\(\\tau = 2\\).\nA power value close to 0.80 or higher indicates that the experiment is well-powered.\n\n\n\n\n\n\n\nTip\n\n\n\nTry changing the sample size or effect size, to see how the power changes."
  },
  {
    "objectID": "unit-1/lec-1-2.html#the-mida-framework-for-simulation",
    "href": "unit-1/lec-1-2.html#the-mida-framework-for-simulation",
    "title": "Unit 1-2: Statistical Conclusion Validity",
    "section": "The MIDA Framework for Simulation",
    "text": "The MIDA Framework for Simulation\nA structured way to think about research design and simulation is the MIDA Framework,4(https://book.declaredesign.org/declaration-diagnosis-redesign/research-design.html)] which consists of:\n\n\nM: The Model - The underlying data-generating process that defines the inquiry.\n\nI: The Inquiry - The specific research question we are trying to answer.\n\nD: The Data Strategy - The way we collect and structure data, including sampling and treatment assignment.\n\nA: The Answer Strategy - The statistical method we use to estimate the effect.\n\n\n\nMIDA Framework, Source: Declaration Design\n\n\n\nElements of Research Design, Source: Declaration Design\n\nIn a real-world study, we can only observe a single realization of the design and generate one empirical answer \\(a_d\\). However, through simulation, we can consider many possible data realizations by repeatedly drawing from different models \\(m_1, m_2, ..., m_k\\) within the model space \\(M\\). This allows us to assess how our research design performs across different scenarios."
  },
  {
    "objectID": "unit-1/lec-1-2.html#diagnosing-research-designs-with-simulation",
    "href": "unit-1/lec-1-2.html#diagnosing-research-designs-with-simulation",
    "title": "Unit 1-2: Statistical Conclusion Validity",
    "section": "Diagnosing Research Designs with Simulation",
    "text": "Diagnosing Research Designs with Simulation\nWhen we simulate a research design, we evaluate its performance by considering:\n\n\nBias: How close is the empirical answer \\(a_d\\) to the true answer \\(a_m\\)?\n\nVariance: How much do the empirical answers fluctuate across different realizations?\n\nCoverage: How often do confidence intervals include the true effect?\n\nPower: How frequently does the study correctly reject the null hypothesis when a true effect exists?\n\nThe bottom half of the figure below illustrates how simulation allows us to examine the research design across multiple models (\\(m_1, ..., m_k\\)), generating different answers (\\(a_{m_1}, a_{m_2}, ..., a_{m_k}\\)) and associated datasets (\\(d_1, d_2, ..., d_k\\)). The simulated research design does not have direct access to the true answer but can assess performance across the models under consideration.\n\n\nSimulations in the MIDA Framework"
  },
  {
    "objectID": "unit-1/lec-1-2.html#the-challenge-of-multiple-hypothesis-testing",
    "href": "unit-1/lec-1-2.html#the-challenge-of-multiple-hypothesis-testing",
    "title": "Unit 1-2: Statistical Conclusion Validity",
    "section": "The Challenge of Multiple Hypothesis Testing",
    "text": "The Challenge of Multiple Hypothesis Testing\nIn many empirical settings, researchers conduct multiple hypothesis tests rather than a single test. This introduces the risk of false positives, or mistakenly finding significant effects simply due to chance. Multiple hypothesis testing arises naturally in at least three key scenarios:\n\n\nMultiple Outcomes: When we examine several outcomes (\\(Y_i\\)) to determine whether any are affected by the treatment.\n\nHeterogeneous Treatment Effects (CATEs): When treatment effects vary across subgroups, and we want to assess which subgroups exhibit an effect.\n\nMultiple Treatments: When we compare multiple interventions (\\(D_i\\)) and want to test their effects relative to a control group or to each other.\n\nTo properly interpret results, we need statistical techniques that adjust for multiple comparisons and control the probability of false discoveries."
  },
  {
    "objectID": "unit-1/lec-1-2.html#types-of-multiple-hypothesis-tests",
    "href": "unit-1/lec-1-2.html#types-of-multiple-hypothesis-tests",
    "title": "Unit 1-2: Statistical Conclusion Validity",
    "section": "Types of Multiple Hypothesis Tests",
    "text": "Types of Multiple Hypothesis Tests\nThere are two broad types of statistical hypothesis testing frameworks when dealing with multiple comparisons:\n\n\nJoint Tests: These assess whether at least one hypothesis is true (e.g., “Is at least one treatment effective?”).\n\nSimultaneous Tests: These examine whether multiple hypotheses are true at the same time (e.g., “Are both Treatment A and Treatment B effective?”).\n\nWhen conducting multiple tests, researchers need to control for an increased family-wise error rate (FWER), which is the probability of making at least one Type I error (false positive)."
  },
  {
    "objectID": "unit-1/lec-1-2.html#controlling-the-family-wise-error-rate-fwer",
    "href": "unit-1/lec-1-2.html#controlling-the-family-wise-error-rate-fwer",
    "title": "Unit 1-2: Statistical Conclusion Validity",
    "section": "Controlling the Family-Wise Error Rate (FWER)",
    "text": "Controlling the Family-Wise Error Rate (FWER)\nA single hypothesis test controls the probability of a Type I error at a given significance level (\\(\\alpha\\)). However, with multiple tests, the probability of making at least one false rejection increases:\n\\[\n\\text{FWER} = 1 - (1 - \\alpha)^k\n\\]\nwhere \\(k\\) is the number of tests. For example, if we conduct 5 tests at \\(\\alpha = 0.05\\), the probability of making no Type I errors across all tests is:\n\\[\n(1 - 0.05)^5 = 0.7738\n\\]\nThus, the probability of making at least one Type I error is:\n\\[\nFWER = 1 - 0.7738 = 0.2262\n\\]\nThis means there is a 22.62% chance of mistakenly rejecting at least one null hypothesis across the five tests. If we perform 20 tests, the FWER increases to 64%."
  },
  {
    "objectID": "unit-1/lec-1-2.html#approaches-to-controlling-the-fwer",
    "href": "unit-1/lec-1-2.html#approaches-to-controlling-the-fwer",
    "title": "Unit 1-2: Statistical Conclusion Validity",
    "section": "Approaches to Controlling the FWER",
    "text": "Approaches to Controlling the FWER\nTo mitigate this issue, researchers employ various multiple testing corrections, such as:\n\n\nBonferroni Correction: Adjusts the significance level by dividing \\(\\alpha\\) by the number of tests: \\(\\alpha^* = \\alpha / k\\). This is simple but conservative.\n\nHolm Method: A stepwise procedure that ranks p-values and adjusts them sequentially to control the FWER more efficiently.\n\nModern Approaches (e.g., Westfall-Young, Benjamini-Hochberg FDR control): These methods control for false discovery rates and are widely used in large-scale testing."
  },
  {
    "objectID": "unit-1/lec-1-2.html#simulating-multiple-hypothesis-testing-in-r",
    "href": "unit-1/lec-1-2.html#simulating-multiple-hypothesis-testing-in-r",
    "title": "Unit 1-2: Statistical Conclusion Validity",
    "section": "Simulating Multiple Hypothesis Testing in R",
    "text": "Simulating Multiple Hypothesis Testing in R\nTo illustrate the impact of multiple testing and FWER correction, let’s do a simulation in R:\n\nlibrary(data.table)\nset.seed(072111)  # Ensures reproducibility\n\n# Define parameters\nN &lt;- 200  # Sample size\nk &lt;- 10  # Number of hypothesis tests\nalpha &lt;- 0.05  # Significance level\n\n# Simulate k independent hypothesis tests\ndt &lt;- data.table(test_id = 1:k)\ndt[, p_value := runif(.N, min = 0, max = 1)]  # Generate uniform random p-values\n\ndt[, bonferroni := p_value &lt; (alpha / k)]  # Bonferroni correction\ndt[, holm := p.adjust(p_value, method = \"holm\") &lt; alpha]  # Holm correction\n\ndt[, naive_reject := p_value &lt; alpha]  # Standard test (without correction)\n\n# Count false positives\nfalse_discoveries &lt;- dt[, sum(naive_reject)]\nadjusted_false_discoveries &lt;- dt[, sum(bonferroni)]\nholm_false_discoveries &lt;- dt[, sum(holm)]\n\ncat(\"False positives (no correction):\", false_discoveries, \"\\n\")\n\nFalse positives (no correction): 2 \n\ncat(\"False positives (Bonferroni correction):\", adjusted_false_discoveries, \"\\n\")\n\nFalse positives (Bonferroni correction): 0 \n\ncat(\"False positives (Holm correction):\", holm_false_discoveries, \"\\n\")\n\nFalse positives (Holm correction): 0 \n\n\nInterpreting the Simulation\n\n\nWithout correction, we expect around \\(\\alpha \\times k\\) false discoveries.\n\nBonferroni correction sharply reduces false positives but may be overly conservative.\n\nHolm correction provides a better balance, controlling FWER while maintaining statistical power.\n\n\n\n\n\n\n\nTip\n\n\n\nNote: In the lab, we’ll cover some modern approaches that are more powerful but a bit more complex."
  },
  {
    "objectID": "unit-1/lec-1-2.html#footnotes",
    "href": "unit-1/lec-1-2.html#footnotes",
    "title": "Unit 1-2: Statistical Conclusion Validity",
    "section": "Footnotes",
    "text": "Footnotes\n\nReference: Research Design in the Social Sciences↩︎\nCan you explain in words how this differs from the Bayesian probability above?↩︎\nWager Chapter 1 refers to this as SATE.↩︎\nReference: Declaration Design↩︎"
  },
  {
    "objectID": "template-pap.html",
    "href": "template-pap.html",
    "title": "Pre-Analysis Plan Template",
    "section": "",
    "text": "This is a template for a Pre-Analysis Plan (PAP), roughly following guidelines from the American Economic Association (AEA) and the World Bank’s Development Impact Evaluation (DIME) group."
  },
  {
    "objectID": "template-pap.html#helpful-resources",
    "href": "template-pap.html#helpful-resources",
    "title": "Pre-Analysis Plan Template",
    "section": "Helpful Resources:",
    "text": "Helpful Resources:\n\nFor guidance on pre-analysis plans, refer to the World Bank’s DIME Wiki: Pre-Analysis Plan - DIME Wiki\nFor examples of pre-analysis plans, explore the AEA’s RCT Registry: AEA RCT Registry"
  },
  {
    "objectID": "unit-6/lec-6-1-slides.html#the-goal-evidence-informed-policy-practice",
    "href": "unit-6/lec-6-1-slides.html#the-goal-evidence-informed-policy-practice",
    "title": "From Petri Dish to Policy: Generating and Applying Scalable Experimental Insights",
    "section": "The Goal: Evidence-Informed Policy & Practice",
    "text": "The Goal: Evidence-Informed Policy & Practice\n\nThroughout this course, we’ve explored methods for generating rigorous evidence about “what works.”\nThe ultimate aim is often to improve decisions, policies, and programs that affect people’s lives.\nBUT… translating research findings into real-world impact is challenging.\nThis lecture tackles two critical hurdles: Generalizability (Does it work elsewhere?) and Scalability (Does it work at scale?).",
    "crumbs": [
      "Unit 6: Generalizability & Scalability",
      "Lecture Slides: Unit 6.1: Generating and Applying Scalable Insights"
    ]
  },
  {
    "objectID": "unit-6/lec-6-1-slides.html#why-this-matters-now-bridging-the-know-do-gap",
    "href": "unit-6/lec-6-1-slides.html#why-this-matters-now-bridging-the-know-do-gap",
    "title": "From Petri Dish to Policy: Generating and Applying Scalable Experimental Insights",
    "section": "Why This Matters Now: Bridging the “Know-Do” Gap",
    "text": "Why This Matters Now: Bridging the “Know-Do” Gap\n\nWe often face a gap between what research suggests could work and what actually gets implemented effectively.\nPolicymakers need actionable insights relevant to their context and constraints.\nResearchers want their work to have impact.\nThis topic connects back to the beginning: rethinking the research question itself.\n\nAn impact evaluation is an intervention on the policy process.\nThe goal (theory of change) of an evaluation is to enable better-informed decisions and policies.",
    "crumbs": [
      "Unit 6: Generalizability & Scalability",
      "Lecture Slides: Unit 6.1: Generating and Applying Scalable Insights"
    ]
  },
  {
    "objectID": "unit-6/lec-6-1-slides.html#roadmap",
    "href": "unit-6/lec-6-1-slides.html#roadmap",
    "title": "From Petri Dish to Policy: Generating and Applying Scalable Experimental Insights",
    "section": "Roadmap",
    "text": "Roadmap\n\nThe Challenge: Using evidence for policy decisions.\nDiagnosing Failure: Why do interventions experience “Voltage Drop”? (List’s Vital Signs)\nGenerating Better Evidence: Designing experiments for scale\nApplying Existing Evidence: Generalizing across contexts\nSynthesis & Takeaways: Implications for your research.",
    "crumbs": [
      "Unit 6: Generalizability & Scalability",
      "Lecture Slides: Unit 6.1: Generating and Applying Scalable Insights"
    ]
  },
  {
    "objectID": "unit-6/lec-6-1-slides.html#part-i-the-challenge-using-evidence-for-policy-decisions",
    "href": "unit-6/lec-6-1-slides.html#part-i-the-challenge-using-evidence-for-policy-decisions",
    "title": "From Petri Dish to Policy: Generating and Applying Scalable Experimental Insights",
    "section": "Part I: The Challenge: Using Evidence for Policy Decisions",
    "text": "Part I: The Challenge: Using Evidence for Policy Decisions",
    "crumbs": [
      "Unit 6: Generalizability & Scalability",
      "Lecture Slides: Unit 6.1: Generating and Applying Scalable Insights"
    ]
  },
  {
    "objectID": "unit-6/lec-6-1-slides.html#the-policymakers-dilemma",
    "href": "unit-6/lec-6-1-slides.html#the-policymakers-dilemma",
    "title": "From Petri Dish to Policy: Generating and Applying Scalable Experimental Insights",
    "section": "The Policymaker’s Dilemma",
    "text": "The Policymaker’s Dilemma\n\nNeed evidence to inform decisions.\nBUT: Finding a high-quality impact evaluation that addresses the exact policy question in the exact target context is highly improbable.\nHow do we bridge the gap between available evidence and specific policy needs?",
    "crumbs": [
      "Unit 6: Generalizability & Scalability",
      "Lecture Slides: Unit 6.1: Generating and Applying Scalable Insights"
    ]
  },
  {
    "objectID": "unit-6/lec-6-1-slides.html#common-misapproaches-to-using-evidence",
    "href": "unit-6/lec-6-1-slides.html#common-misapproaches-to-using-evidence",
    "title": "From Petri Dish to Policy: Generating and Applying Scalable Experimental Insights",
    "section": "Common (Mis)Approaches to Using Evidence",
    "text": "Common (Mis)Approaches to Using Evidence\n\n❌ Waiting for the perfect local RCT (may never happen).\n❌ Relying only on weak local evidence (e.g., correlations).\n❌ Blindly copying rigorous results from different contexts.\n❌ Believing replication count is magic (“How many RCTs needed?”).\n❌ Vague definitions of “similar” contexts.",
    "crumbs": [
      "Unit 6: Generalizability & Scalability",
      "Lecture Slides: Unit 6.1: Generating and Applying Scalable Insights"
    ]
  },
  {
    "objectID": "unit-6/lec-6-1-slides.html#compounding-the-problem-the-voltage-drop",
    "href": "unit-6/lec-6-1-slides.html#compounding-the-problem-the-voltage-drop",
    "title": "From Petri Dish to Policy: Generating and Applying Scalable Experimental Insights",
    "section": "Compounding the Problem: The “Voltage Drop”",
    "text": "Compounding the Problem: The “Voltage Drop”\n\nEven when we do have evidence, interventions often perform worse when scaled.\nVoltage Drop: “The cost–benefit profile depreciates considerably when moving from the small to the large.” (List)\nOccurs in 50-90% of scaled programs across various fields.\nWhy does this happen? We need diagnostics.",
    "crumbs": [
      "Unit 6: Generalizability & Scalability",
      "Lecture Slides: Unit 6.1: Generating and Applying Scalable Insights"
    ]
  },
  {
    "objectID": "unit-6/lec-6-1-slides.html#part-ii-diagnosing-voltage-drop-lists-five-vital-signs",
    "href": "unit-6/lec-6-1-slides.html#part-ii-diagnosing-voltage-drop-lists-five-vital-signs",
    "title": "From Petri Dish to Policy: Generating and Applying Scalable Experimental Insights",
    "section": "Part II: Diagnosing Voltage Drop: List’s Five Vital Signs",
    "text": "Part II: Diagnosing Voltage Drop: List’s Five Vital Signs",
    "crumbs": [
      "Unit 6: Generalizability & Scalability",
      "Lecture Slides: Unit 6.1: Generating and Applying Scalable Insights"
    ]
  },
  {
    "objectID": "unit-6/lec-6-1-slides.html#the-five-vital-signs-of-scaling-list-2024",
    "href": "unit-6/lec-6-1-slides.html#the-five-vital-signs-of-scaling-list-2024",
    "title": "From Petri Dish to Policy: Generating and Applying Scalable Experimental Insights",
    "section": "The Five Vital Signs of Scaling (List 2024)",
    "text": "The Five Vital Signs of Scaling (List 2024)\nFactors that cause voltage drops and threaten scalability:\n\n\n\n\n\n\n\n\nVital Sign\nDefinition\nBrief Example\n\n\n\n\n1. False Positives\nIt didn’t actually work reliably in the first place.\nInitial D.A.R.E. results\n\n\n2. Population\nSampled population differs significantly from the population at scale.\nEarly energy saving adopters vs. later ones\n\n\n3. Situation (Context)\nInitial study context differs crucially from the real world at scale.\nImplementation fidelity issues; Teacher quality (CHECC); Nudges over time\n\n\n4. Spillovers (Gen. Equil.)\nIntervention affects non-participants or market dynamics outside the trial.\nCredential devaluation; Neighbor effects\n\n\n5. Supply-Side\nCosts increase disproportionately during scaling (diseconomies of scale).\nHiring less qualified staff at scale\n\n\n\n(Adapted from List 2024, Table 1)",
    "crumbs": [
      "Unit 6: Generalizability & Scalability",
      "Lecture Slides: Unit 6.1: Generating and Applying Scalable Insights"
    ]
  },
  {
    "objectID": "unit-6/lec-6-1-slides.html#part-iii-generating-better-evidence-designing-for-scale",
    "href": "unit-6/lec-6-1-slides.html#part-iii-generating-better-evidence-designing-for-scale",
    "title": "From Petri Dish to Policy: Generating and Applying Scalable Experimental Insights",
    "section": "Part III: Generating Better Evidence: Designing for Scale",
    "text": "Part III: Generating Better Evidence: Designing for Scale",
    "crumbs": [
      "Unit 6: Generalizability & Scalability",
      "Lecture Slides: Unit 6.1: Generating and Applying Scalable Insights"
    ]
  },
  {
    "objectID": "unit-6/lec-6-1-slides.html#shifting-the-research-question",
    "href": "unit-6/lec-6-1-slides.html#shifting-the-research-question",
    "title": "From Petri Dish to Policy: Generating and Applying Scalable Experimental Insights",
    "section": "Shifting the Research Question",
    "text": "Shifting the Research Question\n\nTraditional Efficacy Question: “Can this intervention work under ideal conditions?” (Petri dish)\nScaling/Policy Question: “Will this intervention work at scale, considering real-world constraints?”\nRequires a shift in research design from the outset.",
    "crumbs": [
      "Unit 6: Generalizability & Scalability",
      "Lecture Slides: Unit 6.1: Generating and Applying Scalable Insights"
    ]
  },
  {
    "objectID": "unit-6/lec-6-1-slides.html#when-is-this-critical-hfids-list-2024",
    "href": "unit-6/lec-6-1-slides.html#when-is-this-critical-hfids-list-2024",
    "title": "From Petri Dish to Policy: Generating and Applying Scalable Experimental Insights",
    "section": "When is This Critical? HFIDs (List 2024)",
    "text": "When is This Critical? HFIDs (List 2024)\nFlipping the model is most crucial for:\n\nHigh Fixed Cost Interventions: Expensive initial setup/testing.\nImpatient Decision-makers: Risk of premature scaling based on early, optimistic results.\n\n(Diagram below)",
    "crumbs": [
      "Unit 6: Generalizability & Scalability",
      "Lecture Slides: Unit 6.1: Generating and Applying Scalable Insights"
    ]
  },
  {
    "objectID": "unit-6/lec-6-1-slides.html#method-1-backward-induction",
    "href": "unit-6/lec-6-1-slides.html#method-1-backward-induction",
    "title": "From Petri Dish to Policy: Generating and Applying Scalable Experimental Insights",
    "section": "Method 1: Backward Induction",
    "text": "Method 1: Backward Induction\n\nStart by imagining the fully scaled program.\nWhat will the constraints be (budget, staff, context, politics)?\nWhat are the potential failure points (Vital Signs)?\nDesign the initial research to gather information relevant to these scaled realities.",
    "crumbs": [
      "Unit 6: Generalizability & Scalability",
      "Lecture Slides: Unit 6.1: Generating and Applying Scalable Insights"
    ]
  },
  {
    "objectID": "unit-6/lec-6-1-slides.html#method-2-option-c-thinking-list-2024",
    "href": "unit-6/lec-6-1-slides.html#method-2-option-c-thinking-list-2024",
    "title": "From Petri Dish to Policy: Generating and Applying Scalable Experimental Insights",
    "section": "Method 2: “Option C Thinking” (List 2024)",
    "text": "Method 2: “Option C Thinking” (List 2024)\nAugment standard A/B testing:\n\nA (Control): Baseline / No intervention.\nB (Efficacy Test): Intervention under ideal conditions.\nC (Scale Test): Intervention under realistic scaling constraints.\nCompare A vs. B (Can it work?) AND A vs. C (Will it likely work at scale?).\n\n(Diagram below)",
    "crumbs": [
      "Unit 6: Generalizability & Scalability",
      "Lecture Slides: Unit 6.1: Generating and Applying Scalable Insights"
    ]
  },
  {
    "objectID": "unit-6/lec-6-1-slides.html#example-parenting-intervention-in-china",
    "href": "unit-6/lec-6-1-slides.html#example-parenting-intervention-in-china",
    "title": "From Petri Dish to Policy: Generating and Applying Scalable Experimental Insights",
    "section": "Example: Parenting Intervention in China",
    "text": "Example: Parenting Intervention in China\n\nHigh Fixed Costs (developing curriculum, training home visitors).\nBackward Induction: Realized home visitor quality would vary at scale.\nOption C Applied: Didn’t just hire superstar home visitors (Arm B). Home visitors from the existing staff of the local family planning commission (part of Arm C logic).\nAllowed testing if the program worked with a representative range of home visitor quality, providing scaling confidence on that dimension.",
    "crumbs": [
      "Unit 6: Generalizability & Scalability",
      "Lecture Slides: Unit 6.1: Generating and Applying Scalable Insights"
    ]
  },
  {
    "objectID": "unit-6/lec-6-1-slides.html#part-iv-applying-existing-evidence-across-contexts",
    "href": "unit-6/lec-6-1-slides.html#part-iv-applying-existing-evidence-across-contexts",
    "title": "From Petri Dish to Policy: Generating and Applying Scalable Experimental Insights",
    "section": "Part IV: Applying Existing Evidence Across Contexts",
    "text": "Part IV: Applying Existing Evidence Across Contexts",
    "crumbs": [
      "Unit 6: Generalizability & Scalability",
      "Lecture Slides: Unit 6.1: Generating and Applying Scalable Insights"
    ]
  },
  {
    "objectID": "unit-6/lec-6-1-slides.html#using-evidence-globally-informed-locally-grounded",
    "href": "unit-6/lec-6-1-slides.html#using-evidence-globally-informed-locally-grounded",
    "title": "From Petri Dish to Policy: Generating and Applying Scalable Experimental Insights",
    "section": "Using Evidence: Globally Informed, Locally Grounded",
    "text": "Using Evidence: Globally Informed, Locally Grounded\n\nWe often need to assess if findings from elsewhere apply here.\nNeed a structured way to combine general lessons with local knowledge.\nEffectively applying evidence requires integrating insights from the broader research literature (including RCTs) with a thorough understanding of the specific local context. You need both; one doesn’t replace the other.\n\nComplements, not substitutes.",
    "crumbs": [
      "Unit 6: Generalizability & Scalability",
      "Lecture Slides: Unit 6.1: Generating and Applying Scalable Insights"
    ]
  },
  {
    "objectID": "unit-6/lec-6-1-slides.html#a-framework-for-generalizability-glennerster",
    "href": "unit-6/lec-6-1-slides.html#a-framework-for-generalizability-glennerster",
    "title": "From Petri Dish to Policy: Generating and Applying Scalable Experimental Insights",
    "section": "A Framework for Generalizability (Glennerster)",
    "text": "A Framework for Generalizability (Glennerster)\nSystematically assess relevance by examining four areas:\n\nTheory/Mechanisms: Unpack the ‘how’ and ‘why’.\nLocal Conditions: Does the context match the theory’s needs?\nGeneral Behavior: How strong is evidence for the core mechanism?\nLocal Implementation: Can we deliver it faithfully here?",
    "crumbs": [
      "Unit 6: Generalizability & Scalability",
      "Lecture Slides: Unit 6.1: Generating and Applying Scalable Insights"
    ]
  },
  {
    "objectID": "unit-6/lec-6-1-slides.html#step-1-program-theory",
    "href": "unit-6/lec-6-1-slides.html#step-1-program-theory",
    "title": "From Petri Dish to Policy: Generating and Applying Scalable Experimental Insights",
    "section": "Step 1: Program Theory",
    "text": "Step 1: Program Theory\n\nDeconstruct the “black box”.\nMap the causal chain (Inputs -&gt; Activities -&gt; Outputs -&gt; Outcomes).\n\ni.e. your Theory of Change.\n\nWhat are the key assumptions/conditions required for the program to work?\n*Example (Immunization Incentives):\n\nAssumes parents want to immunize, can access clinic\nbut procrastinate or struggle with persistence. (Glennerster)",
    "crumbs": [
      "Unit 6: Generalizability & Scalability",
      "Lecture Slides: Unit 6.1: Generating and Applying Scalable Insights"
    ]
  },
  {
    "objectID": "unit-6/lec-6-1-slides.html#step-2-local-conditions",
    "href": "unit-6/lec-6-1-slides.html#step-2-local-conditions",
    "title": "From Petri Dish to Policy: Generating and Applying Scalable Experimental Insights",
    "section": "Step 2: Local Conditions",
    "text": "Step 2: Local Conditions\n\nDoes the problem exist here? (Needs Assessment)\nDo the key conditions from the theory hold in this specific context?\nUse local data (quantitative, qualitative, descriptive).\nExample (Immunization):\n\nCompare immunization drop-off patterns.\nHigh initial uptake but poor completion suggests persistence issue (like India study).\nLow initial uptake suggests access/acceptability issues, making incentives less relevant alone.",
    "crumbs": [
      "Unit 6: Generalizability & Scalability",
      "Lecture Slides: Unit 6.1: Generating and Applying Scalable Insights"
    ]
  },
  {
    "objectID": "unit-6/lec-6-1-slides.html#step-3-general-behavior-mechanism",
    "href": "unit-6/lec-6-1-slides.html#step-3-general-behavior-mechanism",
    "title": "From Petri Dish to Policy: Generating and Applying Scalable Experimental Insights",
    "section": "Step 3: General Behavior / Mechanism",
    "text": "Step 3: General Behavior / Mechanism\n\nWhat is the core behavioral principle driving the effect?\nHow strong is the broader evidence for this principle (beyond the single study)?\nLook across different types of studies testing the same underlying behavior.\nExample (Immunization):\n\nEvidence on present bias, effects of small incentives/costs on health behaviors (deworming, CCTs, HIV tests, contraception).\nPeople are price-sensitive for preventative health.",
    "crumbs": [
      "Unit 6: Generalizability & Scalability",
      "Lecture Slides: Unit 6.1: Generating and Applying Scalable Insights"
    ]
  },
  {
    "objectID": "unit-6/lec-6-1-slides.html#step-4-local-implementation-capacity",
    "href": "unit-6/lec-6-1-slides.html#step-4-local-implementation-capacity",
    "title": "From Petri Dish to Policy: Generating and Applying Scalable Experimental Insights",
    "section": "Step 4: Local Implementation Capacity",
    "text": "Step 4: Local Implementation Capacity\n\nCan we implement the program here with fidelity to its core components?\nWho will implement (Govt vs NGO)? Do they have capacity, resources, buy-in?\nAre there logistical challenges unique to this context?\nMay require a process evaluation or pilot, not necessarily another impact RCT, to assess feasibility.",
    "crumbs": [
      "Unit 6: Generalizability & Scalability",
      "Lecture Slides: Unit 6.1: Generating and Applying Scalable Insights"
    ]
  },
  {
    "objectID": "unit-6/lec-6-1-slides.html#applying-the-framework-key-question",
    "href": "unit-6/lec-6-1-slides.html#applying-the-framework-key-question",
    "title": "From Petri Dish to Policy: Generating and Applying Scalable Experimental Insights",
    "section": "Applying the Framework: Key Question",
    "text": "Applying the Framework: Key Question\n\nInstead of: “Do RCTs replicate?”\nAsk: “What’s the theory of change? Do local conditions support it? How strong is the general behavioral evidence? Can we implement it here?”",
    "crumbs": [
      "Unit 6: Generalizability & Scalability",
      "Lecture Slides: Unit 6.1: Generating and Applying Scalable Insights"
    ]
  },
  {
    "objectID": "unit-6/lec-6-1-slides.html#part-v-synthesis-takeaways-for-phd-research",
    "href": "unit-6/lec-6-1-slides.html#part-v-synthesis-takeaways-for-phd-research",
    "title": "From Petri Dish to Policy: Generating and Applying Scalable Experimental Insights",
    "section": "Part V: Synthesis & Takeaways for PhD Research",
    "text": "Part V: Synthesis & Takeaways for PhD Research",
    "crumbs": [
      "Unit 6: Generalizability & Scalability",
      "Lecture Slides: Unit 6.1: Generating and Applying Scalable Insights"
    ]
  },
  {
    "objectID": "unit-6/lec-6-1-slides.html#takeaways-for-your-research",
    "href": "unit-6/lec-6-1-slides.html#takeaways-for-your-research",
    "title": "From Petri Dish to Policy: Generating and Applying Scalable Experimental Insights",
    "section": "Takeaways for Your Research",
    "text": "Takeaways for Your Research\n\nBe Critical Consumers: Apply the Glennerster framework when reading literature. Ask: Would this generalize? Why/why not?\nDesign Thoughtfully:\n\nClearly articulate your Theory of Change.\nConsider List’s Vital Signs – how might they affect your study or its scalability?\nIf feasible (esp. for potential HFIDs), consider ‘Option C’ elements.\nMeasure mechanisms and heterogeneity.\n\nDocument Diligently: Detail your context, program specifics, and implementation process – this aids future generalization.\nContribute Broadly: Aim for findings that inform fundamental behaviors/principles, not just a single program test.",
    "crumbs": [
      "Unit 6: Generalizability & Scalability",
      "Lecture Slides: Unit 6.1: Generating and Applying Scalable Insights"
    ]
  },
  {
    "objectID": "unit-6/lec-6-1-slides.html#connecting-the-dots-the-impact-evaluation-as-intervention",
    "href": "unit-6/lec-6-1-slides.html#connecting-the-dots-the-impact-evaluation-as-intervention",
    "title": "From Petri Dish to Policy: Generating and Applying Scalable Experimental Insights",
    "section": "Connecting the Dots: The Impact Evaluation as Intervention",
    "text": "Connecting the Dots: The Impact Evaluation as Intervention\n\nThis course covered the journey: Causal inference, robust design, measurement, analysis -&gt; Ensuring internal validity.\nThis unit adds Generalizability & Scale -&gt; Crucial for external validity and policy impact.\nThe Meta-Point: Thinking about scale forces us back to the beginning (the research question). An impact evaluation is itself an intervention on the policy process.\nOur implicit theory of change for the evaluation is that by conducting it rigorously and considering its applicability (generalizability, scalability), we enable better-informed decisions.\nEach stage (design, data, ethics, generalizability, scale) serves this ultimate goal.",
    "crumbs": [
      "Unit 6: Generalizability & Scalability",
      "Lecture Slides: Unit 6.1: Generating and Applying Scalable Insights"
    ]
  },
  {
    "objectID": "unit-6/lec-6-1-slides.html#looking-ahead-your-role-as-evidence-generators-users",
    "href": "unit-6/lec-6-1-slides.html#looking-ahead-your-role-as-evidence-generators-users",
    "title": "From Petri Dish to Policy: Generating and Applying Scalable Experimental Insights",
    "section": "Looking Ahead: Your Role as Evidence Generators & Users",
    "text": "Looking Ahead: Your Role as Evidence Generators & Users\n\nAs PhD researchers, you will contribute to this evidence lifecycle.\nYour Dissertation: Think critically now about the potential generalizability and scalability of your research questions and findings. Use the frameworks discussed.\nBeyond: Whether in academia, policy, or practice, you’ll need to:\n\nCritically evaluate existing evidence.\nDesign studies with an eye towards future application.\nCommunicate findings clearly, including limitations related to context and scale.\n\nThe goal is not just publications, but meaningful, applicable knowledge.",
    "crumbs": [
      "Unit 6: Generalizability & Scalability",
      "Lecture Slides: Unit 6.1: Generating and Applying Scalable Insights"
    ]
  },
  {
    "objectID": "unit-6/lec-6-1-slides.html#final-thought",
    "href": "unit-6/lec-6-1-slides.html#final-thought",
    "title": "From Petri Dish to Policy: Generating and Applying Scalable Experimental Insights",
    "section": "Final Thought",
    "text": "Final Thought\n\nDesigning for scale and applying evidence rigorously requires more upfront thinking.\nBUT: It increases the likelihood that our research contributes meaningfully to policy and avoids the pitfalls of voltage drop and misapplication.\nIt builds long-term trust in the value of evidence.",
    "crumbs": [
      "Unit 6: Generalizability & Scalability",
      "Lecture Slides: Unit 6.1: Generating and Applying Scalable Insights"
    ]
  },
  {
    "objectID": "unit-6/lec-6-1-slides.html#final-questions",
    "href": "unit-6/lec-6-1-slides.html#final-questions",
    "title": "From Petri Dish to Policy: Generating and Applying Scalable Experimental Insights",
    "section": "Final Questions?",
    "text": "Final Questions?",
    "crumbs": [
      "Unit 6: Generalizability & Scalability",
      "Lecture Slides: Unit 6.1: Generating and Applying Scalable Insights"
    ]
  },
  {
    "objectID": "unit-6/lec-6-1-slides.html#core-references",
    "href": "unit-6/lec-6-1-slides.html#core-references",
    "title": "From Petri Dish to Policy: Generating and Applying Scalable Experimental Insights",
    "section": "Core References",
    "text": "Core References\n\nList, J. A. (2024). Optimally generate policy-based evidence before scaling. Nature, 626(7998), 491–499. https://doi.org/10.1038/s41586-023-06972-y\nGlennerster, R. (Based on J-PAL Lecture Materials/Transcript). Globally informed, locally grounded decision making. [Note: Cite specific J-PAL source if available, otherwise use transcript context].\nAl-Ubaydli, O., List, J. A., & Suskind, D. L. (2019). The Science of Using Science: Towards an Understanding of the Threats to Scaling Experiments (Working Paper No. 2019-73). Becker Friedman Institute for Economics Working Paper Series. [Mention as underpinning List 2024]",
    "crumbs": [
      "Unit 6: Generalizability & Scalability",
      "Lecture Slides: Unit 6.1: Generating and Applying Scalable Insights"
    ]
  },
  {
    "objectID": "project.html",
    "href": "project.html",
    "title": "Semester Project Description",
    "section": "",
    "text": "With the semester project, you’ll dive into the full experimental research lifecycle and apply the tools and techniques you’re learning in this course. Working in your assigned group, you’ll design an experiment to answer a meaningful research question of your choosing, develop a pre-analysis plan, and analyze data (we’ll simulate the data given time constraints). By the end of the semester, you’ll have a manuscript and replication package that showcase your ability to design, execute, and report on a rigorous experimental research project. This is your opportunity to apply what we learn in class to a topic of interest.\n\n\n\n\n\n\nThe Pre-Analysis Plan (PAP) ensures your experiment is well-conceived, transparent, and analytically rigorous. This document lays out your research design and specifies how you will analyze the data.\n\n\n\nIntroduction and Background: Explain your research question and its significance.\n\nHypotheses: Clearly state primary and secondary hypotheses.\n\nExperimental Design:\n\nType of experiment (e.g., lab, online A/B test, field).\n\nRandom assignment procedure.\n\nTreatment and control conditions.\n\nParticipant details (e.g., recruitment, sample size, inclusion criteria).\n\n\nKey Outcomes: Define primary and secondary outcomes and describe how they will be measured.\n\nAnalytical Strategy:\n\nSpecify statistical methods and models.\n\nIncorporate analysis related to compliance or heterogeneous treatment effects.\n\nUse a machine learning approach (e.g., predicting treatment heterogeneity or analyzing secondary outcomes).\n\nAddress how you will handle multiple hypotheses testing, if applicable.\n\n\nLimitations: Discuss potential weaknesses and their impact on findings.\n\n\n\n\n\nClarity and Completeness (40%)\n\nRigor of Design and Analysis (40%)\n\nFeasibility and Practicality (20%)\n\n\n\n\n\nPre-Analysis Plan Template\nFor guidance on pre-analysis plans: World Bank DIME Wiki\nFor examples of pre-analysis plans: AEA RCT Registry\n\n\n\n\n\n\nThis deliverable includes a journal-style manuscript and a replication package to showcase your research findings and ensure reproducibility.\n\n\n\nFinal Manuscript: (10–15 pages)\n\nAbstract: Concise summary of your study and findings.\nIntroduction: Background and motivation for your research.\nMethods: Detailed description of your experimental design, treatments, and analytical approach.\nResults: Clear presentation of findings, including tables and figures.\nDiscussion: Interpretation, limitations, and implications for future research or policy.\nReferences: Properly formatted citations.\n\nReplication Package:\n\nSimulated Data: Dataset formatted for analysis.\nCode: Well-documented scripts (R or Python recommended) for data cleaning and analysis, incorporating machine learning where appropriate.\nREADME File: Detailed instructions for replicating your analysis.\n\n\n\n\n\n\nManuscript:\n\nWriting Quality and Organization (20%)\nClarity and Depth of Analysis (30%)\n\nReplication Package:\n\nReproducibility (30%)\nCompleteness and Documentation (20%)\n\n\n\n\n\n\n\n\n\nNow - April 9: Work with team on pre-analysis plan.\nApril 9: Submit Pre-Analysis Plan for feedback.\nApril 9 - April 28: Simulate and analyze data; draft your final deliverable.\nApril 28: Submit Final Manuscript and Replication Package.\n\n\n\n\n\n\nPre-Analysis Plan: 50%\nFinal Manuscript and Replication Package: 50%",
    "crumbs": [
      "Semester Project",
      "Project Description"
    ]
  },
  {
    "objectID": "project.html#overview",
    "href": "project.html#overview",
    "title": "Semester Project Description",
    "section": "",
    "text": "With the semester project, you’ll dive into the full experimental research lifecycle and apply the tools and techniques you’re learning in this course. Working in your assigned group, you’ll design an experiment to answer a meaningful research question of your choosing, develop a pre-analysis plan, and analyze data (we’ll simulate the data given time constraints). By the end of the semester, you’ll have a manuscript and replication package that showcase your ability to design, execute, and report on a rigorous experimental research project. This is your opportunity to apply what we learn in class to a topic of interest.",
    "crumbs": [
      "Semester Project",
      "Project Description"
    ]
  },
  {
    "objectID": "project.html#deliverables",
    "href": "project.html#deliverables",
    "title": "Semester Project Description",
    "section": "",
    "text": "The Pre-Analysis Plan (PAP) ensures your experiment is well-conceived, transparent, and analytically rigorous. This document lays out your research design and specifies how you will analyze the data.\n\n\n\nIntroduction and Background: Explain your research question and its significance.\n\nHypotheses: Clearly state primary and secondary hypotheses.\n\nExperimental Design:\n\nType of experiment (e.g., lab, online A/B test, field).\n\nRandom assignment procedure.\n\nTreatment and control conditions.\n\nParticipant details (e.g., recruitment, sample size, inclusion criteria).\n\n\nKey Outcomes: Define primary and secondary outcomes and describe how they will be measured.\n\nAnalytical Strategy:\n\nSpecify statistical methods and models.\n\nIncorporate analysis related to compliance or heterogeneous treatment effects.\n\nUse a machine learning approach (e.g., predicting treatment heterogeneity or analyzing secondary outcomes).\n\nAddress how you will handle multiple hypotheses testing, if applicable.\n\n\nLimitations: Discuss potential weaknesses and their impact on findings.\n\n\n\n\n\nClarity and Completeness (40%)\n\nRigor of Design and Analysis (40%)\n\nFeasibility and Practicality (20%)\n\n\n\n\n\nPre-Analysis Plan Template\nFor guidance on pre-analysis plans: World Bank DIME Wiki\nFor examples of pre-analysis plans: AEA RCT Registry\n\n\n\n\n\n\nThis deliverable includes a journal-style manuscript and a replication package to showcase your research findings and ensure reproducibility.\n\n\n\nFinal Manuscript: (10–15 pages)\n\nAbstract: Concise summary of your study and findings.\nIntroduction: Background and motivation for your research.\nMethods: Detailed description of your experimental design, treatments, and analytical approach.\nResults: Clear presentation of findings, including tables and figures.\nDiscussion: Interpretation, limitations, and implications for future research or policy.\nReferences: Properly formatted citations.\n\nReplication Package:\n\nSimulated Data: Dataset formatted for analysis.\nCode: Well-documented scripts (R or Python recommended) for data cleaning and analysis, incorporating machine learning where appropriate.\nREADME File: Detailed instructions for replicating your analysis.\n\n\n\n\n\n\nManuscript:\n\nWriting Quality and Organization (20%)\nClarity and Depth of Analysis (30%)\n\nReplication Package:\n\nReproducibility (30%)\nCompleteness and Documentation (20%)",
    "crumbs": [
      "Semester Project",
      "Project Description"
    ]
  },
  {
    "objectID": "project.html#timeline",
    "href": "project.html#timeline",
    "title": "Semester Project Description",
    "section": "",
    "text": "Now - April 9: Work with team on pre-analysis plan.\nApril 9: Submit Pre-Analysis Plan for feedback.\nApril 9 - April 28: Simulate and analyze data; draft your final deliverable.\nApril 28: Submit Final Manuscript and Replication Package.",
    "crumbs": [
      "Semester Project",
      "Project Description"
    ]
  },
  {
    "objectID": "project.html#grading-breakdown",
    "href": "project.html#grading-breakdown",
    "title": "Semester Project Description",
    "section": "",
    "text": "Pre-Analysis Plan: 50%\nFinal Manuscript and Replication Package: 50%",
    "crumbs": [
      "Semester Project",
      "Project Description"
    ]
  },
  {
    "objectID": "quiz-generator/index.html",
    "href": "quiz-generator/index.html",
    "title": "Random Question Generator",
    "section": "",
    "text": "Welcome to the HPM 883 Quiz-o-matic!\nTest your knowledge with randomly generated questions from course content. This interactive tool draws questions from lectures across all course units. (These are representative of the types of questions you can expect to see on exams. You’re welcome! :))\n\n\n\n\n\n\nTip\n\n\n\nHow to Use: Select the number of questions you want to practice with, answer each question, then submit to see your results and explanations. Generate a new quiz to practice with different questions.\nNote: It may mark free-form questions as incorrect; just compare to the “correct answer” provided by the tool.\n\n\n\n\n\n\n\n\nImportant\n\n\n\nUse this tool responsibly: This tool uses a language model (AI) to generate questions and answers. As a result, these questions may be incorrect, have unclear wording, or be completely unrelated to the course content. Although I did my best to ensure the quality and relevence of the questions is generates, language models are succeptible to hallucinations. Please take this into consideration when using this tool and report any issues you find to me so that I can fix errors.\nFound an Issue?: If you believe there’s a problem with any question (unclear wording, incorrect answer, etc.), click the flag icon (⚑) next to the question. You’ll be prompted to describe the issue, which will help improve the question bank.\nTo share flagged questions with Sean, click the gear icon (⚙️) in the top-right corner, enter the admin password (admin123) when prompted, then click “Export CSV” to download your flagged questions report. Please send this CSV file to Sean in Slack. Note: The gear icon doesn’t show on the page with results.\n\n\n\n\n\n\n\n\nNote\n\n\n\nThis quiz generator currently pulls questions from content covered in all course units including:\n\nUnit 3: Introduction to Machine Learning\nUnit 4: Estimation & Heterogeneity",
    "crumbs": [
      "Resources",
      "Random Question Generator"
    ]
  },
  {
    "objectID": "unit-3/unit-3-ml.html",
    "href": "unit-3/unit-3-ml.html",
    "title": "Unit 3: Supervised Machine Learning Crash Course",
    "section": "",
    "text": "Lecture Slides: Unit 3.3",
    "crumbs": [
      "Unit 3: Supervised Learning",
      "Unit 3: Supervised Learning"
    ]
  },
  {
    "objectID": "unit-3/unit-3-ml.html#unit-overview",
    "href": "unit-3/unit-3-ml.html#unit-overview",
    "title": "Unit 3: Supervised Machine Learning Crash Course",
    "section": "",
    "text": "Lecture Slides: Unit 3.3",
    "crumbs": [
      "Unit 3: Supervised Learning",
      "Unit 3: Supervised Learning"
    ]
  },
  {
    "objectID": "unit-3/unit-3-ml.html#labs",
    "href": "unit-3/unit-3-ml.html#labs",
    "title": "Unit 3: Supervised Machine Learning Crash Course",
    "section": "Labs",
    "text": "Labs\n\nReplication of Dynamic Complementarity Analysis",
    "crumbs": [
      "Unit 3: Supervised Learning",
      "Unit 3: Supervised Learning"
    ]
  },
  {
    "objectID": "unit-3/unit-3-ml.html#unit-3.1-intro-to-ml",
    "href": "unit-3/unit-3-ml.html#unit-3.1-intro-to-ml",
    "title": "Unit 3: Supervised Machine Learning Crash Course",
    "section": "Unit 3.1: Intro to ML",
    "text": "Unit 3.1: Intro to ML\n\nPreparation\n\nRead:\n\nISLR Chapter 2\nOptional:\n\nMachine Learning: An Applied Econometric Approach\nLessons and tips for designing a machine learning study using EHR data\n\n\nWatch\n\nSusan Athey - Machine Learning and Economics\n\nSlides\n\nJann Spiess - Applied Machine Learning - An Introduction\n\nslides\n\nJann Spiess - Applied ML: Secret Sauce\n\nslides\n\nJann Spiess - Applied ML: Prediction vs Estimation\n\nslides",
    "crumbs": [
      "Unit 3: Supervised Learning",
      "Unit 3: Supervised Learning"
    ]
  },
  {
    "objectID": "unit-3/unit-3-ml.html#unit-3.2-lasso-and-friends-linear-regression-with-high-dimensional-data",
    "href": "unit-3/unit-3-ml.html#unit-3.2-lasso-and-friends-linear-regression-with-high-dimensional-data",
    "title": "Unit 3: Supervised Machine Learning Crash Course",
    "section": "Unit 3.2: Lasso and friends: Linear Regression with High-Dimensional Data",
    "text": "Unit 3.2: Lasso and friends: Linear Regression with High-Dimensional Data\n\nPreparation\n\nRead:\n\nISLR Chapter 5\n\nWatch:\n\nStatistical Learning - Bias-Variance Traeoff\nStatistical Learning - Cross Validation\nStatistical Learning - The Lasso\n\nDo:\n\nLinear Penalized Regressions",
    "crumbs": [
      "Unit 3: Supervised Learning",
      "Unit 3: Supervised Learning"
    ]
  },
  {
    "objectID": "unit-3/unit-3-ml.html#unit-3.3-tree-based-methods",
    "href": "unit-3/unit-3-ml.html#unit-3.3-tree-based-methods",
    "title": "Unit 3: Supervised Machine Learning Crash Course",
    "section": "Unit 3.3: Tree-based Methods",
    "text": "Unit 3.3: Tree-based Methods\n\nPreparation\n\nRead:\n\nISLR Chapter 6.1, 6.2\nCausal ML - 8.1, 8.2",
    "crumbs": [
      "Unit 3: Supervised Learning",
      "Unit 3: Supervised Learning"
    ]
  },
  {
    "objectID": "unit-3/unit-3-ml.html#other-resources",
    "href": "unit-3/unit-3-ml.html#other-resources",
    "title": "Unit 3: Supervised Machine Learning Crash Course",
    "section": "Other Resources",
    "text": "Other Resources\n\nSupervised Learning (Logistic regression, decision trees, random forest)\nUnsupervised Learning (Clustering, PCA)\nMIT Course: Machine Learning for Healthcare",
    "crumbs": [
      "Unit 3: Supervised Learning",
      "Unit 3: Supervised Learning"
    ]
  },
  {
    "objectID": "unit-4/lec-4-2-slides.html#learning-objectives",
    "href": "unit-4/lec-4-2-slides.html#learning-objectives",
    "title": "Lecture 4.2: Heterogeneous Treatment Effects",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nBy the end of this session, you should be able to:\n\nUnderstand the concept of heterogeneous treatment effects and why they matter\nDistinguish between individual treatment effects, average treatment effects, and conditional average treatment effects\nApply different approaches to analyze treatment effect heterogeneity\nUnderstand the strengths and limitations of pre-specified and data-driven approaches\nInterpret heterogeneity findings in published research",
    "crumbs": [
      "Unit 4: Mechanisms",
      "Lecture Slides: Unit 4.2: Heterogeneous Treatment Effects & Moderation"
    ]
  },
  {
    "objectID": "unit-4/lec-4-2-slides.html#individual-treatment-effects",
    "href": "unit-4/lec-4-2-slides.html#individual-treatment-effects",
    "title": "Lecture 4.2: Heterogeneous Treatment Effects",
    "section": "Individual Treatment Effects",
    "text": "Individual Treatment Effects\nIndividual Treatment Effect (ITE): \\(T_i = Y_i(1) - Y_i(0)\\)\n\nThe individual-specific causal effect of the treatment\n\\(Y_i(1)\\) is the potential outcome under treatment\n\\(Y_i(0)\\) is the potential outcome under control\n\nFundamental Problem of Causal Inference:\n\nWe only observe one potential outcome per individual\nIt’s impossible to directly observe individual treatment effects\nThis makes identifying individual effects infeasible in between-subjects designs",
    "crumbs": [
      "Unit 4: Mechanisms",
      "Lecture Slides: Unit 4.2: Heterogeneous Treatment Effects & Moderation"
    ]
  },
  {
    "objectID": "unit-4/lec-4-2-slides.html#average-treatment-effect-ate",
    "href": "unit-4/lec-4-2-slides.html#average-treatment-effect-ate",
    "title": "Lecture 4.2: Heterogeneous Treatment Effects",
    "section": "Average Treatment Effect (ATE)",
    "text": "Average Treatment Effect (ATE)\nGiven the fundamental problem, we focus on the Average Treatment Effect:\n\\[\n\\tau = E[Y_i(1) - Y_i(0)]\n\\]\n\nATE answers: “What is the expected change in outcome if we randomly select a unit from the population and apply the treatment?”\nATE is estimable through randomization\nLimitation: Assuming equal effect for all subjects is often unrealistic\nInterest in how treatment effects vary with subject characteristics",
    "crumbs": [
      "Unit 4: Mechanisms",
      "Lecture Slides: Unit 4.2: Heterogeneous Treatment Effects & Moderation"
    ]
  },
  {
    "objectID": "unit-4/lec-4-2-slides.html#examples-of-interest-in-heterogeneity",
    "href": "unit-4/lec-4-2-slides.html#examples-of-interest-in-heterogeneity",
    "title": "Lecture 4.2: Heterogeneous Treatment Effects",
    "section": "Examples of Interest in Heterogeneity",
    "text": "Examples of Interest in Heterogeneity\nReal-world examples where heterogeneity matters:\n\nPreventive screening effectiveness: How the benefits of cancer screenings or other preventive services vary by age, risk status, and other patient characteristics\nTelehealth disparities: How rural and urban populations may experience different benefits from telehealth interventions (particularly relevant given the expansion of telehealth)\nCare coordination effectiveness: How complex patients with multiple comorbidities might benefit differently from care coordination programs\nAdherence interventions: How socioeconomic factors might moderate the effectiveness of programs to improve medication adherence\nValue-based payment impacts: How provider characteristics influence responses to payment reform",
    "crumbs": [
      "Unit 4: Mechanisms",
      "Lecture Slides: Unit 4.2: Heterogeneous Treatment Effects & Moderation"
    ]
  },
  {
    "objectID": "unit-4/lec-4-2-slides.html#reasons-for-interest-in-heterogeneity",
    "href": "unit-4/lec-4-2-slides.html#reasons-for-interest-in-heterogeneity",
    "title": "Lecture 4.2: Heterogeneous Treatment Effects",
    "section": "Reasons for Interest in Heterogeneity",
    "text": "Reasons for Interest in Heterogeneity\n\n\nTargeting Treatment\n\nShould we direct interventions to certain types of people?\nCan help maximize impact with limited resources\nEthical considerations in allocation\n\nUnderstanding Mechanisms\n\nWhy did treatment work or not work?\nHelps refine interventions and theory\n\nExternal Validity\n\nWill effects generalize to new populations?\nCrucial for policy scale-up decisions",
    "crumbs": [
      "Unit 4: Mechanisms",
      "Lecture Slides: Unit 4.2: Heterogeneous Treatment Effects & Moderation"
    ]
  },
  {
    "objectID": "unit-4/lec-4-2-slides.html#moderation-analysis",
    "href": "unit-4/lec-4-2-slides.html#moderation-analysis",
    "title": "Lecture 4.2: Heterogeneous Treatment Effects",
    "section": "Moderation Analysis",
    "text": "Moderation Analysis\nModeration Analysis: Examines how treatment effects differ based on pre-determined covariates (moderators)\nTypes of moderation analysis:\n\nDescriptive: Do treatment effects differ based on pre-determined covariates?\nCausal: If we independently varied the moderator, would we see the same CATEs?\n\nInterpretation:\n\nFocus not just on measuring heterogeneities, but understanding why they exist\nDescriptive heterogeneity doesn’t necessarily imply causal moderation",
    "crumbs": [
      "Unit 4: Mechanisms",
      "Lecture Slides: Unit 4.2: Heterogeneous Treatment Effects & Moderation"
    ]
  },
  {
    "objectID": "unit-4/lec-4-2-slides.html#conditional-average-treatment-effects-cates",
    "href": "unit-4/lec-4-2-slides.html#conditional-average-treatment-effects-cates",
    "title": "Lecture 4.2: Heterogeneous Treatment Effects",
    "section": "Conditional Average Treatment Effects (CATEs)",
    "text": "Conditional Average Treatment Effects (CATEs)\nDefinition: \\[\n\\tau(x) = E[\\tau(x) | X_i = x] = E[Y_i(1, x) - Y_i(0, x) | X_i = x]\n\\]\n\nAverage treatment effect for a subgroup with characteristics \\(X_i = x\\)\nPotential interest in changes in CATEs: \\[\n\\Delta\\tau(X_i) = E[Y_i(1,1) - Y_i(1,0) - Y_i(0,1) - Y_i(0,0)]\n\\]\n\nLimitation:\n\nThis object is not directly observed in data\nEach unit only provides a single potential outcome\nEach unit has only a single level of \\(X_i\\)",
    "crumbs": [
      "Unit 4: Mechanisms",
      "Lecture Slides: Unit 4.2: Heterogeneous Treatment Effects & Moderation"
    ]
  },
  {
    "objectID": "unit-4/lec-4-2-slides.html#differences-in-cates",
    "href": "unit-4/lec-4-2-slides.html#differences-in-cates",
    "title": "Lecture 4.2: Heterogeneous Treatment Effects",
    "section": "Differences in CATEs",
    "text": "Differences in CATEs\nWe can measure differences between \\(\\tau(x)\\) for different levels of \\(X_i\\):\n\\[\n\\Delta\\hat\\tau(X_i) = \\tau(1) - \\tau(0) = E[\\tau_i(1) | X_i = 1] - E[\\tau_i(0) | X_i = 0]\n\\]\nImportant Note:\n\n\\(\\Delta\\hat\\tau(X_i)\\) is descriptive, not causal\nIt can show whether CATE is higher for different levels of \\(X_i\\)\nCannot answer whether there is a causal effect of \\(X\\) on the treatment effect\nBut still highly valuable for policy and implementation",
    "crumbs": [
      "Unit 4: Mechanisms",
      "Lecture Slides: Unit 4.2: Heterogeneous Treatment Effects & Moderation"
    ]
  },
  {
    "objectID": "unit-4/lec-4-2-slides.html#example",
    "href": "unit-4/lec-4-2-slides.html#example",
    "title": "Lecture 4.2: Heterogeneous Treatment Effects",
    "section": "Example",
    "text": "Example\nHome-based Parenting Intervention in Rural China (Sylvia et al. 2021)\nBackground:\n\nRandomized experiment evaluating home-based parenting program delivered by family planning cadres in rural China\nProgram significantly increased infant skill development after six months\nBeyond ATE, wanted to understand heterogeneous effects\n\nReasons for examining heterogeneity:\n\nTargeting: Identify which children benefited most\nGeneralization: Understand how effects might vary in other populations\nMechanisms: Gain insights into how the program worked",
    "crumbs": [
      "Unit 4: Mechanisms",
      "Lecture Slides: Unit 4.2: Heterogeneous Treatment Effects & Moderation"
    ]
  },
  {
    "objectID": "unit-4/lec-4-2-slides.html#approach-1-pre-specified-hypotheses",
    "href": "unit-4/lec-4-2-slides.html#approach-1-pre-specified-hypotheses",
    "title": "Lecture 4.2: Heterogeneous Treatment Effects",
    "section": "Approach 1: Pre-specified Hypotheses",
    "text": "Approach 1: Pre-specified Hypotheses\nSimple case: A small set of distinct subgroups that we hypothesize to summarize heterogeneity\nWith sufficient sample sizes per subgroup, treat each subgroup as a “mini-RCT”:\n\nEstimate \\(\\hat\\tau(X) = E[\\tau(X) | X_i = X]\\)\nHold \\(X\\) fixed in estimation\nCan be done separately or using interaction terms in regression\n\nExample: Analyzing treatment effects by baseline parental investment levels",
    "crumbs": [
      "Unit 4: Mechanisms",
      "Lecture Slides: Unit 4.2: Heterogeneous Treatment Effects & Moderation"
    ]
  },
  {
    "objectID": "unit-4/lec-4-2-slides.html#sample-size-and-hte",
    "href": "unit-4/lec-4-2-slides.html#sample-size-and-hte",
    "title": "Lecture 4.2: Heterogeneous Treatment Effects",
    "section": "Sample Size and HTE",
    "text": "Sample Size and HTE\n\nModeration analysis requires larger samples or larger MDEs\nConsiderations:\n\nModeration effects are often smaller than main effects\nYou can have smaller samples when comparing subgroups\nSubgroups don’t have to be evenly split\n\nDesign implications:\n\nConsider oversampling key subgroups\nPower calculations should account for heterogeneity analyses\nStratification can improve precision for key subgroups",
    "crumbs": [
      "Unit 4: Mechanisms",
      "Lecture Slides: Unit 4.2: Heterogeneous Treatment Effects & Moderation"
    ]
  },
  {
    "objectID": "unit-4/lec-4-2-slides.html#challenges-with-pre-specified-approaches",
    "href": "unit-4/lec-4-2-slides.html#challenges-with-pre-specified-approaches",
    "title": "Lecture 4.2: Heterogeneous Treatment Effects",
    "section": "Challenges with Pre-specified Approaches",
    "text": "Challenges with Pre-specified Approaches\n\nModeration analysis typically involves estimating a series of CATEs\nWith many covariates, quickly becomes infeasible\n\nParticularly with smaller samples\n\nContinuous covariates require functional form assumptions\nDifficult to discover/justify interactions of multiple variables ex post\n\nPartial solutions:\n\nPre-analysis plans\nData-driven approaches to predict heterogeneity",
    "crumbs": [
      "Unit 4: Mechanisms",
      "Lecture Slides: Unit 4.2: Heterogeneous Treatment Effects & Moderation"
    ]
  },
  {
    "objectID": "unit-4/lec-4-2-slides.html#approach-2-ml---causal-forests",
    "href": "unit-4/lec-4-2-slides.html#approach-2-ml---causal-forests",
    "title": "Lecture 4.2: Heterogeneous Treatment Effects",
    "section": "Approach 2: ML - Causal Forests",
    "text": "Approach 2: ML - Causal Forests\nCausal forests adapt random forest methods to estimate heterogeneous treatment effects:\n\nTrees are trained on subsets of data and features\nGoal is to partition data to maximize between-group heterogeneity while maintaining within-group homogeneity\nRepeated many times with random splits of the data\nITE estimated by averaging over subgroup effects across iterations\n\nAdvantages:\n\nCan handle high-dimensional covariates\nDoesn’t require functional form assumptions\nCan discover unexpected patterns of heterogeneity",
    "crumbs": [
      "Unit 4: Mechanisms",
      "Lecture Slides: Unit 4.2: Heterogeneous Treatment Effects & Moderation"
    ]
  },
  {
    "objectID": "unit-4/lec-4-2-slides.html#case-study-heterogeneity-in-chinas-parenting-intervention",
    "href": "unit-4/lec-4-2-slides.html#case-study-heterogeneity-in-chinas-parenting-intervention",
    "title": "Lecture 4.2: Heterogeneous Treatment Effects",
    "section": "Case Study: Heterogeneity in China’s Parenting Intervention",
    "text": "Case Study: Heterogeneity in China’s Parenting Intervention\nSylvia et al. (2021) study:\n\nEvaluated home-based parenting program delivered by family planning cadres in rural China\nUsed Generalized Random Forest (GRF) method to identify sources of heterogeneity\nTwo-stage approach:\n\nUsed GRF to identify important predictors of heterogeneity\nUsed these insights for traditional subgroup analysis",
    "crumbs": [
      "Unit 4: Mechanisms",
      "Lecture Slides: Unit 4.2: Heterogeneous Treatment Effects & Moderation"
    ]
  },
  {
    "objectID": "unit-4/lec-4-2-slides.html#grf-findings-variable-importance",
    "href": "unit-4/lec-4-2-slides.html#grf-findings-variable-importance",
    "title": "Lecture 4.2: Heterogeneous Treatment Effects",
    "section": "GRF Findings: Variable Importance",
    "text": "GRF Findings: Variable Importance\n\nKey predictors of heterogeneity: 1. Baseline parental investment (27.16%) 2. Baseline infant skills (16.73%) 3. Distance to FPC office (12.51%)",
    "crumbs": [
      "Unit 4: Mechanisms",
      "Lecture Slides: Unit 4.2: Heterogeneous Treatment Effects & Moderation"
    ]
  },
  {
    "objectID": "unit-4/lec-4-2-slides.html#grf-informed-heterogeneity-analysis",
    "href": "unit-4/lec-4-2-slides.html#grf-informed-heterogeneity-analysis",
    "title": "Lecture 4.2: Heterogeneous Treatment Effects",
    "section": "GRF-Informed Heterogeneity Analysis",
    "text": "GRF-Informed Heterogeneity Analysis",
    "crumbs": [
      "Unit 4: Mechanisms",
      "Lecture Slides: Unit 4.2: Heterogeneous Treatment Effects & Moderation"
    ]
  },
  {
    "objectID": "unit-4/lec-4-2-slides.html#grf-informed-heterogeneity-analysis-1",
    "href": "unit-4/lec-4-2-slides.html#grf-informed-heterogeneity-analysis-1",
    "title": "Lecture 4.2: Heterogeneous Treatment Effects",
    "section": "GRF-Informed Heterogeneity Analysis",
    "text": "GRF-Informed Heterogeneity Analysis\nFindings:\n\nTreatment effects significantly higher for children with low baseline parental investment\n\nFirst quartile: +0.456 SD higher effect\n\nChildren with below-median baseline skills benefited more\n\n+0.340 SD higher effect\n\nNo significant differences based on distance to FPC office\n\nConclusion: Program was most effective for disadvantaged children who lagged behind cognitively and had low initial parental investment",
    "crumbs": [
      "Unit 4: Mechanisms",
      "Lecture Slides: Unit 4.2: Heterogeneous Treatment Effects & Moderation"
    ]
  },
  {
    "objectID": "unit-4/lec-4-2-slides.html#key-takeaways",
    "href": "unit-4/lec-4-2-slides.html#key-takeaways",
    "title": "Lecture 4.2: Heterogeneous Treatment Effects",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nHeterogeneity analysis provides crucial insights beyond average effects\nTwo main approaches:\n\nPre-specified subgroup analysis (theory-driven)\nData-driven methods like causal forests (exploratory)\n\nBest practice: Use both approaches complementarily\nResults can guide:\n\nProgram targeting\nMechanism understanding\nExternal validity assessments",
    "crumbs": [
      "Unit 4: Mechanisms",
      "Lecture Slides: Unit 4.2: Heterogeneous Treatment Effects & Moderation"
    ]
  },
  {
    "objectID": "unit-4/lec-4-2-slides.html#using-heterogeneity-insights",
    "href": "unit-4/lec-4-2-slides.html#using-heterogeneity-insights",
    "title": "Lecture 4.2: Heterogeneous Treatment Effects",
    "section": "Using Heterogeneity Insights",
    "text": "Using Heterogeneity Insights\n\nProgram design: Target children with greatest potential benefit\nResource allocation: Direct resources where most effective\nImplementation science: Understand contextual factors affecting success\nTheory development: Refine understanding of causal mechanisms\nScale-up decisions: Predict effectiveness in new populations",
    "crumbs": [
      "Unit 4: Mechanisms",
      "Lecture Slides: Unit 4.2: Heterogeneous Treatment Effects & Moderation"
    ]
  },
  {
    "objectID": "unit-4/lec-4-2-slides.html#references",
    "href": "unit-4/lec-4-2-slides.html#references",
    "title": "Lecture 4.2: Heterogeneous Treatment Effects",
    "section": "References",
    "text": "References\nSylvia, S., Warrinnier, N., Luo, R., Yue, A., Attanasio, O., Medina, A., & Rozelle, S. (2021). From quantity to quality: Delivering a home-based parenting intervention through China’s family planning cadres. The Economic Journal, 131(635), 1365-1400.\nAthey, S., Tibshirani, J., & Wager, S. (2019). Generalized random forests. The Annals of Statistics, 47(2), 1148-1178.\nWager, S., & Athey, S. (2018). Estimation and inference of heterogeneous treatment effects using random forests. Journal of the American Statistical Association, 113(523), 1228-1242.",
    "crumbs": [
      "Unit 4: Mechanisms",
      "Lecture Slides: Unit 4.2: Heterogeneous Treatment Effects & Moderation"
    ]
  },
  {
    "objectID": "unit-5/unit-5.html",
    "href": "unit-5/unit-5.html",
    "title": "Unit 5: Threats to Validity",
    "section": "",
    "text": "In this unit, we will discuss the threats to validity in experimental settings. We will also discuss how to address these threats in design, implementation, and analysis.",
    "crumbs": [
      "Unit 5: Threats to Validity",
      "Unit 5: Threats to Validity"
    ]
  },
  {
    "objectID": "unit-5/unit-5.html#overview",
    "href": "unit-5/unit-5.html#overview",
    "title": "Unit 5: Threats to Validity",
    "section": "",
    "text": "In this unit, we will discuss the threats to validity in experimental settings. We will also discuss how to address these threats in design, implementation, and analysis.",
    "crumbs": [
      "Unit 5: Threats to Validity",
      "Unit 5: Threats to Validity"
    ]
  },
  {
    "objectID": "unit-5/unit-5.html#readings",
    "href": "unit-5/unit-5.html#readings",
    "title": "Unit 5: Threats to Validity",
    "section": "Readings",
    "text": "Readings\n\nRequired\n\nJPAL:Data Analysis for Social Science\nEGAP Methods Guide on LATE\n\n\n\nOptional\n\nImbens & Wooldridge (2007) Lecture 5 on LATE",
    "crumbs": [
      "Unit 5: Threats to Validity",
      "Unit 5: Threats to Validity"
    ]
  },
  {
    "objectID": "unit-5/unit-5.html#lecture",
    "href": "unit-5/unit-5.html#lecture",
    "title": "Unit 5: Threats to Validity",
    "section": "Lecture",
    "text": "Lecture\n\nLecture 5.0 Slides",
    "crumbs": [
      "Unit 5: Threats to Validity",
      "Unit 5: Threats to Validity"
    ]
  },
  {
    "objectID": "unit-5/unit-5.html#labs",
    "href": "unit-5/unit-5.html#labs",
    "title": "Unit 5: Threats to Validity",
    "section": "Labs",
    "text": "Labs\n\nLab 5: Peer PAP Review",
    "crumbs": [
      "Unit 5: Threats to Validity",
      "Unit 5: Threats to Validity"
    ]
  },
  {
    "objectID": "unit-2/lec-2-1-slides.html#experimental-design-an-econ-101-approach",
    "href": "unit-2/lec-2-1-slides.html#experimental-design-an-econ-101-approach",
    "title": "Unit 2.1: Optimal Experimental Design",
    "section": "Experimental Design: an Econ 101 Approach",
    "text": "Experimental Design: an Econ 101 Approach\n\nPreviously: statistical conclusion validity, power calculations, simulation\nNow we’re in control: designing experiments to maximize learning (i.e. statistical power)\n\nKey Question:\nHow to design an experiment to maximize power subject to constraints (e.g. budget, logistical)?\n\nAnswer:\nLearn some economics!\n\n\nWe’ve covered statistical conclusion validity, power calculations, and simulation. Now we’re shifting to designing experiments."
  },
  {
    "objectID": "unit-2/lec-2-1-slides.html#back-to-econ-101",
    "href": "unit-2/lec-2-1-slides.html#back-to-econ-101",
    "title": "Unit 2.1: Optimal Experimental Design",
    "section": "(Back) to Econ 101",
    "text": "(Back) to Econ 101\nOptimize: Weighing costs and benefits\n\nObjective:\n\nMaximize statistical power\nOr minimize the smallest effect we can see (the Minimum Detectable Effect)\n\nConstraints:\n\nResearch Budget\nlogistical limitations"
  },
  {
    "objectID": "unit-2/lec-2-1-slides.html#key-elements-in-sample-size-calculation",
    "href": "unit-2/lec-2-1-slides.html#key-elements-in-sample-size-calculation",
    "title": "Unit 2.1: Optimal Experimental Design",
    "section": "Key Elements in Sample Size Calculation",
    "text": "Key Elements in Sample Size Calculation\n\n\nSignificance level (\\(\\alpha\\)): The probability of a false positive (rejecting the null when it’s actually true).\nMinimum Detectable Effect (MDE): The smallest true effect size you want to be able to detect with high probability.\nPower (\\(1 - \\beta\\)): The probability of detecting a true effect (i.e., rejecting the null when it’s false).\n\n\n\n\n\n\n\n\n\nWarning\n\n\nMDE ≠ Expected effect size!!"
  },
  {
    "objectID": "unit-2/lec-2-1-slides.html#a-simple-example",
    "href": "unit-2/lec-2-1-slides.html#a-simple-example",
    "title": "Unit 2.1: Optimal Experimental Design",
    "section": "A Simple Example",
    "text": "A Simple Example\n\nYou’ve received a research grant for a two-arm study:\n\nControl group (no intervention)\nTreatment group (new health intervention)\n\nOutcome measure: \\[Y_i = \\text{Health and Happiness Index}\\]\nKey questions:\n\nHow many participants do you need?\nHow to split the sample between treatment and control?"
  },
  {
    "objectID": "unit-2/lec-2-1-slides.html#outcome-model",
    "href": "unit-2/lec-2-1-slides.html#outcome-model",
    "title": "Unit 2.1: Optimal Experimental Design",
    "section": "Outcome Model",
    "text": "Outcome Model\n\n\\(Y_i\\): Outcome for subject \\(i\\)\n\\(\\mathbf{X}_i\\): Observable variables\n\\(\\alpha_i\\): Unobserved effect (“innate personal quirks”)\n\\(\\tau_i\\): Person-specific treatment effect (\\(\\mathbb{E}[\\tau_i] = 0\\))\n\\(\\varepsilon_i\\): i.i.d. error term\n\n\\[Y_{i} \\;=\\; \\alpha_i \\;+\\; \\mathbf{X}_i \\,\\beta \\;+\\; \\bar{\\tau} \\,D_i \\;+\\; \\tau_i\\,D_i \\;+\\; \\varepsilon_i.\\tag{1}\\]\n\n\\(\\bar{\\tau}\\): Average treatment effect\n\\(\\tau_i\\): Idiosyncratic difference around \\(\\bar{\\tau}\\)"
  },
  {
    "objectID": "unit-2/lec-2-1-slides.html#unbiased-ate-estimator",
    "href": "unit-2/lec-2-1-slides.html#unbiased-ate-estimator",
    "title": "Unit 2.1: Optimal Experimental Design",
    "section": "Unbiased ATE Estimator",
    "text": "Unbiased ATE Estimator\n\\[\\hat{\\tau}\\;=\\; \\mathbb{E}[Y_i \\mid D_i=1]\\;-\\;\\mathbb{E}[Y_i \\mid D_i=0]\\]\n\nUnbiased due to randomization\n\\(D_i\\) independent of \\(\\alpha_i, \\tau_i,\\) and \\(\\varepsilon_i\\)\n\nThen we have the Variance of Estimated ATE\n\\[\\mathrm{Var}(\\hat{\\tau})\\;=\\;\\frac{\\sigma^2}{N}\\;=\\;\\frac{\\mathrm{Var}(\\varepsilon_i)}{\\;N \\,\\times\\, \\mathrm{Var}(D_i)\\,}.\\tag{2}\\]\n\n\\(\\mathrm{Var}(\\varepsilon_i)\\): Variance of unobserved “noise”\n\\(N\\): Total number of units\n\\(\\mathrm{Var}(D_i)\\): Variance of treatment assignment indicator"
  },
  {
    "objectID": "unit-2/lec-2-1-slides.html#what-influences-mathrmvarhattau",
    "href": "unit-2/lec-2-1-slides.html#what-influences-mathrmvarhattau",
    "title": "Unit 2.1: Optimal Experimental Design",
    "section": "What influences \\(\\mathrm{Var}(\\hat{\\tau})\\)?",
    "text": "What influences \\(\\mathrm{Var}(\\hat{\\tau})\\)?\n\\[\\mathrm{Var}(\\hat{\\tau})\\;=\\;\\frac{\\sigma^2}{N}\\;=\\;\\frac{\\mathrm{Var}(\\varepsilon_i)}{\\;N \\,\\times\\, \\mathrm{Var}(D_i)\\,}.\\tag{2}\\]\n\n\nIncreases with \\(\\mathrm{Var}(\\varepsilon_i)\\)\n\n\n\n\nDecreases with \\(N\\)\n\n\n\n\nDecreases with \\(\\mathrm{Var}(D_i)\\)"
  },
  {
    "objectID": "unit-2/lec-2-1-slides.html#minimum-detectable-effect-mde---the-star-of-the-show",
    "href": "unit-2/lec-2-1-slides.html#minimum-detectable-effect-mde---the-star-of-the-show",
    "title": "Unit 2.1: Optimal Experimental Design",
    "section": "Minimum Detectable Effect (MDE) - the star of the show!",
    "text": "Minimum Detectable Effect (MDE) - the star of the show!\nNow let’s assume that potential outcomes are conditional potential outcomes y0 and y1 that are normally distributed.\n\\[\nY_{i0} | X_i \\sim \\mathbb{N}(\\mu_0, \\sigma^2) \\mid D_i=0\n\\]\n\\[\nY_{i1} | X_i \\sim \\mathbb{N}(\\mu_1, \\sigma^2) \\mid D_i=1.\n\\]\n\n\nMDE: Smallest effect \\(\\mu_1 - \\mu_0\\) detectable with specified power\nHypotheses: \\(H_0: \\mu_0 = 0 \\quad \\text{vs} \\quad H_1: \\mu_1 \\neq 0\\)\nEstimator: \\(\\bar{Y}_1 - \\bar{Y}_0\\) for \\(\\mu_1 - \\mu_0\\) (Assuming independent observations)"
  },
  {
    "objectID": "unit-2/lec-2-1-slides.html#mde-formula---the-star-of-the-show",
    "href": "unit-2/lec-2-1-slides.html#mde-formula---the-star-of-the-show",
    "title": "Unit 2.1: Optimal Experimental Design",
    "section": "MDE formula - the star of the show!",
    "text": "MDE formula - the star of the show!\n\\[\n\\text{MDE} = (z_{1-\\alpha/2} + z_{1-\\beta}) \\sqrt{\\frac{\\sigma^2}{n_1} + \\frac{\\sigma^2}{n_0}}\n\\]\n\n\\(n_1\\): Treatment sample size\n\\(n_0\\): Control sample size\n\\(\\sigma^2\\): Common outcome variance"
  },
  {
    "objectID": "unit-2/lec-2-1-slides.html#mde-is-effect-size-tau-s.t.-power-has-the-same-cutoff-as-the-significance-level",
    "href": "unit-2/lec-2-1-slides.html#mde-is-effect-size-tau-s.t.-power-has-the-same-cutoff-as-the-significance-level",
    "title": "Unit 2.1: Optimal Experimental Design",
    "section": "MDE is effect size \\(\\tau\\) s.t. power has the same cutoff as the significance level",
    "text": "MDE is effect size \\(\\tau\\) s.t. power has the same cutoff as the significance level"
  },
  {
    "objectID": "unit-2/lec-2-1-slides.html#how-many-participants-should-i-recruit-in-each-arm-of-our-study",
    "href": "unit-2/lec-2-1-slides.html#how-many-participants-should-i-recruit-in-each-arm-of-our-study",
    "title": "Unit 2.1: Optimal Experimental Design",
    "section": "How many participants should I recruit in each arm of our study?",
    "text": "How many participants should I recruit in each arm of our study?\nTo find the optimal sample allocation, we need to minimize the MDE:\n\\[\n\\min_{n_0, n_1} \\text{MDE}\n\\]\nsubject to \\(n_0 + n_1 = N\\)."
  },
  {
    "objectID": "unit-2/lec-2-1-slides.html#the-equal-variance-case",
    "href": "unit-2/lec-2-1-slides.html#the-equal-variance-case",
    "title": "Unit 2.1: Optimal Experimental Design",
    "section": "The Equal Variance Case",
    "text": "The Equal Variance Case\n\nAssume equal variances in treatment and control (and no cost difference)\n\n\\[\\text{MDE} = (z_{1-\\alpha/2} + z_{1-\\beta})\\sqrt{\\frac{\\sigma^2}{n_0} + \\frac{\\sigma^2}{n_1}}\\]\n\n\\(n_1\\): Treatment sample size\n\\(n_0\\): Control sample size\n\\(\\sigma^2\\): Common variance\nResult: \\(n_0^* = n_1^* = \\frac{N}{2}\\)\nOptimal allocation is a 50-50 split"
  },
  {
    "objectID": "unit-2/lec-2-1-slides.html#unequal-variances",
    "href": "unit-2/lec-2-1-slides.html#unequal-variances",
    "title": "Unit 2.1: Optimal Experimental Design",
    "section": "Unequal Variances",
    "text": "Unequal Variances\nBut how often are variances likely to be equal?\n\nWith unequal variances, the MDE formula changes:\n\n\\[\\text{MDE} = (z_{1-\\alpha/2} + z_{1-\\beta})\\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_0^2}{n_0}}\\]\n\nStrategy: Oversample group with higher variance\nChallenges: Estimating \\(\\sigma_0^2\\) and \\(\\sigma_1^2\\) in advance"
  },
  {
    "objectID": "unit-2/lec-2-1-slides.html#so.-much.-math.-lets-make-a-calculator",
    "href": "unit-2/lec-2-1-slides.html#so.-much.-math.-lets-make-a-calculator",
    "title": "Unit 2.1: Optimal Experimental Design",
    "section": "So. Much. Math. Let’s make a calculator",
    "text": "So. Much. Math. Let’s make a calculator\nOptimal Experimental Design Calculator"
  },
  {
    "objectID": "unit-2/lec-2-1-slides.html#real-world-cost-considerations",
    "href": "unit-2/lec-2-1-slides.html#real-world-cost-considerations",
    "title": "Unit 2.1: Optimal Experimental Design",
    "section": "Real-world cost considerations",
    "text": "Real-world cost considerations\n\nTreatment group: Higher cost (\\(c_1\\))\n\nIntervention funding\nStaff hiring\nSupplies\nParticipant incentives\n\nControl group: Lower but non-zero cost (\\(c_0\\))\n\nSurvey administration\nLab visits"
  },
  {
    "objectID": "unit-2/lec-2-1-slides.html#budget-constrained-optimization",
    "href": "unit-2/lec-2-1-slides.html#budget-constrained-optimization",
    "title": "Unit 2.1: Optimal Experimental Design",
    "section": "Budget-constrained optimization",
    "text": "Budget-constrained optimization\n\\[\n\\min_{n_0,\\,n_1} \\text{MDE}\n\\quad \\text{subject to} \\quad\nc_0\\,n_0 + c_1\\,n_1 \\;\\le\\; M\n\\]\n\n\\(c_0\\): Cost per control participant\n\\(c_1\\): Cost per treatment participant\n\\(M\\): Total budget\n\nOptimal allocation\n\\[\n\\frac{n_1}{n_0} \\;=\\; \\sqrt{\\frac{\\sigma_1^2\\,c_0}{\\sigma_0^2\\,c_1}}\n\\]"
  },
  {
    "objectID": "unit-2/lec-2-1-slides.html#what-is-the-implication",
    "href": "unit-2/lec-2-1-slides.html#what-is-the-implication",
    "title": "Unit 2.1: Optimal Experimental Design",
    "section": "What is the implication?",
    "text": "What is the implication?\n\\[\n\\frac{n_1}{n_0} \\;=\\; \\sqrt{\\frac{\\sigma_1^2\\,c_0}{\\sigma_0^2\\,c_1}}\n\\]\nNo Free Lunch: Cost differences can shift your allocation away from the standard 50–50.\n\nIf \\(c_1 \\gg c_0\\): Fewer treatment participants\nIf \\(c_1 \\ll c_0\\): More treatment participants"
  },
  {
    "objectID": "unit-2/lec-2-1-slides.html#design-extensions",
    "href": "unit-2/lec-2-1-slides.html#design-extensions",
    "title": "Unit 2.1: Optimal Experimental Design",
    "section": "Design Extensions",
    "text": "Design Extensions\nSee lecture notes for discussion of some extensions (The premise is the same)\n\nDichotomous treatment with binomial outcome\nMultiple treatment arms\nClustered designs"
  },
  {
    "objectID": "unit-2/lec-2-1-slides.html#strategies-to-increase-power",
    "href": "unit-2/lec-2-1-slides.html#strategies-to-increase-power",
    "title": "Unit 2.1: Optimal Experimental Design",
    "section": "Strategies to Increase Power",
    "text": "Strategies to Increase Power\nAlso in notes, some strategies for increasing power with fixed n:\n\nMaximize compliance\nChoose less noisy outcome measures\nMultiple waves of baseline and endline data\nInclude covariates in estimation\nImplement stratified randomization"
  },
  {
    "objectID": "unit-2/lec-2-1-slides.html#conclusion",
    "href": "unit-2/lec-2-1-slides.html#conclusion",
    "title": "Unit 2.1: Optimal Experimental Design",
    "section": "Conclusion",
    "text": "Conclusion\n\nOne magical thing about experimentation: allows control over design, you’re not just taking what the world gives you\nConsider all factors: sample sizes, costs, clustering, compliance\nPlan thoroughly to maximize power within constraints\n\nNext time: look at more advanced randomization strategies that can further improve your power"
  },
  {
    "objectID": "unit-2/shiny-oed-app.html",
    "href": "unit-2/shiny-oed-app.html",
    "title": "Optimal Experiment Design App",
    "section": "",
    "text": "This interactive tool helps you explore optimal experimental design for randomized controlled trials (RCTs) with one treatment and one control group. The app allows you to:\n\nCalculate optimal sample sizes for different effect sizes\nCompare different allocation ratios between treatment and control groups\nExplore cost-constrained designs",
    "crumbs": [
      "Resources",
      "Optimal Experiment Design App"
    ]
  },
  {
    "objectID": "unit-2/shiny-oed-app.html#overview",
    "href": "unit-2/shiny-oed-app.html#overview",
    "title": "Optimal Experiment Design App",
    "section": "",
    "text": "This interactive tool helps you explore optimal experimental design for randomized controlled trials (RCTs) with one treatment and one control group. The app allows you to:\n\nCalculate optimal sample sizes for different effect sizes\nCompare different allocation ratios between treatment and control groups\nExplore cost-constrained designs",
    "crumbs": [
      "Resources",
      "Optimal Experiment Design App"
    ]
  },
  {
    "objectID": "unit-2/shiny-oed-app.html#key-statistical-concepts",
    "href": "unit-2/shiny-oed-app.html#key-statistical-concepts",
    "title": "Optimal Experiment Design App",
    "section": "Key Statistical Concepts",
    "text": "Key Statistical Concepts\nEffect Size\nThe effect size represents the magnitude of the difference between treatment and control groups. Understanding effect size is crucial for: 1. Determining practical significance (not just statistical significance) 2. Planning sample sizes 3. Comparing results across different studies\nFor Continuous Outcomes\nMeasured as Cohen’s \\(d\\) (standardized mean difference):\n\\[d = \\frac{\\mu_T - \\mu_C}{\\sigma_{\\text{pooled}}}\\]\nTypical interpretations:\n\n\nSmall effect: ~0.2 (e.g., a small improvement in test scores)\n\nMedium effect: ~0.5 (e.g., noticeable improvement in patient outcomes)\n\nLarge effect: ~0.8 (e.g., dramatic improvement in treatment response)\n\nThe standardization by pooled standard deviation (\\(\\sigma_{\\text{pooled}}\\)) allows comparison across different scales and measures.\nFor Binary Outcomes\nMeasured as the difference in proportions (\\(p_T - p_C\\)):\n\n\n\\(p_T\\) is the success rate in the treatment group\n\n\\(p_C\\) is the success rate in the control group\n\nExample interpretations:\n\nA difference of 0.05 (5 percentage points) might be meaningful for rare events\nA difference of 0.20 (20 percentage points) might be expected for effective interventions\n\nThe variance for binary outcomes follows the binomial distribution:\n\n\\(\\text{Var}(p) = \\frac{p(1-p)}{n}\\)\nLarger variance near \\(p = 0.5\\)\n\nSmaller variance near \\(p = 0\\) or \\(p = 1\\)\n\nStatistical Power and Sample Size\nThe interplay between power, sample size, and effect size forms the foundation of experimental design. These concepts are connected through a fundamental trade-off:\nPower \\((1-\\beta)\\)\n\nThe probability of detecting a true effect when it exists:\n\nDefinition: Power \\(= P(\\text{Reject }H_0 \\mid H_1 \\text{ is true})\\)\nInterpretation: The probability of a study finding a real effect\n\nTypical value: \\(0.80\\) (80%)\n\nLower values (e.g., 70%) increase risk of missing real effects\nHigher values (e.g., 90%) require substantially larger sample sizes\n\n\nType II Error \\((\\beta)\\)\n\n\n\nDefinition: \\(\\beta = P(\\text{Fail to reject }H_0 \\mid H_1 \\text{ is true})\\)\n\n\nInterpretation: The probability of missing a real effect\n\nRelationship: Power \\(= 1 - \\beta\\)\n\n\nExample: With 80% power, β = 20% chance of missing a real effect\nSignificance Level \\((\\alpha)\\)\n\nThe probability of falsely claiming an effect exists:\n\nDefinition: \\(\\alpha = P(\\text{Reject }H_0 \\mid H_0 \\text{ is true})\\)\nInterpretation: Risk of false positive findings\n\nCommon values:\n\n0.05 (5%): Standard for most research\n0.01 (1%): More stringent, used for critical decisions\n0.10 (10%): Sometimes used in pilot studies\n\n\nTrade-offs\n\n\nSample Size vs. Power:\n\nLarger samples → Higher power\nDoubling power often requires more than doubling sample size\n\n\n\nEffect Size vs. Sample Size:\n\nSmaller effects require larger samples\nRelationship is quadratic \\((n \\propto 1/d^2)\\)\n\n\n\n\nType I vs. Type II Errors:\n\nReducing one type of error often increases the other\nMust balance based on consequences of each error type\n\n\nMinimum Detectable Effect (MDE)\nThe MDE is the smallest true effect size that can be detected with the specified power and significance level. For a two-sided test:\n\\[\\text{MDE} = (z_{1-\\alpha/2} + z_{1-\\beta}) \\sqrt{\\frac{\\text{Var}_T}{n_T} + \\frac{\\text{Var}_C}{n_C}}\\]\nwhere:\n\n\n\\(z_{1-\\alpha/2}\\) is the critical value for significance level \\(\\alpha\\)\n\n\n\\(z_{1-\\beta}\\) is the critical value for power \\(1-\\beta\\)\n\n\n\\(\\text{Var}_T\\), \\(\\text{Var}_C\\) are the variances in treatment and control groups\n\n\\(n_T\\), \\(n_C\\) are the sample sizes in treatment and control groups\nOptimal Allocation\nThe optimal allocation ratio between treatment and control groups depends on:\n\nEqual Sample Sizes: When costs are equal and variances are similar, a 1:1 ratio is optimal\nUnequal Variances: The optimal ratio is proportional to the standard deviations: \\[\\frac{n_T}{n_C} \\propto \\frac{\\sigma_T}{\\sigma_C}\\]\nUnequal Costs: When costs differ, the optimal ratio is: \\[\\frac{n_T}{n_C} \\propto \\frac{\\sigma_T}{\\sigma_C} \\sqrt{\\frac{c_C}{c_T}}\\] where \\(c_T\\), \\(c_C\\) are the per-unit costs",
    "crumbs": [
      "Resources",
      "Optimal Experiment Design App"
    ]
  },
  {
    "objectID": "unit-2/shiny-oed-app.html#using-the-app",
    "href": "unit-2/shiny-oed-app.html#using-the-app",
    "title": "Optimal Experiment Design App",
    "section": "Using the App",
    "text": "Using the App\nLocal RStudio Version\nTo run this app locally in RStudio:\n\nshiny::runApp(\"unit-2/oed-app.R\")\n\nBrowser Version\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 1500\n\nlibrary(shiny)\nlibrary(ggplot2)\n\nui &lt;- fluidPage(\n  titlePanel(\"Optimal Experimental Design for One Treatment vs. One Control\"),\n  \n  # Add help text at the top\n  fluidRow(\n    column(12,\n      h4(\"Understanding Power Analysis and Optimal Design\"),\n      p(\"This app helps you determine optimal sample sizes and allocation ratios for experiments.\",\n        \"The calculations consider both statistical power and practical constraints.\"),\n      p(\"Key concepts:\"),\n      tags$ul(\n        tags$li(strong(\"Minimum Detectable Effect (MDE):\"), \n               \"The smallest true effect size that your study can reliably detect with the specified power.\"),\n        tags$li(strong(\"Optimal Allocation:\"), \n               \"The best way to divide participants between treatment and control groups, considering:\",\n               tags$ul(\n                 tags$li(\"Statistical efficiency (minimizing MDE)\"),\n                 tags$li(\"Cost constraints\"),\n                 tags$li(\"Practical implementation\"))\n               ),\n        tags$li(strong(\"Power vs. Sample Size:\"), \n               \"There's always a trade-off - higher power requires larger samples.\")\n      ),\n      p(\"Adjust the parameters below to explore how different choices affect your study design.\"),\n      tags$hr()\n    )\n  ),\n  \n  sidebarLayout(\n    sidebarPanel(\n      # --- Choose outcome type (binary or continuous)\n      radioButtons(\n        inputId = \"outcomeType\", \n        label = \"Outcome Type\",\n        choices = c(\"Binary\" = \"binary\", \"Continuous\" = \"continuous\"),\n        selected = \"continuous\"\n      ),\n      div(class = \"well\",\n        h5(\"Outcome Types:\"),\n        p(strong(\"Binary Outcomes:\"),\n          \"Measured as proportions or percentages (e.g., success rates, mortality rates).\",\n          \"The variance is determined by the proportion itself - highest at p=0.5.\"),\n        p(strong(\"Continuous Outcomes:\"),\n          \"Measured on a continuous scale (e.g., blood pressure, test scores).\",\n          \"Requires estimates of population variance in each group.\")\n      ),\n      \n      # --- Significance level (alpha)\n      sliderInput(\n        inputId = \"alpha\", \n        label = \"Significance level (alpha):\",\n        min = 0.001, \n        max = 0.10, \n        step = 0.001,\n        value = 0.05\n      ),\n      div(class = \"well\",\n        h5(\"Significance Level (α):\"),\n        p(\"The probability of falsely concluding there is an effect when there isn't one (Type I error).\"),\n        tags$ul(\n          tags$li(strong(\"0.05 (5%)\"), \": Standard choice - 1 in 20 chance of false positive\"),\n          tags$li(strong(\"0.01 (1%)\"), \": More conservative - use for critical decisions\"),\n          tags$li(strong(\"0.10 (10%)\"), \": More liberal - might use in pilot studies\")\n        )\n      ),\n      \n      # --- Power (1 - beta)\n      sliderInput(\n        inputId = \"power\", \n        label = \"Statistical Power (1 - beta):\",\n        min = 0.50, \n        max = 0.99, \n        step = 0.01,\n        value = 0.80\n      ),\n      div(class = \"well\",\n        h5(\"Statistical Power (1-β):\"),\n        p(\"The probability of detecting a true effect when it exists.\"),\n        tags$ul(\n          tags$li(strong(\"0.80 (80%)\"), \": Standard choice - accepts 20% chance of missing real effects\"),\n          tags$li(strong(\"0.90 (90%)\"), \": Higher power - use for critical studies, but requires larger samples\"),\n          tags$li(strong(\"0.70 (70%)\"), \": Lower power - might use in pilot studies or with resource constraints\")\n        ),\n        p(\"Remember: Increasing power requires larger sample sizes, often substantially.\")\n      ),\n      \n      # --- Parameters for continuous outcome\n      conditionalPanel(\n        condition = \"input.outcomeType == 'continuous'\",\n        numericInput(\"sigmaT\", \"Treatment Variance (σ²_T):\", 1),\n        numericInput(\"sigmaC\", \"Control Variance (σ²_C):\", 1),\n        helpText(\"The expected variances in each group. If unknown, assume equal variances.\")\n      ),\n      \n      # --- Parameters for binary outcome\n      conditionalPanel(\n        condition = \"input.outcomeType == 'binary'\",\n        sliderInput(\"pT\", \"Treatment proportion (p_T):\",\n                    min = 0, max = 1, step = 0.01, value = 0.3),\n        sliderInput(\"pC\", \"Control proportion (p_C):\",\n                    min = 0, max = 1, step = 0.01, value = 0.3),\n        helpText(\"Set the difference in these values to determine your minimum detectable effect (MDE).\",\n                 \"p_C might be derived from historical data.\")\n      ),\n      \n      # --- Allocation: either fix total sample size or total cost\n      radioButtons(\n        inputId = \"constraintType\",\n        label = \"Constraint Type:\",\n        choices = c(\"Fixed Total Sample Size\" = \"sample\",\n                    \"Fixed Total Cost\"        = \"cost\")\n      ),\n      div(class = \"well\",\n        h5(\"Design Constraints:\"),\n        p(strong(\"Fixed Total Sample Size:\"),\n          \"Use when you have a specific total number of participants available.\",\n          \"The app will help you determine the optimal split between groups.\"),\n        p(strong(\"Fixed Total Cost:\"),\n          \"Use when you have a fixed budget and different costs per group.\",\n          \"Common when treatment is more expensive than control, or when recruitment costs differ.\")\n      ),\n      \n      # --- If total sample size is the constraint\n      conditionalPanel(\n        condition = \"input.constraintType == 'sample'\",\n        numericInput(\"N\", \"Total Sample Size (N):\", 200, min = 2, step = 1)\n      ),\n      \n      # --- If total cost is the constraint\n      conditionalPanel(\n        condition = \"input.constraintType == 'cost'\",\n        numericInput(\"costT\", \"Cost per Treatment Unit:\", 2, min = 1, step = 1),\n        numericInput(\"costC\", \"Cost per Control Unit:\", 1, min = 1, step = 1),\n        numericInput(\"budget\", \"Total Budget:\", 300, min = 1, step = 1)\n      ),\n      \n      # --- Option to let the user pick ratio or find optimum\n      radioButtons(\n        inputId = \"ratioChoice\",\n        label = \"Allocation Choice:\",\n        choices = c(\"Manually Pick Ratio\" = \"manual\",\n                    \"Find Optimal Ratio\"  = \"optimal\")\n      ),\n      \n      sliderInput(\n        inputId = \"ratio\",\n        label = \"Treatment:Control Ratio (r = n_T / n_C):\",\n        min = 0.1,\n        max = 10,\n        step = 0.1,\n        value = 1\n      )\n    ),\n    \n    mainPanel(\n      tabsetPanel(\n        tabPanel(\"MDE vs. Ratio\", \n                 plotOutput(\"plotMDE\"),\n                 br(),\n                 verbatimTextOutput(\"textResult\"),\n                 br(),\n                 h4(\"Step-by-Step Calculations\"),\n                 uiOutput(\"mdeCalcSteps\")\n        ),\n        tabPanel(\"Cost Allocation Plot\",\n                 plotOutput(\"plotCost\", height = \"500px\"),\n                 br(),\n                 h4(\"Step-by-Step Calculations\"),\n                 uiOutput(\"costCalcSteps\"),\n                 br(),\n                 helpText(\"This plot shows the budget constraint line \",\n                          \"and iso-MDE curves for different cost allocations \",\n                          \"(c_C on the x-axis, c_T on the y-axis). \",\n                          \"The red point indicates the optimal allocation \",\n                          \"where MDE is minimized, if available.\")\n        )\n      )\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n  \n  # Helper: z-value for alpha/2 (two-sided) and for power\n  zAlpha &lt;- reactive({\n    qnorm(1 - input$alpha / 2)\n  })\n  zBeta &lt;- reactive({\n    qnorm(input$power)\n  })\n  \n  # Function to compute MDE given n_C, n_T (the sample sizes)\n  computeMDE_from_n &lt;- function(nC, nT) {\n    if (input$outcomeType == \"continuous\") {\n      sigmaT &lt;- input$sigmaT\n      sigmaC &lt;- input$sigmaC\n      varTerm &lt;- sigmaT^2 / nT + sigmaC^2 / nC\n    } else {\n      pT &lt;- input$pT\n      pC &lt;- input$pC\n      varTerm &lt;- pT*(1 - pT)/nT + pC*(1 - pC)/nC\n    }\n    (zAlpha() + zBeta()) * sqrt(varTerm)\n  }\n  \n  # Function to compute MDE given a ratio (for either sample or cost constraint)\n  computeMDE_ratio &lt;- function(r) {\n    # 1) Determine n_T and n_C based on constraints and ratio\n    if (input$constraintType == \"sample\") {\n      # n_T + n_C = N, and n_T / n_C = r\n      n_C &lt;- input$N / (r + 1)\n      n_T &lt;- r * n_C\n    } else {\n      # costT * n_T + costC * n_C = budget, n_T / n_C = r\n      n_C &lt;- input$budget / (r * input$costT + input$costC)\n      n_T &lt;- r * n_C\n    }\n    \n    # 2) Compute MDE\n    computeMDE_from_n(n_C, n_T)\n  }\n  \n  # Plot: MDE vs. ratio\n  output$plotMDE &lt;- renderPlot({\n    # We'll vary r from 0.1 to 10 in increments of 0.1\n    rVals &lt;- seq(0.1, 10, by = 0.1)\n    mdeVals &lt;- sapply(rVals, computeMDE_ratio)\n    \n    plot(rVals, mdeVals, type = \"l\", lwd = 2, \n         xlab = \"Ratio (n_T / n_C)\", ylab = \"MDE\",\n         main = \"Minimum Detectable Effect vs. Allocation Ratio\")\n    \n    # If in manual mode, highlight user's chosen ratio\n    if (input$ratioChoice == \"manual\") {\n      rUser &lt;- input$ratio\n      userMde &lt;- computeMDE_ratio(rUser)\n      points(rUser, userMde, col = \"red\", pch = 19, cex = 1.5)\n    }\n  })\n  \n  # Helper function to format numbers nicely\n  format_num &lt;- function(x) format(round(x, 4), nsmall = 4)\n  \n  # Text output: either user-chosen ratio or optimal ratio\n  output$textResult &lt;- renderPrint({\n    if (input$ratioChoice == \"optimal\") {\n      # Find ratio that yields smallest MDE\n      rTest &lt;- seq(0.1, 10, 0.01)\n      mdeTest &lt;- sapply(rTest, computeMDE_ratio)\n      idxMin &lt;- which.min(mdeTest)\n      rOpt &lt;- rTest[idxMin]\n      mdeOpt &lt;- mdeTest[idxMin]\n      \n      if (input$constraintType == \"sample\") {\n        n_C_opt &lt;- input$N / (rOpt + 1)\n        n_T_opt &lt;- rOpt * n_C_opt\n      } else {\n        n_C_opt &lt;- input$budget / (rOpt * input$costT + input$costC)\n        n_T_opt &lt;- rOpt * n_C_opt\n      }\n      \n      cat(\"Optimal Allocation:\\n\")\n      cat(\"Optimal ratio (n_T / n_C):\", round(rOpt, 3), \"\\n\")\n      cat(\"Control group size (n_C):\", round(n_C_opt), \"\\n\")\n      cat(\"Treatment group size (n_T):\", round(n_T_opt), \"\\n\")\n      cat(\"Minimum Detectable Effect:\", round(mdeOpt, 3), \"\\n\")\n      \n    } else {\n      # User-chosen ratio\n      r &lt;- input$ratio\n      mde &lt;- computeMDE_ratio(r)\n      \n      if (input$constraintType == \"sample\") {\n        n_C &lt;- input$N / (r + 1)\n        n_T &lt;- r * n_C\n      } else {\n        n_C &lt;- input$budget / (r * input$costT + input$costC)\n        n_T &lt;- r * n_C\n      }\n      \n      cat(\"Current Allocation:\\n\")\n      cat(\"Ratio (n_T / n_C):\", round(r, 3), \"\\n\")\n      cat(\"Control group size (n_C):\", round(n_C), \"\\n\")\n      cat(\"Treatment group size (n_T):\", round(n_T), \"\\n\")\n      cat(\"Minimum Detectable Effect:\", round(mde, 3), \"\\n\")\n    }\n  })\n  \n  # Plot: Cost allocation\n  output$plotCost &lt;- renderPlot({\n    # Only show this plot if user is in \"cost\" constraint\n    if (input$constraintType == \"cost\") {\n      # Generate grid of n_C, n_T values across full plot range\n      max_n_C &lt;- input$budget/input$costC * 1.2  # Extend 20% beyond budget line\n      max_n_T &lt;- input$budget/input$costT * 1.2\n      n_C &lt;- seq(1, max_n_C, length.out = 100)\n      n_T &lt;- seq(1, max_n_T, length.out = 100)\n      \n      # Create matrix of MDE values\n      mde_matrix &lt;- matrix(NA, nrow = length(n_C), ncol = length(n_T))\n      for (i in 1:length(n_C)) {\n        for (j in 1:length(n_T)) {\n          mde_matrix[i,j] &lt;- computeMDE_from_n(n_C[i], n_T[j])\n        }\n      }\n      \n      # Find optimal allocation\n      rTest &lt;- seq(0.1, 10, 0.01)\n      mdeTest &lt;- sapply(rTest, computeMDE_ratio)\n      rOpt &lt;- rTest[which.min(mdeTest)]\n      n_C_opt &lt;- input$budget / (rOpt * input$costT + input$costC)\n      n_T_opt &lt;- rOpt * n_C_opt\n      mde_opt &lt;- computeMDE_from_n(n_C_opt, n_T_opt)\n      \n      # Create the plot\n      par(mar = c(5, 5, 4, 2))  # Increase margins for labels\n      \n      # Calculate reasonable MDE range for contours\n      min_mde &lt;- max(min(mde_matrix, na.rm = TRUE), mde_opt * 0.8)  # Don't go too far below optimal\n      max_mde &lt;- min(max(mde_matrix, na.rm = TRUE), mde_opt * 2.0)  # Don't show extremely large MDEs\n      \n      # Create evenly spaced levels, including the optimal MDE\n      contour_levels &lt;- sort(unique(c(\n        seq(min_mde, max_mde, length.out = 8),  # 8 regular levels\n        mde_opt  # Include the optimal MDE level\n      )))\n      \n      # Plot contours\n      contour(n_C, n_T, mde_matrix, \n              xlab = \"Control Group Size (n_C)\",\n              ylab = \"Treatment Group Size (n_T)\",\n              main = \"Cost-constrained Allocation\",\n              levels = contour_levels,\n              labcex = 0.8,  # Slightly smaller contour labels\n              drawlabels = TRUE)\n      \n      # Add budget constraint line\n      budget_line_n_C &lt;- seq(0, max_n_C, length.out = 100)\n      budget_line_n_T &lt;- (input$budget - input$costC * budget_line_n_C) / input$costT\n      lines(budget_line_n_C, budget_line_n_T, col = \"red\", lwd = 2)\n      \n      # Add optimal MDE curve (the contour that passes through optimal point)\n      n_C_curve &lt;- seq(1, max_n_C, length.out = 200)\n      n_T_curve &lt;- numeric(length(n_C_curve))\n      for(i in 1:length(n_C_curve)) {\n        # Find n_T that gives the optimal MDE for this n_C\n        n_T_test &lt;- seq(1, max_n_T, length.out = 200)\n        mde_test &lt;- sapply(n_T_test, function(nt) computeMDE_from_n(n_C_curve[i], nt))\n        n_T_curve[i] &lt;- n_T_test[which.min(abs(mde_test - mde_opt))]\n      }\n      lines(n_C_curve, n_T_curve, col = \"blue\", lwd = 2, lty = 2)\n      \n      # Add optimal point\n      points(n_C_opt, n_T_opt, col = \"red\", pch = 19, cex = 1.5)\n      \n      # Add legend for optimal MDE curve\n      legend(\"topright\", \n             legend = sprintf(\"Optimal MDE = %.3f\", mde_opt),\n             col = \"blue\", lwd = 2, lty = 2)\n    }\n  })\n  \n  # Render step-by-step MDE calculations\n  output$mdeCalcSteps &lt;- renderUI({\n    # Get current values\n    if (input$ratioChoice == \"optimal\") {\n      rTest &lt;- seq(0.1, 10, 0.01)\n      mdeTest &lt;- sapply(rTest, computeMDE_ratio)\n      r &lt;- rTest[which.min(mdeTest)]\n    } else {\n      r &lt;- input$ratio\n    }\n    \n    # Calculate sample sizes\n    if (input$constraintType == \"sample\") {\n      n_C &lt;- input$N / (r + 1)\n      n_T &lt;- r * n_C\n    } else {\n      n_C &lt;- input$budget / (r * input$costT + input$costC)\n      n_T &lt;- r * n_C\n    }\n    \n    # Calculate variances\n    if (input$outcomeType == \"continuous\") {\n      varT &lt;- input$sigmaT^2\n      varC &lt;- input$sigmaC^2\n      var_term &lt;- varT/n_T + varC/n_C\n    } else {\n      varT &lt;- input$pT * (1 - input$pT)\n      varC &lt;- input$pC * (1 - input$pC)\n      var_term &lt;- varT/n_T + varC/n_C\n    }\n    \n    # Critical values\n    z_alpha &lt;- qnorm(1 - input$alpha/2)\n    z_beta &lt;- qnorm(input$power)\n    \n    # Final MDE\n    mde &lt;- (z_alpha + z_beta) * sqrt(var_term)\n    \n    # Create step-by-step explanation with LaTeX\n    tagList(\n      withMathJax(),\n      \n      h5(\"1. Sample Size Calculation:\"),\n      if (input$constraintType == \"sample\") {\n        tagList(\n          p(sprintf(\"With total N = %d and ratio r = %.3f:\", input$N, r)),\n          withMathJax(sprintf(\"$$n_C = \\\\frac{N}{r + 1} = \\\\frac{%d}{%.3f + 1} = %.1f$$\", input$N, r, n_C)),\n          withMathJax(sprintf(\"$$n_T = r \\\\cdot n_C = %.3f \\\\cdot %.1f = %.1f$$\", r, n_C, n_T))\n        )\n      } else {\n        tagList(\n          p(sprintf(\"With budget = %d, c_T = %d, c_C = %d, and ratio r = %.3f:\", \n                    input$budget, input$costT, input$costC, r)),\n          withMathJax(sprintf(\"$$n_C = \\\\frac{\\\\text{budget}}{rc_T + c_C} = \\\\frac{%d}{%.3f \\\\cdot %d + %d} = %.1f$$\", \n                               input$budget, r, input$costT, input$costC, n_C)),\n          withMathJax(sprintf(\"$$n_T = r \\\\cdot n_C = %.3f \\\\cdot %.1f = %.1f$$\", r, n_C, n_T))\n        )\n      },\n      \n      h5(\"2. Variance Terms:\"),\n      if (input$outcomeType == \"continuous\") {\n        withMathJax(sprintf(\"$$\\\\sigma^2_T = %.3f, \\\\quad \\\\sigma^2_C = %.3f$$\", varT, varC))\n      } else {\n        tagList(\n          withMathJax(sprintf(\"$$\\\\text{Var}_T = p_T(1-p_T) = %.3f(1-%.3f) = %.3f$$\", input$pT, input$pT, varT)),\n          withMathJax(sprintf(\"$$\\\\text{Var}_C = p_C(1-p_C) = %.3f(1-%.3f) = %.3f$$\", input$pC, input$pC, varC))\n        )\n      },\n      withMathJax(sprintf(\"$$\\\\text{Combined variance} = \\\\frac{\\\\text{Var}_T}{n_T} + \\\\frac{\\\\text{Var}_C}{n_C} = \\\\frac{%.3f}{%.1f} + \\\\frac{%.3f}{%.1f} = %.4f$$\",\n                          varT, n_T, varC, n_C, var_term)),\n      \n      h5(\"3. Critical Values:\"),\n      withMathJax(sprintf(\"$$z_{1-\\\\alpha/2} = %.4f \\\\quad (\\\\text{for } \\\\alpha = %.3f)$$\", z_alpha, input$alpha)),\n      withMathJax(sprintf(\"$$z_{1-\\\\beta} = %.4f \\\\quad (\\\\text{for power } = %.3f)$$\", z_beta, input$power)),\n      \n      h5(\"4. Final MDE Calculation:\"),\n      withMathJax(\"$$\\\\text{MDE} = (z_{1-\\\\alpha/2} + z_{1-\\\\beta}) \\\\sqrt{\\\\frac{\\\\text{Var}_T}{n_T} + \\\\frac{\\\\text{Var}_C}{n_C}}$$\"),\n      withMathJax(sprintf(\"$$\\\\text{MDE} = (%.4f + %.4f) \\\\sqrt{%.4f} = %.4f$$\", \n                          z_alpha, z_beta, var_term, mde))\n    )\n  })\n  \n  # Render step-by-step cost allocation calculations\n  output$costCalcSteps &lt;- renderUI({\n    if (input$constraintType != \"cost\") {\n      return(p(\"Step-by-step calculations are shown when using cost constraints.\"))\n    }\n    \n    # Get current optimal values if in optimal mode\n    if (input$ratioChoice == \"optimal\") {\n      rTest &lt;- seq(0.1, 10, 0.01)\n      mdeTest &lt;- sapply(rTest, computeMDE_ratio)\n      r &lt;- rTest[which.min(mdeTest)]\n      n_C &lt;- input$budget / (r * input$costT + input$costC)\n      n_T &lt;- r * n_C\n      total_cost &lt;- input$costT * n_T + input$costC * n_C\n      \n      tagList(\n        withMathJax(),\n        h4(\"Optimal Cost Allocation:\"),\n        \n        h5(\"1. Budget Constraint:\"),\n        withMathJax(sprintf(\"$$%d = %d n_T + %d n_C$$\", \n                           input$budget, input$costT, input$costC)),\n        \n        h5(\"2. Optimal Allocation Ratio:\"),\n        withMathJax(sprintf(\"$$r = \\\\frac{n_T}{n_C} = %.3f$$\", r)),\n        \n        h5(\"3. Sample Sizes:\"),\n        withMathJax(sprintf(\"$$n_C = \\\\frac{\\\\text{budget}}{rc_T + c_C} = \\\\frac{%d}{%.3f \\\\cdot %d + %d} = %.1f$$\",\n                           input$budget, r, input$costT, input$costC, n_C)),\n        withMathJax(sprintf(\"$$n_T = r \\\\cdot n_C = %.3f \\\\cdot %.1f = %.1f$$\", \n                           r, n_C, n_T)),\n        \n        h5(\"4. Cost Verification:\"),\n        withMathJax(sprintf(\"$$\\\\text{Total cost} = c_T n_T + c_C n_C = %d \\\\cdot %.1f + %d \\\\cdot %.1f = %d$$\",\n                           input$costT, n_T, input$costC, n_C, total_cost))\n      )\n    } else {\n      n_C &lt;- input$budget / (input$ratio * input$costT + input$costC)\n      n_T &lt;- input$ratio * n_C\n      total_cost &lt;- input$costT * n_T + input$costC * n_C\n      \n      tagList(\n        withMathJax(),\n        h4(\"Manual Cost Allocation:\"),\n        \n        h5(\"1. Budget Constraint:\"),\n        withMathJax(sprintf(\"$$%d = %d n_T + %d n_C$$\", \n                           input$budget, input$costT, input$costC)),\n        \n        h5(\"2. Fixed Allocation Ratio:\"),\n        withMathJax(sprintf(\"$$r = \\\\frac{n_T}{n_C} = %.3f$$\", input$ratio)),\n        \n        h5(\"3. Sample Sizes:\"),\n        withMathJax(sprintf(\"$$n_C = \\\\frac{\\\\text{budget}}{rc_T + c_C} = \\\\frac{%d}{%.3f \\\\cdot %d + %d} = %.1f$$\",\n                           input$budget, input$ratio, input$costT, input$costC, n_C)),\n        withMathJax(sprintf(\"$$n_T = r \\\\cdot n_C = %.3f \\\\cdot %.1f = %.1f$$\", \n                           input$ratio, n_C, n_T)),\n        \n        h5(\"4. Cost Verification:\"),\n        withMathJax(sprintf(\"$$\\\\text{Total cost} = c_T n_T + c_C n_C = %d \\\\cdot %.1f + %d \\\\cdot %.1f = %d$$\",\n                           input$costT, n_T, input$costC, n_C, total_cost))\n      )\n    }\n  })\n}\n\nshinyApp(ui = ui, server = server)",
    "crumbs": [
      "Resources",
      "Optimal Experiment Design App"
    ]
  },
  {
    "objectID": "unit-2/lec-2-1.html",
    "href": "unit-2/lec-2-1.html",
    "title": "Unit 2: Design of Experiments",
    "section": "",
    "text": "Welcome, dear students, to another joyous adventure in the realm of experimental design! In our previous sessions, we tackled the fundamentals of statistical conclusion validity and dipped our toes into power calculations and simulation. We even brushed up against the perils of clustering, which appears A LOT in health services research since this is how health care is organized – think hospitals, clinics, communities, social networks, etc.\nNow, we turn the tables. Instead of passively accepting whatever nature (or large administrative database) throws at us, we’re in control. Unlike our secondary data analysis friends, we get to design our experiments to make the most of the resources we have. Specifically, we’re interested in maximizing our abilitly to learn (i.e. statistical power) subject to our constraints (e.g. budget, logistical).\nOf course, nothing is for free; this is both a blessing and a curse. Tradeoffs abound as usual. We need a framework for thinking about how to optimally weigh costs and benefits. If only there was an entire discipline devoted to this…..OH, WAIT! (Yes, my friends, you are all economists now. You’re welcome.) As an experimentalist, you can optimize your design choices in ways our secondary-data-using colleagues can only dream of (or envy, or curse, depending on their temperament). So let’s dig in, shall we?\n\n\n\n\n\n\nNote\n\n\n\nNote: Suboptimal design choices won’t necessarily ruin your study’s internal validity, but they will keep you from making the best inference possible. And that might cost you that sweet, sweet grant renewal next year."
  },
  {
    "objectID": "unit-2/lec-2-1.html#introduction",
    "href": "unit-2/lec-2-1.html#introduction",
    "title": "Unit 2: Design of Experiments",
    "section": "",
    "text": "Welcome, dear students, to another joyous adventure in the realm of experimental design! In our previous sessions, we tackled the fundamentals of statistical conclusion validity and dipped our toes into power calculations and simulation. We even brushed up against the perils of clustering, which appears A LOT in health services research since this is how health care is organized – think hospitals, clinics, communities, social networks, etc.\nNow, we turn the tables. Instead of passively accepting whatever nature (or large administrative database) throws at us, we’re in control. Unlike our secondary data analysis friends, we get to design our experiments to make the most of the resources we have. Specifically, we’re interested in maximizing our abilitly to learn (i.e. statistical power) subject to our constraints (e.g. budget, logistical).\nOf course, nothing is for free; this is both a blessing and a curse. Tradeoffs abound as usual. We need a framework for thinking about how to optimally weigh costs and benefits. If only there was an entire discipline devoted to this…..OH, WAIT! (Yes, my friends, you are all economists now. You’re welcome.) As an experimentalist, you can optimize your design choices in ways our secondary-data-using colleagues can only dream of (or envy, or curse, depending on their temperament). So let’s dig in, shall we?\n\n\n\n\n\n\nNote\n\n\n\nNote: Suboptimal design choices won’t necessarily ruin your study’s internal validity, but they will keep you from making the best inference possible. And that might cost you that sweet, sweet grant renewal next year."
  },
  {
    "objectID": "unit-2/lec-2-1.html#optimal-experimental-design-insights-from-econ-101",
    "href": "unit-2/lec-2-1.html#optimal-experimental-design-insights-from-econ-101",
    "title": "Unit 2: Design of Experiments",
    "section": "1. Optimal Experimental Design: Insights from Econ 101",
    "text": "1. Optimal Experimental Design: Insights from Econ 101\n\n1.1 Our Objective\nThinking back to Econ 101,1 recall that we can pose an optimization problem as maximizing (or minimizing) an objective function subject to constraints. In our case, we’ll use this to set up our experimental design problem, i.e,\n\nObjective function: Statistical power,\nSubject to: Budget constraints.\n\nIn other words, we want to choose our design to maximize power subject to our budget (or other) constraints. It turns out that there are loads of things in our control; usually the only things that aren’t are feasibility and the budget we have to work with.\nConcept Map\n\n\nCode\nflowchart LR\n   %% Nodes\n    A((\"To calculate optimal sample sizes\")):::navy\n    B[\"Desired significance level\"]:::carolinaBlue\n    C[\"Desired statistical power\"]:::carolinaBlue\n    D[\"Minimum detectable effect size\"]:::carolinaBlue\n    E[\"Experimental budget and treatment costs\"]:::carolinaBlue\n    F[\"How the data will be analyzed\"]:::carolinaBlue\n    G[\"Available pre-treatment covariates\"]:::carolinaBlue\n    \n    H[\"Unit of assignment\"]:::teal\n    I[\"Number of distinct outcomes of interest\"]:::teal\n    \n    J([\"Design with clustered assignment\"]):::tarHeelBlue\n    K([\"Design for multiple hypothesis testing\"]):::tarHeelBlue\n\n    %% Edges\n    A --&gt; B\n    A --&gt; C\n    A --&gt; D\n    A --&gt; E\n    A --&gt; F\n    A --&gt; G\n    \n    F --&gt; H\n    F --&gt; I\n    \n    H --&gt; J\n    I --&gt; K\n\n    %% UNC Brand Colors\n    classDef navy fill:#13294B,stroke:#13294B,stroke-width:1px,color:#FFFFFF\n    classDef carolinaBlue fill:#4B9CD3,stroke:#13294B,stroke-width:1px,color:#FFFFFF\n    classDef teal fill:#00788C,stroke:#13294B,stroke-width:1px,color:#FFFFFF\n    classDef tarHeelBlue fill:#7BAFD4,stroke:#13294B,stroke-width:1px,color:#FFFFFF\n\n\n\n\n\nflowchart LR\n   %% Nodes\n    A((\"To calculate optimal sample sizes\")):::navy\n    B[\"Desired significance level\"]:::carolinaBlue\n    C[\"Desired statistical power\"]:::carolinaBlue\n    D[\"Minimum detectable effect size\"]:::carolinaBlue\n    E[\"Experimental budget and treatment costs\"]:::carolinaBlue\n    F[\"How the data will be analyzed\"]:::carolinaBlue\n    G[\"Available pre-treatment covariates\"]:::carolinaBlue\n    \n    H[\"Unit of assignment\"]:::teal\n    I[\"Number of distinct outcomes of interest\"]:::teal\n    \n    J([\"Design with clustered assignment\"]):::tarHeelBlue\n    K([\"Design for multiple hypothesis testing\"]):::tarHeelBlue\n\n    %% Edges\n    A --&gt; B\n    A --&gt; C\n    A --&gt; D\n    A --&gt; E\n    A --&gt; F\n    A --&gt; G\n    \n    F --&gt; H\n    F --&gt; I\n    \n    H --&gt; J\n    I --&gt; K\n\n    %% UNC Brand Colors\n    classDef navy fill:#13294B,stroke:#13294B,stroke-width:1px,color:#FFFFFF\n    classDef carolinaBlue fill:#4B9CD3,stroke:#13294B,stroke-width:1px,color:#FFFFFF\n    classDef teal fill:#00788C,stroke:#13294B,stroke-width:1px,color:#FFFFFF\n    classDef tarHeelBlue fill:#7BAFD4,stroke:#13294B,stroke-width:1px,color:#FFFFFF\n\n\n\n\n\n\n\n\n1.2 A Simple Setup\nTo make this concrete, imagine you’ve received a research grant (yay!) or have a wealthy aunt who’s willing to bankroll your next foray into experimental design. You have two arms in your study:\n\nA control group (no intervention).\n\nA treatment group (some fancy new health intervention).\n\nAnd you have a single, continuous outcome measure, say:\n\\[\nY_i = \\text{Health and Happiness Index}\n\\]\nNow, your big question: How many participants do you need? How do you split that precious sample between treatment and control?\n\n\n1.3 The Big Three Elements\nWhen computing your required sample size (or deciding the “optimal” split), there are three main ingredients:\n\nSignificance level (\\(\\alpha\\)): The probability of a false positive (rejecting the null when it’s actually true).\n\nMinimum Detectable Effect (MDE): The smallest true effect size you want to be able to detect with high probability.\n\nPower (\\(1 - \\beta\\)): The probability of detecting a true effect (i.e., rejecting the null when it’s false).\n\n\n\n\n\n\n\nWarning\n\n\n\nThe MDE is not the effect you expect to see. It’s the smallest effect you care to rule in or rule out. People often mix these up, leading to underpowered studies, heartbreak, and wasted coffee budgets."
  },
  {
    "objectID": "unit-2/lec-2-1.html#the-variance-of-the-average-treatment-effect-ate-and-the-minimum-detectable-effect-mde",
    "href": "unit-2/lec-2-1.html#the-variance-of-the-average-treatment-effect-ate-and-the-minimum-detectable-effect-mde",
    "title": "Unit 2: Design of Experiments",
    "section": "2. The Variance of the Average Treatment Effect (ATE) and the Minimum Detectable Effect (MDE)",
    "text": "2. The Variance of the Average Treatment Effect (ATE) and the Minimum Detectable Effect (MDE)\nAt this point, you may be wondering: “Why do we keep obsessing about the variance of our estimated effect?” Because variance is basically the Grim Reaper of statistical power—bigger variance, bigger standard errors, lower power. So let’s peek under the hood of our ATE estimator.\n\n2.1 Setting Up the Outcome Model\nConsider the outcome \\(Y_i\\) for subject \\(i\\). Under treatment \\(D=1\\) and control \\(D=0\\), \\(Y_i\\) is influenced by:\n\nObservable variables \\(\\mathbf{X}_i\\).\n\nAn unobserved effect \\(\\alpha_i\\) (think “innate personal quirks”).\n\nA person-specific treatment effect \\(\\tau_i\\), whose expectation is zero (\\(\\mathbb{E}[\\tau_i] = 0\\)). (We typically assume the average of these individual effects is our main parameter, \\(\\bar{\\tau}\\).)\n\nAn error term \\(\\varepsilon_i\\), assumed to be i.i.d. (pure luck-of-the-draw stuff).\n\nSo a possible model could be:\n\\[\nY_{i} \\;=\\; \\alpha_i \\;+\\; \\mathbf{X}_i \\,\\beta \\;+\\; \\bar{\\tau} \\,D_i \\;+\\; \\tau_i\\,D_i \\;+\\; \\varepsilon_i.\n\\tag{1}\n\\]\nHere, \\(\\bar{\\tau}\\) is the “average” treatment effect across individuals (the main star of our show), and \\(\\tau_i\\) captures the idiosyncratic difference around that average.\nBecause randomization ensures \\(D_i\\) is (in expectation) independent of \\(\\alpha_i, \\tau_i,\\) and \\(\\varepsilon_i\\), your estimator for the ATE:\n\\[\n\\hat{\\tau}\n\\;=\\;\n\\mathbb{E}[Y_i \\mid D_i=1]\n\\;-\\;\n\\mathbb{E}[Y_i \\mid D_i=0],\n\\]\nis unbiased. Translation: we’re not systematically off the mark. If the true effect is 2, we’re not going to estimate 1.6 or 2.7 just because the allocation was rigged.\nBut to actually detect \\(\\hat{\\tau}\\) in a statistical test, we need the noise to be sufficiently small relative to the signal. Enter the variance of the estimated ATE:\n\\[\n\\mathrm{Var}(\\hat{\\tau})\n\\;=\\;\n\\frac{\\sigma^2}{N}\n\\;=\\;\n\\frac{\\mathrm{Var}(\\varepsilon_i)}{\\;N \\,\\times\\, \\mathrm{Var}(D_i)\\,}.\n\\tag{2}\n\\]\nHere,\n\n\\(\\mathrm{Var}(\\varepsilon_i)\\): is the variance of the unobserved “noise” in outcomes.\n\\(N\\): is the total number of units (subjects).\n\n\\(\\mathrm{Var}(D_i)\\): is the variance of your treatment assignment indicator. If the treatment is binary and \\(p\\) is the fraction in treatment, then \\(\\mathrm{Var}(D_i) = p(1-p)\\).\n\nHence, \\(\\mathrm{Var}(\\hat{\\tau})\\) is:\n\nIncreasing in \\(\\mathrm{Var}(\\varepsilon_i)\\): More “noise” = bigger standard errors.\n\nDecreasing in \\(N\\): More data = smaller standard errors.\n\nDecreasing in \\(\\mathrm{Var}(D_i)\\)\n\nIf you only have one treatment arm with proportion (p) under treatment, then ((D_i) = p(1-p)). So yes, that half-and-half split is not just for black-and-white cookies—it’s also a straightforward way to keep variance in check.\nThat’s the core story. Once we drag real-world complexities in—like different costs per treatment participant, cluster randomization, or multiple waves of data—things get more involved. (Stay tuned!) But for now, remember:\n\nThe formula \\(\\mathrm{Var}(\\hat{\\tau}) = \\sigma^2 / [N \\,\\mathrm{Var}(D)]\\) is your guiding light.\n\nKeep that variance down and your power up."
  },
  {
    "objectID": "unit-2/lec-2-1.html#the-minimum-detectible-effect-mde",
    "href": "unit-2/lec-2-1.html#the-minimum-detectible-effect-mde",
    "title": "Unit 2: Design of Experiments",
    "section": "2.2 The Minimum Detectible Effect (MDE)",
    "text": "2.2 The Minimum Detectible Effect (MDE)\nNow for our simple case, let’s assume that a single treatment results in (conditional) outcomes \\(Y_0\\) and \\(Y_1\\) that are normally distributed for simplicity, i.e.:\n\\[\nY_{i0} | X_i \\sim \\mathcal{N}(\\mu_0, \\sigma^2) if D_i=0;\nY_{i1} | X_i \\sim \\mathcal{N}(\\mu_1, \\sigma^2) if D_i=1.\n\\]\nHere, \\(\\mu_0\\) and \\(\\mu_1\\) are the treatment effects, and \\(\\sigma^2\\) is the variance of the outcome.\nWe can now define the minimum detectable effect (MDE) as the smallest effect \\(\\mu_1 - \\mu_0\\) that we can detect with a specified level of statistical power. Remember, to calculate statistical power, we need to specify a null hypothesis and an alternative hypothesis:\n\\[\nH_0: \\mu_0 = 0\nH_1: \\mu_1 \\neq 0\n\\]\nIf observations are independent, then the difference in sample means (\\(\\bar{Y}_0 - \\bar{Y}_1\\)) is our estimator of \\(\\mu_1 - \\mu_0\\). we can define the MDE as:\n\\[\n\\text{MDE} = z_{1-\\alpha/2} \\sqrt{\\frac{\\sigma^2}{n_1} + \\frac{\\sigma^2}{n_0}} \\;+\\; z_{1-\\beta} \\sqrt{\\frac{\\sigma^2}{n_1} + \\frac{\\sigma^2}{n_0}}.\n\\]\n(Exact form depends on one- or two-sided tests, but the gist is the same.)\n\n\n\n\n\n\nNote\n\n\n\nTo derive the above, start by defining the probability (\\(\\alpha\\)) of a Type I error and the probability (\\(\\beta\\)) of a Type II error as a function of \\(\\bar{Y}_0 - \\bar{Y}_1\\):\n\\[\nz_{1-\\alpha/2} = \\frac{\\bar{Y}_0 - \\bar{Y}_1}{\\sqrt{\\frac{\\sigma_0^2}{n_0} + \\frac{\\sigma_1 ^2}{n_1}}} \\implies\n\\bar{Y}_0 - \\bar{Y}_1 \\;=\\; z_{1-\\alpha/2} \\sqrt{\\frac{\\sigma_0^2}{n_0} + \\frac{\\sigma_1 ^2}{n_1}}\n\\]\nwhere \\(n_0\\) and \\(n_1\\) are the sample sizes in each group and \\(\\sigma_0^2\\) and \\(\\sigma_1^2\\) are the (conditional) variances of the outcomes in each group.\nThe probability, \\(\\beta\\), of a Type II error is:\n\\[\nz_{\\beta} = \\frac{\\bar{Y}_0 - \\bar{Y}_1 - MDE}{\\sqrt{\\frac{\\sigma_0^2}{n_0} + \\frac{\\sigma_1 ^2}{n_1}}} \\implies\n\\bar{Y}_0 - \\bar{Y}_1 \\;=\\; MDE - z_{\\beta} \\sqrt{\\frac{\\sigma_0^2}{n_0} + \\frac{\\sigma_1 ^2}{n_1}}\n\\]\nand the MDE is:\n\\[\n\\text{MDE} = z_{1-\\alpha/2} \\sqrt{\\frac{\\sigma^2}{n_1} + \\frac{\\sigma^2}{n_0}} \\;+\\; z_{\\beta} \\sqrt{\\frac{\\sigma^2}{n_1} + \\frac{\\sigma^2}{n_0}}.\n\\]\n\n\nGraphically, the MDE is effect size \\(\\tau\\) such that power has the same cutoff as the significance level:\n\n\n\nType 1 and Type 2 Errors\n\n\nNow to reiterate a very important point that people mess up all the time: The MDE is not the expected effect size. It is the minimum effect size that you want to be able to detect. Think about how much underpowered research is out there because people make this simple mistake."
  },
  {
    "objectID": "unit-2/lec-2-1.html#figuring-out-optimal-sample-sizes-a.k.a.-minimizing-the-mde",
    "href": "unit-2/lec-2-1.html#figuring-out-optimal-sample-sizes-a.k.a.-minimizing-the-mde",
    "title": "Unit 2: Design of Experiments",
    "section": "3. Figuring Out Optimal Sample Sizes (a.k.a. Minimizing the MDE)",
    "text": "3. Figuring Out Optimal Sample Sizes (a.k.a. Minimizing the MDE)\nLet’s turn now to the practical question on everyone’s mind: How many participants should I recruit in each arm of my study? Or put more formally, How do I minimize the Minimum Detectable Effect (MDE) given constraints on my time, budget, and sanity?\nAssume that the variances are equal in the treatment and control group. We can rewrite the MDE formula as:\n\\[\n\\text{MDE}\n\\;=\\;\n\\bigl(z_{1-\\alpha/2} + z_{1-\\beta}\\bigr)\n\\sqrt{\n  \\frac{\\sigma^2}{n_0} + \\frac{\\sigma^2}{n_1}\n},\n\\]\nwhere - \\(n_1\\) is the sample size in treatment,\n- \\(n_0\\) is the sample size in control, and\n- \\(\\sigma^2\\) is the (common) variance of the outcome.\nBecause the stuff before the square root is just a constant, minimizing the \\(\\text{MDE}\\) with respect to \\((n_0, n_1)\\) subject to a total sample size \\(N = n_0 + n_1\\), boils down to minimizing\n\\[\n\\sqrt{\\frac{1}{n_0} + \\frac{1}{n_1}}.\n\\]\nThen because\n\\[\n\\frac{1}{n_0} + \\frac{1}{n_1}\n\\;=\\;\n\\frac{n_0 + n_1}{n_0 \\,n_1}\n\\;=\\;\n\\frac{N}{n_0\\,n_1},\n\\]\nwe want tomaximize \\(n_0\\,n_1\\) given \\(n_0 + n_1 = N\\).2\nHence, for equal variances and no cost differences, we end up with the classic result:\n\\[\nn_0^*\n\\;=\\;\nn_1^*\n\\;=\\;\n\\frac{N}{2}.\n\\]\nSo a 50–50 split is optimal under those assumptions. Easy enough!\n\nBut what if the variance in the treatment group, \\(\\sigma_1^2\\), differs from that in the control group, \\(\\sigma_0^2\\)? Then the formula for the MDE looks more like:\n\\[\n\\text{MDE}\n\\;=\\;\n\\bigl(z_{1-\\alpha/2} + z_{1-\\beta}\\bigr)\n\\sqrt{\n  \\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_0^2}{n_0}\n}.\n\\]\nMinimizing that suggests you should oversample the group with the higher variance. (Intuition: you get more “statistical bang” for each extra participant in the group that has the largest contribution to the total standard error.)\nOf course, we rarely know \\(\\sigma_0^2\\) and \\(\\sigma_1^2\\) exactly in advance, so we often do a best guess from pilot studies, previous literature, or sheer optimism (spoiler: that last one sometimes backfires)."
  },
  {
    "objectID": "unit-2/lec-2-1.html#enter-the-budget-constraint-when-aunties-not-that-rich",
    "href": "unit-2/lec-2-1.html#enter-the-budget-constraint-when-aunties-not-that-rich",
    "title": "Unit 2: Design of Experiments",
    "section": "4. Enter the Budget Constraint (When Auntie’s Not That Rich)",
    "text": "4. Enter the Budget Constraint (When Auntie’s Not That Rich)\nIn real life, each participant in the treatment group might cost more because you have to fund the intervention (hire staff, buy supplies, bribe them with candy, etc.). Meanwhile, enrolling a control participant might be cheaper—but still not free, because you need them to fill out surveys or come in for labs. Let’s denote:\n\n\\(c_0\\): the cost per control participant,\n\n\\(c_1\\): the cost per treatment participant, and\n\n\\(M\\): your total budget.\n\nNow your problem is:\n\\[\n\\min_{n_0,\\,n_1}\n\\text{MDE}\n\\quad\n\\text{subject to}\n\\quad\nc_0\\,n_0 + c_1\\,n_1 \\;\\le\\; M.\n\\]\nSolving for \\(n_0\\) and \\(n_1\\) yields:\n\\[\n\\frac{n_1}{n_0}\n\\;=\\;\n\\sqrt{\n  \\frac{\\sigma_1^2\\,c_0}{\\sigma_0^2\\,c_1}\n},\n\\]\nwhich tells us:\n\nIf \\(c_1\\) is much higher than \\(c_0\\) (treatment is expensive!), you’ll want fewer participants in treatment.\n\nIf \\(\\sigma_0^2 &gt; \\sigma_1^2\\) (control variance is larger), you might want more control participants.\n\nEssentially, you compare the ratio of your marginal benefit (reducing variance) to your marginal cost (how expensive it is to add participants to each arm). You buy the “cheapest variance reduction” first—like a true economist.\n\nPractical Implications\n\nNo Free Lunch: Even if you have philanthropic relatives, resources are finite. Incorporating cost differences can shift your allocation away from the standard 50–50.\n\nPilot or Prior Data: Because the formula involves \\(\\sigma_0^2\\) and \\(\\sigma_1^2\\), it helps to get decent estimates of group variances so you don’t design suboptimally.\n\nSensitivity Checks: If you’re not sure about the cost ratio or the variance ratio, do a few “what if” analyses or simulations to see how your MDE changes when you tweak these assumptions.\n\n\n\n\n\n\n\nNote\n\n\n\nPro Tip: In many health interventions, the cost difference can be huge—especially if the treatment requires lots of staff time, medical supplies, or specialized devices. Don’t ignore that in your design.\n\n\n\n\nToo much math?\nI know this is a lot of math, so to help you visualize what’s going on, I’ve created a Shiny app that lets you play around with different values and see how the “optimal” \\(n_0\\) and \\(n_1\\) changes. Play around with the parameters and see what happens.\nYou can access the app here: Shiny App\nUnderstanding these trade‐offs is the heart of optimal experimental design. If your control group is nearly free, sample the heck out of it; if your intervention is super expensive, you might want fewer treatment participants but enough to be adequately powered."
  },
  {
    "objectID": "unit-2/lec-2-1.html#design-extensions",
    "href": "unit-2/lec-2-1.html#design-extensions",
    "title": "Unit 2: Design of Experiments",
    "section": "5. Design Extensions",
    "text": "5. Design Extensions\n\n5.1 Dichotomous Treatment with a Binomial Outcome\nWhen the outcome is binary (e.g., success/failure) and the null is that treatment = control, variance depends on the underlying mean. If \\(\\bar{p}_1\\) and \\(\\bar{p}_0\\) differ, you get different variances. Optimal allocation tries to oversample the arm whose proportion is nearer 0.5, because that’s where variance is highest.\n\n\n5.2 Multiple Treatment Arms or “Dose-Response” Designs\nNow until now we’ve just considered one treatment group and one control group. There are some cases where instead of just estimating an effect of a treatment, we might be interested in estimating a dose response curve. So this would be like asking what is the effect of going from a dose of 100 milligrams to 200 milligrams, etc.\nNow if we only care about a linear effect, or more precisely, we are assuming that there’s a linear effect in the dose, we really only need two arms of the experiment, e.g. a zero dose (control) and 200 mg. This puts us back into the treatment and control scenario we saw before.\nBut often, and actually usually, it’s not a good idea to assume a linear effect. For example, the effect of going from zero to 100 milligrams might be a lot larger than the effect of taking a patient from 100 milligrams to 200 milligrams.\nNow it turns out here as you expand treatment levels, the sample allocation is not equal even if you’re assuming the same variances and have equal costs. Not assuming any funny business with differences in outcome variance or unequal costs, optimal allocations are as follows for different polynomials:\n\n\n\n\n\n\n\n\nHighest & Only Polynomial Order\nNumber of Treatment Cells\nSample Allocation\n\n\n\n\n1\n2\n{1/2, 0, 1/2}\n\n\n2\n3\n{1/4, 1/2, 1/4}\n\n\n3\n4\n{1/6, 1/3, 1/3, 1/6}\n\n\n…\n…\n…\n\n\n10\n11\n{1/20, 1/10, …, 1/10, 1/20}\n\n\n\n\n\n5.3 Clustered Designs\nIn many health economics studies, entire clinics, schools, or communities get randomized. Let \\(\\rho\\) be the intracluster correlation coefficient (ICC). A higher \\(\\rho\\) means participants within a cluster look more alike, so you effectively have fewer independent observations. That means you need a bigger total sample (and more clusters) to achieve the same power. The formula for the number of clusters needed often looks like:\n\n\n\n\n\n\nCluster Randomized Designs\n\n\n\n\\[\nn \\;\\approx\\; \\left(\\frac{z_{1-\\alpha/2} + z_{1-\\beta}}{MDE}\\right)^2\n\\bigl(1 + (\\bar{m}-1)\\rho\\bigr)\\sigma^2,\n\\] where \\(MDE\\) is your effect size and \\(\\bar{m}\\) is the average cluster size."
  },
  {
    "objectID": "unit-2/lec-2-1.html#tips-and-tricks-to-increase-power",
    "href": "unit-2/lec-2-1.html#tips-and-tricks-to-increase-power",
    "title": "Unit 2: Design of Experiments",
    "section": "6. Tips and Tricks to Increase Power",
    "text": "6. Tips and Tricks to Increase Power\nLucky for us, there are several strategies to squeeze more power out of your design. We’ll focus on a few big ones.\n\n6.1 Designing Your Study to Maximize Compliance\nEnsure high take-up rates of the treatment. Low participation dilutes the observable effect, making detection challenging. If only half the sample adopts the treatment, you would need four times as many units to detect the same effect as with full participation.3\nConsider strategies like smaller but more motivated populations, or supportive interventions that encourage use. Implementation scientists are your friends here (just feed them coffee and data).\n\n\n6.2 Choosing (Less Noisy) Outcome Measures\nIf your chosen outcome is extremely noisy (maybe self-reported “happiness” on a 0–100 scale?), you’ll need an enormous sample to detect moderate effects.\nAccurate measurements reduce variability in your data. Employ techniques such as embedding consistency checks in surveys, using anchoring and triangulation methods, and considering multiple questions to assess the same concept, thereby averaging out noise.\nAnother trick is to measure outcomes repeatedly and average them. For example, do two lab tests for the same biomarker and average them. Or measure mental health weekly for 4 weeks. Each additional measurerement can reduce the measurement error component.\n\n\n6.3 Multiple Waves of Baseline and Endline Data\nCollecting more than one wave of baseline and endline data can yield substantial power gains if your outcome is either noisy or only weakly autocorrelated. Here are the key ideas from the paper by McKenzie in the reading:\n\nWhy does it help?\n\nWhen outcomes are less autocorrelated (think sporadic monthly income or ephemeral daily health measures), a single baseline and a single endline might not capture the dynamic changes well. Multiple waves help average out the noise.\n\nYou can pick up more precise estimates of changes over time and reduce the standard error.\n\nPractical considerations\n\nWith a fixed budget, you often face a trade-off: Do I survey the same individuals multiple times, or do I get more individuals in fewer waves?\n\nIf your outcome is highly autocorrelated (e.g., certain test scores, stable biometrics), you might not gain as much from multiple waves.\n\nIf your outcome is noisy and less autocorrelated—like short-term business profits or daily stress measurements—multiple waves can dramatically reduce variance.\n\nImplementation\n\nIn practice, plan for multiple data collection rounds, at least for the key outcome variables. If your budget is tight, think carefully about the ratio of cross-sectional coverage (number of individuals) to time-series coverage (number of repeated measurements).\n\nAnalytical methods\n\nUsing repeated measures properly often entails repeated-measures ANOVA or difference-in-differences approaches (when you have a baseline plus multiple follow-ups).\n\nIf the treatment might change the autocorrelation of the outcome, consider that in your power calculations (there are advanced methods for this, but the main takeaway: be mindful of potential changes in variance structure).\n\n\nIn short: More waves = more data points = smaller standard errors (usually). That’s a formula for improved power if the correlation structure isn’t working against you.\n\n\n6.4 Including Covariates in the Estimation Model\nIf you can incorporate pre-treatment covariates that predict outcomes, you can reduce residual variance. Even though randomization ensures that on average groups are balanced, controlling for strong predictors of the outcome yields a more precise estimate.\n\nPre-treatment outcomes are gold if your outcome is stable over time.\n\nStratification variables used in random assignment can (and often should) be controlled in analysis.\n\nBaseline characteristics that strongly predict your outcome.\n\nBe warned: never include post-treatment variables that might be affected by the intervention. That’s a sure-fire path to bias.\n\n\n6.5 Implement Stratified Randomization\nBy dividing the sample into strata based on characteristics related to the outcome and randomizing within these strata, you can control for confounding variables and reduce variance, leading to more precise estimates.\n\n\n\n\n\n\nNote\n\n\n\nThis will be a major topic in the next unit."
  },
  {
    "objectID": "unit-2/lec-2-1.html#concluding-thoughts",
    "href": "unit-2/lec-2-1.html#concluding-thoughts",
    "title": "Unit 2: Design of Experiments",
    "section": "7. Concluding Thoughts",
    "text": "7. Concluding Thoughts\nExperimentation is exciting because you control the design. Yet with great power comes great responsibility: you have to juggle sample sizes, cost constraints, intracluster correlations, multiple time points, and the dreaded compliance issue. The big takeaway is to think through these choices up front rather than scramble in the final hour.\n\nOptimal sample size depends on the usual suspects: significance level, power, MDE, variance, and cost.\nAllocation across treatment arms is rarely a simple 50/50 if costs or variances differ.\nClustering matters—a lot—especially in health policy research.\nCovariates, compliance, outcome selection, multiple waves are your friends when battling the tyranny of limited resources.\n\nThat’s it for now. Next time, we’ll look at more advanced randomization strategies (e.g., stratified, matched-pair, and adaptive designs) that can further improve your power."
  },
  {
    "objectID": "unit-2/lec-2-1.html#footnotes",
    "href": "unit-2/lec-2-1.html#footnotes",
    "title": "Unit 2: Design of Experiments",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIf you were daydreaming in Econ 101, fear not. We’ll keep things simple.↩︎\nif you want to maximize the product of two nonnegative numbers with a fixed sum, you set them equal.↩︎\nsee: World Bank Blog↩︎"
  },
  {
    "objectID": "unit-2/lec-2-2-slides.html#randomization-the-foundation-of-causal-inference",
    "href": "unit-2/lec-2-2-slides.html#randomization-the-foundation-of-causal-inference",
    "title": "Unit 2.2: Advanced Randomization Techniques",
    "section": "Randomization: The Foundation of Causal Inference",
    "text": "Randomization: The Foundation of Causal Inference\n\n\nLast session: Maximizing power through optimal experimental design\nToday: The art and science of randomization\nRandomization enables causal claims by balancing all factors:\n\nObservable characteristics\nUnobservable characteristics\nPotential outcomes\n\n\n\n\nEmphasize that randomization is what allows us to make causal claims - it’s the key distinction between experimental and observational studies."
  },
  {
    "objectID": "unit-2/lec-2-2-slides.html#why-does-randomization-work",
    "href": "unit-2/lec-2-2-slides.html#why-does-randomization-work",
    "title": "Unit 2.2: Advanced Randomization Techniques",
    "section": "Why Does Randomization Work?",
    "text": "Why Does Randomization Work?\nBecause it ensures:\n\n\n\nNon-zero probability condition: Everyone has a chance of treatment\n\nIndividualism: Independence across units\n\nUnconfoundedness: Balance on observed/unobserved covariates\n\n\n\\[E[Y_i(0)|D_i=1] = E[Y_i(0)|D_i=0]\\]\n\nThe untreated potential outcomes are the same in both groups!"
  },
  {
    "objectID": "unit-2/lec-2-2-slides.html#what-you-need-for-randomization",
    "href": "unit-2/lec-2-2-slides.html#what-you-need-for-randomization",
    "title": "Unit 2.2: Advanced Randomization Techniques",
    "section": "What You Need for Randomization",
    "text": "What You Need for Randomization\n\n\n\nSample of units: Who or what will be randomized\n\nAllocation ratio: How many units to each condition\n\nRandomization device: Physical or computational\n\nBaseline covariates: (for some approaches)"
  },
  {
    "objectID": "unit-2/lec-2-2-slides.html#random-sampling-vs.-random-assignment",
    "href": "unit-2/lec-2-2-slides.html#random-sampling-vs.-random-assignment",
    "title": "Unit 2.2: Advanced Randomization Techniques",
    "section": "Random Sampling vs. Random Assignment",
    "text": "Random Sampling vs. Random Assignment\n\nRandom Sampling vs. Random Assignment"
  },
  {
    "objectID": "unit-2/lec-2-2-slides.html#classical-assignment-mechanisms-framework",
    "href": "unit-2/lec-2-2-slides.html#classical-assignment-mechanisms-framework",
    "title": "Unit 2.2: Advanced Randomization Techniques",
    "section": "Classical Assignment Mechanisms Framework",
    "text": "Classical Assignment Mechanisms Framework\n\n\n\n\n\nflowchart TD\n    %% Top nodes - conditions\n    A[\"Non-zero probability condition\"]:::gold\n    B[\"Individualism condition\"]:::gold\n    C[\"Unconfoundedness condition\"]:::gold\n    \n    %% Middle node - mechanisms\n    D{{\"Classical Random Assignment Mechanisms\"}}:::tarHeelRed\n    \n    %% Assignment types\n    E[\"Bernoulli Trial\"]:::carolinaBlue\n    F[\"Complete Randomized\\nExperiment (CRE)\"]:::carolinaBlue\n    G[\"Stratified Randomization\"]:::carolinaBlue\n    H[\"Rerandomization\"]:::carolinaBlue\n    I[\"Matched Pairs\"]:::carolinaBlue\n    \n    %% Bottom node - inference\n    J{{\"Design-conscious Inference\"}}:::uncGreen\n    \n    %% Connections\n    A --&gt; D\n    B --&gt; D\n    C --&gt; D\n    \n    D --&gt; E\n    D --&gt; F\n    D --&gt; G\n    D --&gt; H\n    D --&gt; I\n    \n    E --&gt; J\n    F --&gt; J\n    G --&gt; J\n    H --&gt; J\n    I --&gt; J\n    \n    %% UNC Brand Colors\n    classDef gold fill:#FFD100,stroke:#13294B,stroke-width:1px,color:#13294B\n    classDef tarHeelRed fill:#DC143C,stroke:#13294B,stroke-width:1px,color:#FFFFFF\n    classDef carolinaBlue fill:#4B9CD3,stroke:#13294B,stroke-width:1px,color:#FFFFFF\n    classDef uncGreen fill:#8DB434,stroke:#13294B,stroke-width:1px,color:#FFFFFF"
  },
  {
    "objectID": "unit-2/lec-2-2-slides.html#classical-assignment-mechanisms",
    "href": "unit-2/lec-2-2-slides.html#classical-assignment-mechanisms",
    "title": "Unit 2.2: Advanced Randomization Techniques",
    "section": "Classical Assignment Mechanisms",
    "text": "Classical Assignment Mechanisms\n\nBernoulli Trials\nComplete Randomization\nRe-randomization\nStratified Randomization\nMatched-Pair Designs"
  },
  {
    "objectID": "unit-2/lec-2-2-slides.html#bernoulli-trials",
    "href": "unit-2/lec-2-2-slides.html#bernoulli-trials",
    "title": "Unit 2.2: Advanced Randomization Techniques",
    "section": "Bernoulli Trials",
    "text": "Bernoulli Trials\n\nSimplest approach: independent coin flips\n\n\\(P(Z_i = 1) = p\\) for all units \\(i\\)\n\n\n\n\n\nAdvantages:\n\nSimple to implement\nCan randomize as participants arrive\nNo baseline data needed\n\n\n\n\nDisadvantages:\n\nRandom group sizes\nPotential imbalance on key covariates\nImplementation vulnerability"
  },
  {
    "objectID": "unit-2/lec-2-2-slides.html#case-study-the-canadian-national-breast-screening-study",
    "href": "unit-2/lec-2-2-slides.html#case-study-the-canadian-national-breast-screening-study",
    "title": "Unit 2.2: Advanced Randomization Techniques",
    "section": "Case Study: The Canadian National Breast Screening Study",
    "text": "Case Study: The Canadian National Breast Screening Study\n\n\nMajor randomized trial evaluating mammography screening effectiveness\nUsed alternating assignment (first to treatment, second to control)\nDesign flaw: clinical breast exams conducted before randomization\nNurses and physicians could (and did) influence group assignments"
  },
  {
    "objectID": "unit-2/lec-2-2-slides.html#cnbss-randomization-failures",
    "href": "unit-2/lec-2-2-slides.html#cnbss-randomization-failures",
    "title": "Unit 2.2: Advanced Randomization Techniques",
    "section": "CNBSS: Randomization Failures",
    "text": "CNBSS: Randomization Failures\n\n\n\nPre-randomization examination: Detected suspicious lumps before group assignment\n\nSelection bias: Women with palpable lumps disproportionately assigned to mammography group\n\nInadequate concealment: Study staff could influence group assignments\n\nEvidence of manipulation: Names overwritten, identities reversed, lines skipped\n\n\n\nResult: Mammography group had 68% higher incidence of advanced cancers at baseline!"
  },
  {
    "objectID": "unit-2/lec-2-2-slides.html#cnbss-impact-on-study-validity",
    "href": "unit-2/lec-2-2-slides.html#cnbss-impact-on-study-validity",
    "title": "Unit 2.2: Advanced Randomization Techniques",
    "section": "CNBSS: Impact on Study Validity",
    "text": "CNBSS: Impact on Study Validity\n\n\nStudy reported no mortality benefit from mammography screening\n\nHowever: The randomization bias likely masked true benefits\nHigher-risk patients concentrated in treatment group\nControl group contamination: ~25% received mammograms outside the study\n\n\n\nBroader lesson: Compromise in randomization can fundamentally undermine study validity and lead to decades of scientific controversy"
  },
  {
    "objectID": "unit-2/lec-2-2-slides.html#complete-randomization",
    "href": "unit-2/lec-2-2-slides.html#complete-randomization",
    "title": "Unit 2.2: Advanced Randomization Techniques",
    "section": "Complete Randomization",
    "text": "Complete Randomization\n\nset.seed(072311)  # Set seed for reproducibility\n\n# Parameters\nN &lt;- 100          # Total number of units\np &lt;- 0.5          # Proportion to assign to treatment\n\n# Generate random numbers and sort\nunits &lt;- data.frame(\n  id = 1:N,\n  random_num = runif(N)\n)\nunits &lt;- units[order(units$random_num),]\n\n# Assign first p% to treatment\nunits$treatment &lt;- 0\nunits$treatment[1:(N*p)] &lt;- 1\n\n# Check resulting allocation\ntable(units$treatment)\n\n\n 0  1 \n50 50 \n\n\nEach participant has fixed probability of assignment, with total group sizes fixed in advance."
  },
  {
    "objectID": "unit-2/lec-2-2-slides.html#chance-imbalance-with-complete-randomization",
    "href": "unit-2/lec-2-2-slides.html#chance-imbalance-with-complete-randomization",
    "title": "Unit 2.2: Advanced Randomization Techniques",
    "section": "Chance Imbalance with Complete Randomization",
    "text": "Chance Imbalance with Complete Randomization\n\n\nEven with perfect implementation, covariates may be imbalanced\n\nExample: In a study of 722 people (NLSY data):\n\n~45% of randomizations had all covariates balanced\n~30% had one imbalanced covariate\nRemaining had multiple imbalanced covariates\n\n\n\n\n\nThis raises two critical questions: 1. How can we ensure better balance in design? 2. What do we do if imbalance occurs?"
  },
  {
    "objectID": "unit-2/lec-2-2-slides.html#re-randomization",
    "href": "unit-2/lec-2-2-slides.html#re-randomization",
    "title": "Unit 2.2: Advanced Randomization Techniques",
    "section": "Re-randomization",
    "text": "Re-randomization\n\n\nGenerate multiple randomizations\nKeep only those with good balance\nApproach 1: All p-values &gt; threshold (e.g., 0.05)\nApproach 2: Choose iteration with best overall balance"
  },
  {
    "objectID": "unit-2/lec-2-2-slides.html#re-randomization-code",
    "href": "unit-2/lec-2-2-slides.html#re-randomization-code",
    "title": "Unit 2.2: Advanced Randomization Techniques",
    "section": "Re-randomization Code",
    "text": "Re-randomization Code\n\nbalance_check &lt;- function(data, treatment_var, balance_vars) {\n  # Initialize minimum p-value\n  min_pval &lt;- 1\n  \n  # Check each covariate\n  for (var in balance_vars) {\n    # T-test for continuous variables\n    t_result &lt;- t.test(data[[var]] ~ data[[treatment_var]])\n    min_pval &lt;- min(min_pval, t_result$p.value)\n  }\n  \n  return(min_pval)\n}\n\n# Re-randomization function\nrerandomize &lt;- function(data, p_threshold = 0.1, max_attempts = 1000) {\n  n &lt;- nrow(data)\n  n_treat &lt;- floor(n * 0.5)  # 50% to treatment\n  attempts &lt;- 0\n  \n  repeat {\n    # Generate a new randomization\n    treatment &lt;- rep(0, n)\n    treatment[sample(1:n, n_treat)] &lt;- 1\n    data$treatment &lt;- treatment\n    \n    # Check balance\n    pval &lt;- balance_check(data, \"treatment\", c(\"age\", \"income\", \"education\"))\n    \n    attempts &lt;- attempts + 1\n    \n    # Accept if p-value threshold met or max attempts reached\n    if (pval &gt; p_threshold || attempts &gt;= max_attempts) {\n      break\n    }\n  }\n  \n  return(list(treatment = data$treatment, attempts = attempts, min_pval = pval))\n}\n\nThis function repeats randomization until finding one where all p-values exceed our threshold."
  },
  {
    "objectID": "unit-2/lec-2-2-slides.html#drawbacks-of-re-randomization",
    "href": "unit-2/lec-2-2-slides.html#drawbacks-of-re-randomization",
    "title": "Unit 2.2: Advanced Randomization Techniques",
    "section": "Drawbacks of Re-randomization",
    "text": "Drawbacks of Re-randomization\n\n\nOpaque constraints: “Black box” process\nUnusual handling of outliers\nComputationally expensive\nCould run forever if criteria too strict\nStatistical inference must account for the procedure\nCannot balance on unobserved covariates"
  },
  {
    "objectID": "unit-2/lec-2-2-slides.html#stratified-block-randomization",
    "href": "unit-2/lec-2-2-slides.html#stratified-block-randomization",
    "title": "Unit 2.2: Advanced Randomization Techniques",
    "section": "Stratified (Block) Randomization",
    "text": "Stratified (Block) Randomization\n\n\n\n\nStratified Randomization\n\n\n\n\nDivide sample into strata based on covariates\nRandomize separately within each stratum\nPerfect balance on stratification variables"
  },
  {
    "objectID": "unit-2/lec-2-2-slides.html#implementing-stratified-randomization",
    "href": "unit-2/lec-2-2-slides.html#implementing-stratified-randomization",
    "title": "Unit 2.2: Advanced Randomization Techniques",
    "section": "Implementing Stratified Randomization",
    "text": "Implementing Stratified Randomization\n\nset.seed(072311)\n\n# Create example data with categorical covariates\ndata &lt;- data.frame(\n  id = 1:100,\n  gender = sample(c(\"Male\", \"Female\"), 100, replace = TRUE),\n  age_group = sample(c(\"Under 30\", \"30-50\", \"Over 50\"), 100, replace = TRUE),\n  stringsAsFactors = FALSE\n)\n\n# Create strata based on combinations of covariates\ndata$stratum &lt;- paste(data$gender, data$age_group, sep = \"_\")\n\n# Function for stratified randomization\nstratified_randomize &lt;- function(data, strata_var, p = 0.5) {\n  # Initialize assignment vector\n  assignment &lt;- rep(NA, nrow(data))\n  \n  # Get unique strata\n  strata &lt;- unique(data[[strata_var]])\n  \n  # Randomize within each stratum\n  for (s in strata) {\n    # Get indices for this stratum\n    indices &lt;- which(data[[strata_var]] == s)\n    n_stratum &lt;- length(indices)\n    \n    # Calculate number to assign to treatment\n    n_treat &lt;- round(n_stratum * p)\n    \n    # Ensure at least one in each group if possible\n    if (n_stratum &gt; 1) {\n      n_treat &lt;- min(max(n_treat, 1), n_stratum - 1)\n    } else {\n      n_treat &lt;- sample(0:1, 1)  # Random for singletons\n    }\n    \n    # Perform randomization within stratum\n    treat_indices &lt;- sample(indices, n_treat)\n    assignment[indices] &lt;- 0\n    assignment[treat_indices] &lt;- 1\n  }\n  \n  return(assignment)\n}\n\n# Apply stratified randomization\ndata$treatment &lt;- stratified_randomize(data, \"stratum\")\n\n# Check balance by strata\ntable(data$stratum, data$treatment)\n\n                 \n                   0  1\n  Female_30-50    11 10\n  Female_Over 50   8  8\n  Female_Under 30  7  7\n  Male_30-50       9  9\n  Male_Over 50     7  8\n  Male_Under 30    8  8\n\n\nThis code creates strata from combinations of gender and age group, then randomizes within each stratum."
  },
  {
    "objectID": "unit-2/lec-2-2-slides.html#selecting-stratification-variables",
    "href": "unit-2/lec-2-2-slides.html#selecting-stratification-variables",
    "title": "Unit 2.2: Advanced Randomization Techniques",
    "section": "Selecting Stratification Variables",
    "text": "Selecting Stratification Variables\n\n\n\nDiscrete variables are easier to implement\nPrioritize variables that strongly predict outcomes\n\nInclude variables where heterogeneous effects are expected\nBe careful of too many strata - causes “small cell” problems\n\n\n\nHandling “misfits” (when strata size not divisible by treatments):\n\nRemove units randomly\nCreate separate strata for misfits\nUse different randomization approach for misfits"
  },
  {
    "objectID": "unit-2/lec-2-2-slides.html#matched-pairs-randomization",
    "href": "unit-2/lec-2-2-slides.html#matched-pairs-randomization",
    "title": "Unit 2.2: Advanced Randomization Techniques",
    "section": "Matched Pairs Randomization",
    "text": "Matched Pairs Randomization\n\n\nCreate pairs of similar units\nRandomize one to treatment within each pair\nLike stratification taken to the extreme\nPerfect for continuous covariates"
  },
  {
    "objectID": "unit-2/lec-2-2-slides.html#implementing-matched-pairs",
    "href": "unit-2/lec-2-2-slides.html#implementing-matched-pairs",
    "title": "Unit 2.2: Advanced Randomization Techniques",
    "section": "Implementing Matched Pairs",
    "text": "Implementing Matched Pairs\n\nlibrary(optmatch)  # For optimal matching\n\n# Generate example data\nset.seed(072311)\ndata &lt;- data.frame(\n  id = 1:100,\n  age = rnorm(100, 45, 10),\n  income = rnorm(100, 50000, 15000),\n  health_score = rnorm(100, 70, 15)\n)\n\n# Create distance matrix based on covariates\nX &lt;- as.matrix(data[, c(\"age\", \"income\", \"health_score\")])\nX_scaled &lt;- scale(X)  # Standardize to have equal importance\ndist_matrix &lt;- dist(X_scaled)\n\n# Create optimal matches\nmatches &lt;- pairmatch(dist_matrix, data = data)\ndata$pair_id &lt;- matches\n\n# Randomize within pairs\npair_randomize &lt;- function(data, pair_var) {\n  # Initialize assignment vector\n  assignment &lt;- rep(NA, nrow(data))\n  \n  # Get unique pairs\n  pairs &lt;- unique(data[[pair_var]])\n  pairs &lt;- pairs[!is.na(pairs)]  # Remove NA pairs\n  \n  # Randomize within each pair\n  for (p in pairs) {\n    # Get indices for this pair\n    indices &lt;- which(data[[pair_var]] == p)\n    \n    # Skip if not exactly 2 units\n    if (length(indices) != 2) next\n    \n    # Randomly assign one to treatment\n    treat_index &lt;- sample(indices, 1)\n    assignment[indices] &lt;- 0\n    assignment[treat_index] &lt;- 1\n  }\n  \n  return(assignment)\n}\n\n# Apply pair randomization\ndata$treatment &lt;- pair_randomize(data, \"pair_id\")\n\nThis code matches participants based on age, income, and health score, then randomizes one member of each pair to treatment."
  },
  {
    "objectID": "unit-2/lec-2-2-slides.html#balance-tests-approaches-for-verification",
    "href": "unit-2/lec-2-2-slides.html#balance-tests-approaches-for-verification",
    "title": "Unit 2.2: Advanced Randomization Techniques",
    "section": "Balance Tests: Approaches for Verification",
    "text": "Balance Tests: Approaches for Verification\n\n\n\nIndividual covariate tests: t-tests, Chi-square tests\n\nJoint omnibus tests: F-tests testing multiple covariates simultaneously\n\nRegression-based tests: Regress treatment on covariates\n\nStandardized differences: Sample size-independent assessment"
  },
  {
    "objectID": "unit-2/lec-2-2-slides.html#standardized-differences-for-large-samples",
    "href": "unit-2/lec-2-2-slides.html#standardized-differences-for-large-samples",
    "title": "Unit 2.2: Advanced Randomization Techniques",
    "section": "Standardized Differences for Large Samples",
    "text": "Standardized Differences for Large Samples\n\n\nProblem with p-values: With large samples, tiny imbalances become “significant”\n\nSolution: Standardized mean difference (SMD)\n\n\\[SMD = \\frac{\\bar{X}_{treatment} - \\bar{X}_{control}}{\\sqrt{\\frac{s^2_{treatment} + s^2_{control}}{2}}}\\]\n\n\nScale-free measure of imbalance\nIndependent of sample size\nRule of thumb: |SMD| &lt; 0.1 is negligible imbalance"
  },
  {
    "objectID": "unit-2/lec-2-2-slides.html#the-table-1-debate",
    "href": "unit-2/lec-2-2-slides.html#the-table-1-debate",
    "title": "Unit 2.2: Advanced Randomization Techniques",
    "section": "The “Table 1” Debate",
    "text": "The “Table 1” Debate\n\n\nFor Including\n\nTransparency\nDemonstrates randomization effectiveness\nProvides context for readers\nShows extent of imbalance\n\n\nAgainst Including\n\nRandomization guarantees balance in expectation\nOveremphasis on statistically significant differences\nCan lead to inappropriate adjustments\nJournal space limitations\n\n\n\nCompromise: Report balance without p-values, use standardized differences"
  },
  {
    "objectID": "unit-2/lec-2-2-slides.html#design-based-inference",
    "href": "unit-2/lec-2-2-slides.html#design-based-inference",
    "title": "Unit 2.2: Advanced Randomization Techniques",
    "section": "Design-Based Inference",
    "text": "Design-Based Inference\n\n\nRandomization creates the foundation for inference\n\nFisher’s approach: Test sharp null that treatment has no effect on any unit\n\nRandomization distribution: Generate distribution of test statistics under all possible randomizations\n\nPermutation tests: Compare observed statistic to randomization distribution"
  },
  {
    "objectID": "unit-2/lec-2-2-slides.html#complex-experimental-designs",
    "href": "unit-2/lec-2-2-slides.html#complex-experimental-designs",
    "title": "Unit 2.2: Advanced Randomization Techniques",
    "section": "Complex Experimental Designs",
    "text": "Complex Experimental Designs\n\nUnits of Randomization & Spillovers\nCluster Randomization\nFactorial Designs\nFractional Factorial Designs\n\nWithin-Subject vs. Between-Subject\nRandomized Phase-in\nAdaptive Designs\nVariable Treatment Probabilities"
  },
  {
    "objectID": "unit-2/lec-2-2-slides.html#choosing-the-unit-of-randomization",
    "href": "unit-2/lec-2-2-slides.html#choosing-the-unit-of-randomization",
    "title": "Unit 2.2: Advanced Randomization Techniques",
    "section": "Choosing the Unit of Randomization",
    "text": "Choosing the Unit of Randomization\n\n\nKey Considerations\n\nShould match observational unit when possible\nMust align with treatment delivery\nNeed to minimize spillovers\nConsider statistical power\n\n\nCommon Units\n\n\nIndividual: Patients, students\n\nCluster: Villages, clinics, schools\n\nTime periods: Days, weeks, shifts\n\nNetworks: Households, peer groups\n\n\n\nCritical trade-off: Statistical power vs. internal validity"
  },
  {
    "objectID": "unit-2/lec-2-2-slides.html#the-spillover-problem",
    "href": "unit-2/lec-2-2-slides.html#the-spillover-problem",
    "title": "Unit 2.2: Advanced Randomization Techniques",
    "section": "The Spillover Problem",
    "text": "The Spillover Problem\n\n\nSpillovers occur when treatment affects untreated units\nTypes of spillovers:\n\nDirect interaction between units\nGeneral equilibrium effects\nInformation diffusion\nResource competition\n\n\n\n\n\nRandomization at a higher level (clustering) can minimize unwanted spillovers."
  },
  {
    "objectID": "unit-2/lec-2-2-slides.html#measuring-spillovers-two-stage-randomization",
    "href": "unit-2/lec-2-2-slides.html#measuring-spillovers-two-stage-randomization",
    "title": "Unit 2.2: Advanced Randomization Techniques",
    "section": "Measuring Spillovers: Two-Stage Randomization",
    "text": "Measuring Spillovers: Two-Stage Randomization\n\nFirst stage: Randomize clusters to high or low treatment intensity\nSecond stage: Randomize individuals within clusters to treatment or control\nAllows measurement of:\n\nDirect treatment effects\nWithin-cluster spillovers\nBetween-cluster spillovers\n\n\nProvides estimates of:\n\nTotal treatment effect (direct + spillover)\nIsolated direct effect\nSpillover effect on untreated individuals"
  },
  {
    "objectID": "unit-2/lec-2-2-slides.html#cluster-randomization",
    "href": "unit-2/lec-2-2-slides.html#cluster-randomization",
    "title": "Unit 2.2: Advanced Randomization Techniques",
    "section": "Cluster Randomization",
    "text": "Cluster Randomization\n\n\n\nDefinition: Randomize groups (clusters) rather than individuals\n\nCommon clusters: Schools, clinics, villages, neighborhoods\n\nKey parameter: Intraclass correlation coefficient (ICC)\n\nDesign effect: \\(DE = 1 + (m-1) \\times ICC\\)\n\n\n\\(m\\) = average cluster size\nIncreases required sample size\n\n\n\n\n\nAnalysis must account for clustering! * Cluster-robust standard errors * Mixed-effects models * GEE approaches"
  },
  {
    "objectID": "unit-2/lec-2-2-slides.html#factorial-designs-testing-multiple-treatments",
    "href": "unit-2/lec-2-2-slides.html#factorial-designs-testing-multiple-treatments",
    "title": "Unit 2.2: Advanced Randomization Techniques",
    "section": "Factorial Designs: Testing Multiple Treatments",
    "text": "Factorial Designs: Testing Multiple Treatments\n\n\n\n\nIntervention B\nNo Intervention A\nIntervention A\n\n\n\nNo\nControl\nA only\n\n\nYes\nB only\nA and B\n\n\n\n\n\n\nTest multiple treatments simultaneously\nEstimate main effects AND interactions\nEfficient use of resources\nExample: Two interventions with four groups\n\nNo intervention (control)\nIntervention A only\nIntervention B only\nBoth A and B"
  },
  {
    "objectID": "unit-2/lec-2-2-slides.html#randomized-phase-in-designs",
    "href": "unit-2/lec-2-2-slides.html#randomized-phase-in-designs",
    "title": "Unit 2.2: Advanced Randomization Techniques",
    "section": "Randomized Phase-in Designs",
    "text": "Randomized Phase-in Designs\n\n\nAll units eventually receive treatment\nRandomize the timing of treatment\nAdvantages:\n\nBetter compliance (everyone gets treatment eventually)\nClear for partners with resource constraints\nPolitically appealing compromise\n\n\n\n\n\nBut watch out for:\n\nAnticipation effects\nLimited long-term impact measurement"
  },
  {
    "objectID": "unit-2/lec-2-2-slides.html#adaptive-randomization",
    "href": "unit-2/lec-2-2-slides.html#adaptive-randomization",
    "title": "Unit 2.2: Advanced Randomization Techniques",
    "section": "Adaptive Randomization",
    "text": "Adaptive Randomization\n\n\nUpdates assignment probabilities based on accumulated data\nBalances:\n\n\nExploration: Learning which treatment works best\n\nExploitation: Assigning more units to better treatments\n\n\nKey ethical advantage: Fewer participants receive inferior treatments\n\n\n\nNote: We’ll cover adaptive designs in more detail in a future lecture"
  },
  {
    "objectID": "unit-2/lec-2-2-slides.html#practical-implementation",
    "href": "unit-2/lec-2-2-slides.html#practical-implementation",
    "title": "Unit 2.2: Advanced Randomization Techniques",
    "section": "Practical Implementation",
    "text": "Practical Implementation\n\n\nCreate single entry per randomization unit\nSort file in reproducible way\nSet and preserve random seed\nAssign treatments\nSave assignments securely\nTest balance extensively\n\n\nAlways document your randomization procedure thoroughly!"
  },
  {
    "objectID": "unit-2/lec-2-2-slides.html#ethical-considerations",
    "href": "unit-2/lec-2-2-slides.html#ethical-considerations",
    "title": "Unit 2.2: Advanced Randomization Techniques",
    "section": "Ethical Considerations",
    "text": "Ethical Considerations\n\n\n\nLong-term benefits vs. short-term resource distribution\n\nEquity in who receives potentially beneficial treatments\n\nTransparency with participants about randomization\n\nMinimize harm from potentially ineffective interventions"
  },
  {
    "objectID": "unit-2/lec-2-2-slides.html#combining-randomization-approaches",
    "href": "unit-2/lec-2-2-slides.html#combining-randomization-approaches",
    "title": "Unit 2.2: Advanced Randomization Techniques",
    "section": "Combining Randomization Approaches",
    "text": "Combining Randomization Approaches\n\n\nStratify on discrete variables\nRe-randomize on continuous variables within strata\nUse matched pairs for important continuous variables\nAdapt to your specific context and constraints\n\n\n\nThere is no one-size-fits-all approach!"
  },
  {
    "objectID": "unit-2/lec-2-2-slides.html#key-takeaways",
    "href": "unit-2/lec-2-2-slides.html#key-takeaways",
    "title": "Unit 2.2: Advanced Randomization Techniques",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\n\nChoose randomization unit carefully (individual vs. cluster)\nBalance variables that strongly predict outcomes\nUse re-randomization, stratification, or matching when feasible\nDocument your randomization procedure completely\nTest balance appropriately for your sample size\nAccount for your randomization procedure in analysis"
  },
  {
    "objectID": "unit-2/lec-2-2-slides.html#which-method-when",
    "href": "unit-2/lec-2-2-slides.html#which-method-when",
    "title": "Unit 2.2: Advanced Randomization Techniques",
    "section": "Which Method When?",
    "text": "Which Method When?\n\n\n\n\n\n\n\nApproach\nWhen to Use\nKey Consideration\n\n\n\nSimple\nLarge samples\nSimplicity\n\n\nStratified\nStrong predictors known\nNumber of strata\n\n\nMatched-Pair\nSmall samples\nFinding good matches\n\n\nRe-randomization\nBalance is critical\nComplexity of inference\n\n\nCluster\nGroup-level intervention\nICC and number of clusters"
  },
  {
    "objectID": "unit-2/lec-2-2-slides.html#coming-up-machine-learning-for-causal-inference",
    "href": "unit-2/lec-2-2-slides.html#coming-up-machine-learning-for-causal-inference",
    "title": "Unit 2.2: Advanced Randomization Techniques",
    "section": "Coming Up: Machine Learning for Causal Inference",
    "text": "Coming Up: Machine Learning for Causal Inference\nWe’ll explore how modern prediction algorithms can enhance our ability to:\n\n\nEstimate average treatment effects more precisely\nDiscover heterogeneous treatment effects\nSelect optimal treatments for individuals"
  }
]