[
  {
    "objectID": "unit-2/lec-2-2.html",
    "href": "unit-2/lec-2-2.html",
    "title": "Unit 2.2: Randomization Techniques",
    "section": "",
    "text": "Building on our previous discussion of optimal experimental design where we focused on maximizing statistical power under various constraints, today we turn our attention to the art and science of randomization itself. Randomization is the cornerstone of causal inference in experimental research, enabling us to make causal claims by balancing both observable and unobservable characteristics between treatment and control groups. Whereas observational studies must rely on often-questionable assumptions about selection mechanisms, properly randomized experiments provide a foundation for causal inference that is far more credible.\nThe power of randomization comes from its ability to create groups that are, in expectation, identical on all characteristics—not just those we can observe and measure, but also on unobservable factors that might influence outcomes. This property allows us to attribute any differences in outcomes between treatment and control groups to the treatment itself, rather than to pre-existing differences between groups.\n\nRandomization works because it satisfies three essential conditions:\n\n\nNon-zero probability condition: Each unit has a positive probability of receiving any treatment assignment.\n\nIndividualism: The assignment of one unit doesn’t depend on the assignments of other units.\n\nUnconfoundedness: The treatment assignment is independent of potential outcomes.\n\nWhen these conditions are met, we can write:\n\\[E[Y_i(0)|D_i=1] = E[Y_i(0)|D_i=0]\\]\nThis equation states that the expected untreated potential outcome for those in the treatment group equals the expected untreated potential outcome for those in the control group. In other words, the groups are balanced on the counterfactual outcome we never get to observe for the treatment group. This balance on unobservables is the key to establishing causality.\n\nBefore discussing specific randomization methods, let’s identify what’s generally required to implement randomization:\n\n\nSample of units: The individuals, clusters, or entities to be randomized\n\nAllocation ratio: The proportion of units to assign to each treatment condition\n\nRandomization device: A physical or computational mechanism to generate random assignments\n\nBaseline covariates: (For some approaches) Information on characteristics to balance across groups\n\n\n\nRandom Sampling vs. Random Assignment\n\nIt’s important to distinguish between two distinct concepts that are sometimes confused:\n\n\nRandom sampling: The process of selecting units from a population so that each unit has a known probability of selection\n\nRandom assignment: The process of allocating units to treatment conditions through a random process\n\nRandom sampling helps with external validity (generalizability), while random assignment helps with internal validity (causal inference). In many experiments, we don’t have a random sample from the population, but we still randomize treatment assignment within our convenience sample.\n\n\n\n\n\n\nflowchart TD\n    %% Top nodes - conditions\n    A[\"Non-zero probability condition\"]:::gold\n    B[\"Individualism condition\"]:::gold\n    C[\"Unconfoundedness condition\"]:::gold\n    \n    %% Middle node - mechanisms\n    subgraph D[Classical Random Assignment Mechanisms]\n        F[\"Bernoulli Trial\"]:::carolinaBlue\n        G[\"Complete Randomized\\nExperiment (CRE)\"]:::carolinaBlue\n        H[\"Stratified Randomization\"]:::carolinaBlue\n        I[\"Rerandomization\"]:::carolinaBlue\n        J[\"Matched Pairs\"]:::carolinaBlue\n    end\n    subgraph E[Complex Experimental Designs]\n        K[\"Blocking\"]:::carolinaBlue\n        L[\"Covariate-adaptive Randomization\"]:::carolinaBlue\n        M[\"Minimization\"]:::carolinaBlue\n    end\n    \n    %% Bottom node - inference\n    N{{\"Design-conscious Inference\"}}:::uncGreen\n    \n    %% Connections\n    A --&gt; D\n    B --&gt; D\n    C --&gt; D\n    A --&gt; E\n    B --&gt; E\n    C --&gt; E\n    D --&gt; N\n    E --&gt; N\n\n    %% UNC Brand Colors\n    classDef gold fill:#FFD100,stroke:#13294B,stroke-width:1px,color:#13294B\n    classDef lightGrey fill:#F7F7F7,stroke:#13294B,stroke-width:1px,color:#13294B\n    classDef carolinaBlue fill:#4B9CD3,stroke:#13294B,stroke-width:1px,color:#FFFFFF\n    classDef uncGreen fill:#8DB434,stroke:#13294B,stroke-width:1px,color:#13294B\n    \n    %% Apply lightGrey style to subgraphs\n    style D fill:#F7F7F7,stroke:#13294B,stroke-width:1px,color:#13294B\n    style E fill:#F7F7F7,stroke:#13294B,stroke-width:1px,color:#13294B\n\n\n\n\n\n\n\nThere are five primary approaches to random assignment, each with distinct advantages and disadvantages:\n\nBernoulli trials\nComplete randomization\nRe-randomization\nStratified randomization\nMatched-pair designs\n\nLet’s examine each approach in detail."
  },
  {
    "objectID": "unit-2/lec-2-2.html#introduction",
    "href": "unit-2/lec-2-2.html#introduction",
    "title": "Unit 2.2: Randomization Techniques",
    "section": "",
    "text": "Building on our previous discussion of optimal experimental design where we focused on maximizing statistical power under various constraints, today we turn our attention to the art and science of randomization itself. Randomization is the cornerstone of causal inference in experimental research, enabling us to make causal claims by balancing both observable and unobservable characteristics between treatment and control groups. Whereas observational studies must rely on often-questionable assumptions about selection mechanisms, properly randomized experiments provide a foundation for causal inference that is far more credible.\nThe power of randomization comes from its ability to create groups that are, in expectation, identical on all characteristics—not just those we can observe and measure, but also on unobservable factors that might influence outcomes. This property allows us to attribute any differences in outcomes between treatment and control groups to the treatment itself, rather than to pre-existing differences between groups.\n\nRandomization works because it satisfies three essential conditions:\n\n\nNon-zero probability condition: Each unit has a positive probability of receiving any treatment assignment.\n\nIndividualism: The assignment of one unit doesn’t depend on the assignments of other units.\n\nUnconfoundedness: The treatment assignment is independent of potential outcomes.\n\nWhen these conditions are met, we can write:\n\\[E[Y_i(0)|D_i=1] = E[Y_i(0)|D_i=0]\\]\nThis equation states that the expected untreated potential outcome for those in the treatment group equals the expected untreated potential outcome for those in the control group. In other words, the groups are balanced on the counterfactual outcome we never get to observe for the treatment group. This balance on unobservables is the key to establishing causality.\n\nBefore discussing specific randomization methods, let’s identify what’s generally required to implement randomization:\n\n\nSample of units: The individuals, clusters, or entities to be randomized\n\nAllocation ratio: The proportion of units to assign to each treatment condition\n\nRandomization device: A physical or computational mechanism to generate random assignments\n\nBaseline covariates: (For some approaches) Information on characteristics to balance across groups\n\n\n\nRandom Sampling vs. Random Assignment\n\nIt’s important to distinguish between two distinct concepts that are sometimes confused:\n\n\nRandom sampling: The process of selecting units from a population so that each unit has a known probability of selection\n\nRandom assignment: The process of allocating units to treatment conditions through a random process\n\nRandom sampling helps with external validity (generalizability), while random assignment helps with internal validity (causal inference). In many experiments, we don’t have a random sample from the population, but we still randomize treatment assignment within our convenience sample.\n\n\n\n\n\n\nflowchart TD\n    %% Top nodes - conditions\n    A[\"Non-zero probability condition\"]:::gold\n    B[\"Individualism condition\"]:::gold\n    C[\"Unconfoundedness condition\"]:::gold\n    \n    %% Middle node - mechanisms\n    subgraph D[Classical Random Assignment Mechanisms]\n        F[\"Bernoulli Trial\"]:::carolinaBlue\n        G[\"Complete Randomized\\nExperiment (CRE)\"]:::carolinaBlue\n        H[\"Stratified Randomization\"]:::carolinaBlue\n        I[\"Rerandomization\"]:::carolinaBlue\n        J[\"Matched Pairs\"]:::carolinaBlue\n    end\n    subgraph E[Complex Experimental Designs]\n        K[\"Blocking\"]:::carolinaBlue\n        L[\"Covariate-adaptive Randomization\"]:::carolinaBlue\n        M[\"Minimization\"]:::carolinaBlue\n    end\n    \n    %% Bottom node - inference\n    N{{\"Design-conscious Inference\"}}:::uncGreen\n    \n    %% Connections\n    A --&gt; D\n    B --&gt; D\n    C --&gt; D\n    A --&gt; E\n    B --&gt; E\n    C --&gt; E\n    D --&gt; N\n    E --&gt; N\n\n    %% UNC Brand Colors\n    classDef gold fill:#FFD100,stroke:#13294B,stroke-width:1px,color:#13294B\n    classDef lightGrey fill:#F7F7F7,stroke:#13294B,stroke-width:1px,color:#13294B\n    classDef carolinaBlue fill:#4B9CD3,stroke:#13294B,stroke-width:1px,color:#FFFFFF\n    classDef uncGreen fill:#8DB434,stroke:#13294B,stroke-width:1px,color:#13294B\n    \n    %% Apply lightGrey style to subgraphs\n    style D fill:#F7F7F7,stroke:#13294B,stroke-width:1px,color:#13294B\n    style E fill:#F7F7F7,stroke:#13294B,stroke-width:1px,color:#13294B\n\n\n\n\n\n\n\nThere are five primary approaches to random assignment, each with distinct advantages and disadvantages:\n\nBernoulli trials\nComplete randomization\nRe-randomization\nStratified randomization\nMatched-pair designs\n\nLet’s examine each approach in detail."
  },
  {
    "objectID": "unit-2/lec-2-2.html#bernoulli-trials",
    "href": "unit-2/lec-2-2.html#bernoulli-trials",
    "title": "Unit 2.2: Randomization Techniques",
    "section": "Bernoulli Trials",
    "text": "Bernoulli Trials\nBernoulli trials represent the simplest approach to randomization, where each unit is assigned to treatment independently with a fixed probability. This is conceptually equivalent to flipping a coin for each participant, with heads resulting in treatment assignment and tails resulting in control assignment.\nImplementation\nLet’s first create a dataset to work with:\n\nlibrary(data.table)\n\n# Generate data with 1000 participants\nn &lt;- 1000\n\n# Create baseline covariates\ndt &lt;- data.table(\n  # ID variable\n  id = 1:n,\n  \n  # Covariates that will be used for stratification\n  age = sample(18:80, n, replace = TRUE),                   # Continuous \n  education = sample(c(\"None\", \"Primary\", \"Secondary\", \"Higher\"), n, replace = TRUE, \n                    prob = c(0.1, 0.3, 0.4, 0.2)),          # Categorical\n  \n  # Additional covariates not used for stratification\n  female = rbinom(n, 1, 0.55),                              # Binary\n  income = round(rlnorm(n, meanlog = 10, sdlog = 1), 2),    # Continuous, right-skewed\n  rural = rbinom(n, 1, 0.4),                                # Binary\n  chronic_disease = rbinom(n, 1, 0.3),                      # Binary\n  satisfaction = sample(1:5, n, replace = TRUE,             # Ordinal 1-5 scale\n                       prob = c(0.1, 0.2, 0.4, 0.2, 0.1))\n)\n\n# Convert categorical variables to factors for clearer output\ndt[, education := factor(education, levels = c(\"None\", \"Primary\", \"Secondary\", \"Higher\"))]\n\n# View data structure\nprint(\"Data structure:\")\n\n[1] \"Data structure:\"\n\nstr(dt)\n\nClasses 'data.table' and 'data.frame':  1000 obs. of  8 variables:\n $ id             : int  1 2 3 4 5 6 7 8 9 10 ...\n $ age            : int  40 47 21 23 64 51 39 41 61 67 ...\n $ education      : Factor w/ 4 levels \"None\",\"Primary\",..: 3 3 3 3 4 2 3 4 2 3 ...\n $ female         : int  1 0 1 1 1 0 1 0 0 1 ...\n $ income         : num  5792 12208 9697 6121 10606 ...\n $ rural          : int  0 0 0 1 1 1 0 0 0 1 ...\n $ chronic_disease: int  0 1 0 0 1 0 0 1 0 1 ...\n $ satisfaction   : int  3 4 4 1 4 2 3 2 1 2 ...\n - attr(*, \".internal.selfref\")=&lt;externalptr&gt; \n\n# View the first 10 observations\nhead(dt)\n\n      id   age education female   income rural chronic_disease satisfaction\n   &lt;int&gt; &lt;int&gt;    &lt;fctr&gt;  &lt;int&gt;    &lt;num&gt; &lt;int&gt;           &lt;int&gt;        &lt;int&gt;\n1:     1    40 Secondary      1  5791.94     0               0            3\n2:     2    47 Secondary      0 12207.81     0               1            4\n3:     3    21 Secondary      1  9696.58     0               0            4\n4:     4    23 Secondary      1  6120.62     1               0            1\n5:     5    64    Higher      1 10606.00     1               1            4\n6:     6    51   Primary      0 11727.67     1               0            2\n\n\nNow let’s randomize using Bernoulli:\n\n# Make a copy of the data (so we can compare to other randomization methods below)\ndt_bern &lt;- copy(dt)\n\n# Bernoulli trial example\nset.seed(072111)\n\np &lt;- 0.5  # Probability of treatment assignment\n\n# Independent random assignment\ndt_bern[, treatment := rbinom(.N, 1, p)]\n\n# Check resulting allocation\ndt_bern[, .N, by = treatment]\n\n   treatment     N\n       &lt;int&gt; &lt;int&gt;\n1:         0   514\n2:         1   486\n\n\nAdvantages and Disadvantages\nAdvantages:\n\nSimple to implement\nCan randomize as participants arrive (no need to know full sample in advance)\nNo baseline data needed\n\nDisadvantages: - Random group sizes (can result in imbalanced treatment allocation) - Potential imbalance on key covariates - Vulnerable to implementation problems\nCase Study: The Canadian National Breast Screening Study\nThe Canadian National Breast Screening Study (CNBSS) provides a cautionary tale about vulnerabilities in Bernoulli-type randomization. This major randomized trial evaluated the effectiveness of mammography screening for breast cancer in the 1980s.\nThe study used a variant of simple alternating assignment—assigning the first woman to treatment, the second to control, and so on. However, several critical flaws emerged:\n\n\nPre-randomization examination: Women received clinical breast exams before randomization, providing information that could influence assignment\n\nKnowledge of the assignment schedule: Staff knew that assignments alternated, creating opportunities for manipulation\n\nInadequate concealment: Study coordinators could see which group the next woman would be assigned to\n\nEvidence of manipulation: Later audits found names overwritten, identities reversed, and lines skipped in assignment ledgers\n\nThe consequences were severe: women with palpable lumps were disproportionately assigned to the mammography group, creating a significant selection bias. The mammography group had a 68% higher incidence of advanced cancers at baseline! This likely masked any potential benefits of screening, as the study ultimately reported no mortality benefit from mammography.[^1]\nThis case highlights how vulnerable simple randomization schemes can be to manipulation, especially when those implementing the study have preferences about treatment assignment or when the randomization process is transparent and predictable. Note that there is nothing wrong with Bernoulli trials, but you should carefully consider how to impelment randomization to preserve the integrity of the randomization process. Especially when working with partner organizations (which we do all the time in health services research), one needs to work with these partners to devise a randomization protocol that fits their existing workflow but protects the randomization process."
  },
  {
    "objectID": "unit-2/lec-2-2.html#complete-randomization",
    "href": "unit-2/lec-2-2.html#complete-randomization",
    "title": "Unit 2.2: Randomization Techniques",
    "section": "Complete Randomization",
    "text": "Complete Randomization\nComplete randomization addresses some of the limitations of Bernoulli trials by fixing the number of units assigned to each treatment condition, ensuring the desired allocation ratio is achieved exactly.\nImplementation\n\n# Copy data\ndt_cr &lt;- copy(dt)\n\nset.seed(072111)  # Set seed for reproducibility\n\n# Parameters\np &lt;- 0.5          # Proportion to assign to treatment\n\n# Add a column with uniform random numbers from 0 to 1.\ndt_cr[, random_num := runif(.N, min = 0, max = 1)]\n\n# Sort by random number\nsetorder(dt_cr, random_num)\n\n# Assign first p% to treatment\ndt_cr[, treatment := 0]\ndt_cr[1:(.N*p), treatment := 1]\n\n# Check resulting allocation\ndt_cr[, .N, by = treatment]\n\n   treatment     N\n       &lt;num&gt; &lt;int&gt;\n1:         1   500\n2:         0   500\n\n\nIn complete randomization, we first determine exactly how many units will receive each treatment. Then we generate a random ordering of all units and assign the first \\(N_p\\) units to treatment and the remaining \\(N_0\\) units to control.\nAdvantages and Disadvantages\nAdvantages:\n\nGuarantees exactly the desired allocation ratio\nAvoids power loss from uneven group sizes - Still relatively simple to implement\n\nDisadvantages:\n\nRequires knowing the full sample in advance\nStill subject to chance imbalance on covariates\n\nLet’s check the balance we got for the two examples above by running t-tests comparing the treatment and control groups.\n\n# First let's create a function to perform t-tests and create a formatted results table\nrun_ttests &lt;- function(dt, title) {\n  results &lt;- data.table(\n    variable = character(),\n    method = character(),\n    mean_control = numeric(),\n    mean_treated = numeric(),\n    diff = numeric(),\n    p_value = numeric(),\n    significant = character()\n  )\n  \n  for (var in baseline_vars) {\n    # For categorical variables (factor), we need to handle differently\n    if (is.factor(dt[[var]])) {\n      # For each level of the factor\n      for (level in levels(dt[[var]])) {\n        # Create temporary binary indicator\n        dt[, temp := as.numeric(get(var) == level)]\n        \n        # Calculate means\n        mean_control &lt;- dt[treatment == 0, mean(temp)]\n        mean_treated &lt;- dt[treatment == 1, mean(temp)]\n        \n        # Run t-test\n        t_result &lt;- t.test(dt[treatment == 1, temp], \n                           dt[treatment == 0, temp])\n        \n        # Add to results\n        results &lt;- rbind(results, data.table(\n          variable = paste0(var, \": \", level),\n          method = title,\n          mean_control = mean_control,\n          mean_treated = mean_treated,\n          diff = mean_treated - mean_control,\n          p_value = t_result$p.value,\n          significant = ifelse(t_result$p.value &lt; 0.05, \"*\", \"\")\n        ))\n        \n        # Remove temporary variable\n        dt[, temp := NULL]\n      }\n    } else {\n      # For continuous and binary variables\n      mean_control &lt;- dt[treatment == 0, mean(get(var), na.rm = TRUE)]\n      mean_treated &lt;- dt[treatment == 1, mean(get(var), na.rm = TRUE)]\n      \n      # Run t-test\n      t_result &lt;- t.test(\n        dt[treatment == 1, get(var)], \n        dt[treatment == 0, get(var)]\n      )\n      \n      # Add to results\n      results &lt;- rbind(results, data.table(\n        variable = var,\n        method = title,\n        mean_control = mean_control,\n        mean_treated = mean_treated,\n        diff = mean_treated - mean_control,\n        p_value = t_result$p.value,\n        significant = ifelse(t_result$p.value &lt; 0.05, \"*\", \"\")\n      ))\n    }\n  }\n  \n  return(results)\n}\n\n# Now let's run the tests\n\n## List of all baseline covariates to test\nbaseline_vars &lt;- c(\"age\", \"education\", \"female\", \"income\", \"rural\", \"chronic_disease\", \"satisfaction\")\n\n# Run t-tests\nttest_bern &lt;- run_ttests(dt_bern, \"Bernoulli - t-test\")\nttest_cr &lt;- run_ttests(dt_cr, \"Complete - t-test\")\n\nprint(ttest_bern)\n\n                variable             method mean_control mean_treated\n                  &lt;char&gt;             &lt;char&gt;        &lt;num&gt;        &lt;num&gt;\n 1:                  age Bernoulli - t-test 4.887938e+01 4.976749e+01\n 2:      education: None Bernoulli - t-test 1.070039e-01 1.090535e-01\n 3:   education: Primary Bernoulli - t-test 2.801556e-01 3.168724e-01\n 4: education: Secondary Bernoulli - t-test 3.754864e-01 3.806584e-01\n 5:    education: Higher Bernoulli - t-test 2.373541e-01 1.934156e-01\n 6:               female Bernoulli - t-test 5.564202e-01 5.679012e-01\n 7:               income Bernoulli - t-test 3.452184e+04 3.747225e+04\n 8:                rural Bernoulli - t-test 4.377432e-01 4.300412e-01\n 9:      chronic_disease Bernoulli - t-test 2.996109e-01 2.860082e-01\n10:         satisfaction Bernoulli - t-test 2.990272e+00 3.045267e+00\n             diff    p_value significant\n            &lt;num&gt;      &lt;num&gt;      &lt;char&gt;\n 1:  8.881123e-01 0.44957890            \n 2:  2.049607e-03 0.91699859            \n 3:  3.671679e-02 0.20535455            \n 4:  5.172055e-03 0.86629743            \n 5: -4.393845e-02 0.09099811            \n 6:  1.148100e-02 0.71486875            \n 7:  2.950418e+03 0.29454099            \n 8: -7.702038e-03 0.80620579            \n 9: -1.360266e-02 0.63693948            \n10:  5.499512e-02 0.41151295            \n\nprint(ttest_cr)\n\n                variable            method mean_control mean_treated      diff\n                  &lt;char&gt;            &lt;char&gt;        &lt;num&gt;        &lt;num&gt;     &lt;num&gt;\n 1:                  age Complete - t-test       49.994       48.628    -1.366\n 2:      education: None Complete - t-test        0.110        0.106    -0.004\n 3:   education: Primary Complete - t-test        0.316        0.280    -0.036\n 4: education: Secondary Complete - t-test        0.380        0.376    -0.004\n 5:    education: Higher Complete - t-test        0.194        0.238     0.044\n 6:               female Complete - t-test        0.566        0.558    -0.008\n 7:               income Complete - t-test    37361.170    34550.307 -2810.863\n 8:                rural Complete - t-test        0.428        0.440     0.012\n 9:      chronic_disease Complete - t-test        0.286        0.300     0.014\n10:         satisfaction Complete - t-test        3.048        2.986    -0.062\n       p_value significant\n         &lt;num&gt;      &lt;char&gt;\n 1: 0.24483919            \n 2: 0.83873244            \n 3: 0.21371058            \n 4: 0.89635215            \n 5: 0.09109053            \n 6: 0.79900493            \n 7: 0.31669697            \n 8: 0.70219559            \n 9: 0.62712551            \n10: 0.35364891            \n\n\nChance Imbalance in Complete Randomization\nEven with perfect implementation of complete randomization, covariates may still be imbalanced by chance. In a simulation study using data from the National Longitudinal Survey of Youth (NLSY) with 722 subjects:[^1]\n\n~45% of randomizations had all covariates balanced\n~30% had one imbalanced covariate\nThe remaining had multiple imbalanced covariates\n\nThis raises two critical questions: 1. How can we ensure better balance in the design phase? 2. What should we do if imbalance occurs after randomization?\nOur next three approaches address the first question by improving balance through more sophisticated randomization techniques."
  },
  {
    "objectID": "unit-2/lec-2-2.html#re-randomization",
    "href": "unit-2/lec-2-2.html#re-randomization",
    "title": "Unit 2.2: Randomization Techniques",
    "section": "Re-randomization",
    "text": "Re-randomization\nRe-randomization attempts to improve covariate balance by generating multiple possible randomizations and selecting one with good balance.\nThere are two common approaches to re-randomization:\n\n\nThreshold approach: Generate randomizations until all p-values for covariate balance exceed a threshold (e.g., p &gt; 0.1)\n\nOptimization approach: Generate a large number of randomizations (e.g., 1,000) and select the one with the best overall balance\n\nThreshold Approach\n\n# Copy the data again\ndt_rerand_threshold &lt;- copy(dt)\n\n# Re-randomization - Threshold Approach:\n# Function to test balance on key covariates\ntest_balance &lt;- function(dt) {\n  # Define key covariates to check balance on\n  balance_vars &lt;- c(\"age\", \"education\", \"female\", \"income\", \"rural\", \"chronic_disease\", \"satisfaction\")\n  \n  # Store p-values\n  p_values &lt;- numeric(length(balance_vars))\n  names(p_values) &lt;- balance_vars\n  \n  for (i in seq_along(balance_vars)) {\n    var &lt;- balance_vars[i]\n    \n    # For categorical variables\n    if (is.factor(dt[[var]])) {\n      # Create model matrix (one-hot encoding)\n      formula_str &lt;- paste0(\"~ \", var, \" - 1\")\n      mm &lt;- model.matrix(as.formula(formula_str), data = dt)\n      # Test each level except the reference\n      p_vals_cat &lt;- numeric(ncol(mm))\n      for (j in 1:ncol(mm)) {\n        t_result &lt;- t.test(mm[dt$treatment == 1, j], mm[dt$treatment == 0, j])\n        p_vals_cat[j] &lt;- t_result$p.value\n      }\n      # Use minimum p-value (most imbalanced category)\n      p_values[i] &lt;- min(p_vals_cat)\n    } else {\n      # For continuous or binary variables\n      t_result &lt;- t.test(dt[treatment == 1, get(var)], dt[treatment == 0, get(var)])\n      p_values[i] &lt;- t_result$p.value\n    }\n  }\n  \n  return(p_values)\n}\n\n# Re-randomization with threshold approach\n# Continue generating randomizations until all p-values &gt; 0.10\nmax_attempts &lt;- 1000  # Safety limit to prevent infinite loops\nattempt &lt;- 0\nbalanced &lt;- FALSE\n\ncat(\"\\nPerforming re-randomization with threshold approach...\\n\")\n\n\nPerforming re-randomization with threshold approach...\n\nwhile (!balanced && attempt &lt; max_attempts) {\n  attempt &lt;- attempt + 1\n  \n  # Generate a new randomization\n  dt_rerand_threshold[, treatment := rbinom(.N, 1, 0.5)]\n  \n  # Test balance\n  p_values &lt;- test_balance(dt_rerand_threshold)\n  \n  # Check if all p-values are above threshold (0.10)\n  if (all(p_values &gt; 0.10)) {\n    balanced &lt;- TRUE\n    cat(\"Found balanced randomization after\", attempt, \"attempts\\n\")\n    cat(\"Balance p-values:\", paste(names(p_values), round(p_values, 4), collapse=\", \"), \"\\n\\n\")\n  } else if (attempt %% 100 == 0) {\n    cat(\"Completed\", attempt, \"attempts, continuing search...\\n\")\n  }\n}\n\nFound balanced randomization after 7 attempts\nBalance p-values: age 0.3183, education 0.5236, female 0.143, income 0.5659, rural 0.6529, chronic_disease 0.6613, satisfaction 0.2225 \n\nif (!balanced) {\n  cat(\"Warning: Could not find perfectly balanced randomization after\", max_attempts, \"attempts\\n\")\n  cat(\"Using best randomization found so far\\n\\n\")\n}\n\nOptimization approach\nYou can define the “best” overall balance in different ways. Here we’ll use the Mahalanobis distance.\n\n\n\n\n\n\nNote\n\n\n\nThe Mahalanobis distance is a statistical measure that quantifies the distance between a point and a distribution in multivariate space. The Mahalanobis distance is formally defined as:\n\\[\nd_{maha} = \\sqrt{(x_B - X_A)^TC^{-1}(x_B - x_A)}\n\\]\nWhere: • \\(x_A\\) and \\(x_B\\) are a pair of objects • \\(C\\) is the sample covariance matrix • \\(C^{-1}\\) is the inverse of the covariance matrix • \\(T\\) denotes the transpose operation\n\n\n\n# Copy data\n\ndt_rerand_optimal &lt;- copy(dt)\n\n# Re-randomization - Optimization Approach:\n# Generate multiple randomizations and select the one with best overall balance\n\ncat(\"Performing re-randomization with optimization approach...\\n\")\n\nPerforming re-randomization with optimization approach...\n\nn_candidates &lt;- 1000\nmahalanobis_distances &lt;- numeric(n_candidates)\n\n# Generate covariates matrix (need to handle factors separately)\ncat(\"Converting covariates to numeric for Mahalanobis distance calculation...\\n\")\n\nConverting covariates to numeric for Mahalanobis distance calculation...\n\ncov_vars &lt;- c(\"age\", \"female\", \"income\", \"rural\", \"chronic_disease\", \"satisfaction\")\n\n# Add dummy variables for education\nedu_dummies &lt;- model.matrix(~ education - 1, data = dt_rerand_optimal)\nX &lt;- cbind(\n  as.matrix(dt_rerand_optimal[, ..cov_vars]),\n  edu_dummies\n)\n\n# Calculate covariance matrix of covariates\nS &lt;- cov(X)\nS_inv &lt;- tryCatch({\n  solve(S)  # Try to compute inverse\n}, error = function(e) {\n  cat(\"Covariance matrix is singular, using pseudoinverse...\\n\")\n  # Use pseudoinverse if matrix is singular\n  library(MASS)\n  ginv(S)\n})\n\nCovariance matrix is singular, using pseudoinverse...\n\n# Generate candidate randomizations and calculate balance measure\nfor (i in 1:n_candidates) {\n  # Generate a new randomization\n  treatment_assign &lt;- rbinom(n, 1, 0.5)\n  \n  # Calculate difference in means\n  mean_diff &lt;- colMeans(X[treatment_assign == 1, ]) - colMeans(X[treatment_assign == 0, ])\n  \n  # Calculate Mahalanobis distance\n  mahalanobis_distances[i] &lt;- t(mean_diff) %*% S_inv %*% mean_diff\n  \n  if (i %% 200 == 0) cat(\"Generated\", i, \"candidate randomizations...\\n\")\n}\n\nGenerated 200 candidate randomizations...\nGenerated 400 candidate randomizations...\nGenerated 600 candidate randomizations...\nGenerated 800 candidate randomizations...\nGenerated 1000 candidate randomizations...\n\n# Find the randomization with smallest Mahalanobis distance\nbest_idx &lt;- which.min(mahalanobis_distances)\ncat(\"Selected optimal randomization (candidate\", best_idx, \"with Mahalanobis distance =\", \n    round(mahalanobis_distances[best_idx], 4), \")\\n\\n\")\n\nSelected optimal randomization (candidate 190 with Mahalanobis distance = 0 )\n\n# Generate the best randomization\nset.seed(best_idx)  # For reproducibility\ndt_rerand_optimal[, treatment := rbinom(.N, 1, 0.5)]\n\nDrawbacks of Re-randomization\nWhile re-randomization can improve balance, it has several limitations:\n\n\nOpaque constraints: The process creates a “black box” where it’s unclear what constraints are being imposed\n\nUnusual handling of outliers: Extreme values may force unusual allocation patterns\n\nComputational cost: May require many iterations, especially with multiple covariates\n\nPotential futility: If criteria are too strict, acceptable randomizations may be extremely rare\n\nStatistical inference complications: Standard methods don’t account for the re-randomization process\n\nLimited scope: Still cannot balance on unobserved covariates"
  },
  {
    "objectID": "unit-2/lec-2-2.html#stratified-block-randomization",
    "href": "unit-2/lec-2-2.html#stratified-block-randomization",
    "title": "Unit 2.2: Randomization Techniques",
    "section": "Stratified (Block) Randomization",
    "text": "Stratified (Block) Randomization\nStratified randomization (also called block randomization) directly addresses the balance issue by dividing the sample into strata based on covariates and randomizing separately within each stratum.\nIn this approach, we first create strata based on combinations of important covariates, then randomize separately within each stratum. This guarantees perfect balance on the stratification variables.\nSelecting Stratification Variables\nNot all covariates are equally important for stratification. Consider these guidelines:\n\n\nDiscrete variables are easier to implement than continuous ones\nPrioritize variables that strongly predict outcomes (baseline values of the outcome variable are especially important)\nInclude variables where heterogeneous effects are expected (facilitates subgroup analysis)\nBe careful about creating too many strata, which can lead to “small cell” problems\nHandling “Misfits”\nA practical challenge in stratified randomization occurs when strata sizes are not divisible by the number of treatment conditions (e.g., three people in a stratum with two treatment conditions). Options for these “misfits” include:\n\nRemove units randomly to create divisible strata\nCreate a separate stratum for misfits\nUse a different randomization approach for misfits\nImplementation\n\n# Copy the Data\ndt_strat &lt;- copy(dt)\n\n# Set the seed using Zoe's bday\nset.seed(072111)  # For reproducibility\n\n# Convert categorical variables to factors for clearer output\ndt_strat[, education := factor(education, levels = c(\"None\", \"Primary\", \"Secondary\", \"Higher\"))]\n\n# Create age quintiles\ndt_strat[, age_quintile := cut(age, \n                         breaks = quantile(age, probs = seq(0, 1, 0.2), na.rm = TRUE), \n                         labels = 1:5, \n                         include.lowest = TRUE)]\n\n# View data structure\nprint(\"Data structure:\")\n\n[1] \"Data structure:\"\n\nstr(dt_strat)\n\nClasses 'data.table' and 'data.frame':  1000 obs. of  9 variables:\n $ id             : int  1 2 3 4 5 6 7 8 9 10 ...\n $ age            : int  40 47 21 23 64 51 39 41 61 67 ...\n $ education      : Factor w/ 4 levels \"None\",\"Primary\",..: 3 3 3 3 4 2 3 4 2 3 ...\n $ female         : int  1 0 1 1 1 0 1 0 0 1 ...\n $ income         : num  5792 12208 9697 6121 10606 ...\n $ rural          : int  0 0 0 1 1 1 0 0 0 1 ...\n $ chronic_disease: int  0 1 0 0 1 0 0 1 0 1 ...\n $ satisfaction   : int  3 4 4 1 4 2 3 2 1 2 ...\n $ age_quintile   : Factor w/ 5 levels \"1\",\"2\",\"3\",\"4\",..: 2 3 1 1 4 3 2 2 4 4 ...\n - attr(*, \".internal.selfref\")=&lt;externalptr&gt; \n\n\n\n# Create strata ID by combining education and age quintile\ndt_strat[, strata := .GRP, by = .(education, age_quintile)]\n\n# Print strata information\nprint(\"Strata information:\")\n\n[1] \"Strata information:\"\n\ndt_strat[, .(count = .N), by = .(education, age_quintile, strata)][order(strata)]\n\n    education age_quintile strata count\n       &lt;fctr&gt;       &lt;fctr&gt;  &lt;int&gt; &lt;int&gt;\n 1: Secondary            2      1    68\n 2: Secondary            3      2    75\n 3: Secondary            1      3    78\n 4:    Higher            4      4    37\n 5:   Primary            3      5    61\n 6:    Higher            2      6    42\n 7:   Primary            4      7    55\n 8: Secondary            4      8    78\n 9: Secondary            5      9    79\n10:   Primary            1     10    58\n11:    Higher            5     11    38\n12:   Primary            2     12    65\n13:    Higher            1     13    48\n14:      None            3     14    23\n15:   Primary            5     15    59\n16:    Higher            3     16    51\n17:      None            5     17    18\n18:      None            1     18    27\n19:      None            2     19    15\n20:      None            4     20    25\n    education age_quintile strata count\n\n\nNow let’s randomize half of the observations in each strata to treatment and half to control. To ensure 50/50 split, we’ll use the Complete Randomization method within each strata:\n\ndt_strat[, treatment := 0]  # Initialize all to control\nfor (s in unique(dt_strat$strata)) {\n  # Get all units in this stratum\n  stratum_units &lt;- dt_strat[strata == s, id]\n  n_units &lt;- length(stratum_units)\n  \n  # Determine number to treat (half, rounded down)\n  n_treat &lt;- floor(n_units/2)\n  \n  # Misfits: If n_units is odd, flip a coin to decide whether to round up or down\n  if (n_units %% 2 == 1) {\n    if (runif(1) &lt; 0.5) n_treat &lt;- n_treat + 1\n  }\n  \n  # Randomly select units for treatment\n  treated_units &lt;- sample(stratum_units, n_treat)\n  \n  # Assign treatment\n  dt_strat[id %in% treated_units, treatment := 1]\n}\n\n# Print treatment assignment by strata for stratified randomization\nprint(\"Treatment assignment by strata in stratified randomization:\")\n\n[1] \"Treatment assignment by strata in stratified randomization:\"\n\ndt_strat[, .(N = .N, \n             n_treated = sum(treatment), \n             pct_treated = mean(treatment)*100), \n         by = strata][order(strata)]\n\n    strata     N n_treated pct_treated\n     &lt;int&gt; &lt;int&gt;     &lt;num&gt;       &lt;num&gt;\n 1:      1    68        34    50.00000\n 2:      2    75        37    49.33333\n 3:      3    78        39    50.00000\n 4:      4    37        19    51.35135\n 5:      5    61        31    50.81967\n 6:      6    42        21    50.00000\n 7:      7    55        28    50.90909\n 8:      8    78        39    50.00000\n 9:      9    79        40    50.63291\n10:     10    58        29    50.00000\n11:     11    38        19    50.00000\n12:     12    65        33    50.76923\n13:     13    48        24    50.00000\n14:     14    23        12    52.17391\n15:     15    59        29    49.15254\n16:     16    51        26    50.98039\n17:     17    18         9    50.00000\n18:     18    27        14    51.85185\n19:     19    15         8    53.33333\n20:     20    25        13    52.00000\n    strata     N n_treated pct_treated"
  },
  {
    "objectID": "unit-2/lec-2-2.html#matched-pairs-randomization",
    "href": "unit-2/lec-2-2.html#matched-pairs-randomization",
    "title": "Unit 2.2: Randomization Techniques",
    "section": "Matched Pairs Randomization",
    "text": "Matched Pairs Randomization\nMatched pairs randomization represents the extreme case of stratification, where each stratum contains exactly two similar units, and one is randomly assigned to treatment and one to control.\nIn this approach, we first create pairs of similar units based on a distance metric, then randomly assign one member of each pair to treatment and the other to control. This guarantees excellent balance on the matching variables."
  },
  {
    "objectID": "unit-2/lec-2-2.html#implementation-3",
    "href": "unit-2/lec-2-2.html#implementation-3",
    "title": "Unit 2.2: Randomization Techniques",
    "section": "Implementation",
    "text": "Implementation\n\nIn this example, we’re using what is referred to as a “greedy” matching algorithm. It is “greedy” because it makes locally optimal choices at each step without reconsidering earlier decisions, potentially missing the globally optimal solution. We’re using this for now because it is clear and simple to impelment.\nIn practice, “canned” matching commands are commonly used to form matches and these often use other optimal matching algorithms. A non-greedy, optimal matching algorithm would consider all possible ways to pair units and select the configuration that minimizes the total sum of distances across all pairs. This is computationally more intensive (often solved using network optimization methods like the Hungarian algorithm) but guarantees the global minimum total distance.\n\n\ndt_matched &lt;- copy(dt)\n\n# Set the seed using Zoe's bday\nset.seed(072111)  # For reproducibility\n\n# 1. Prepare data for matching\ncat(\"\\nPerforming matched randomization using Mahalanobis distance...\\n\")\n\n\nPerforming matched randomization using Mahalanobis distance...\n\n# Create matrix of variables to match on\nmatch_vars &lt;- c(\"age\", \"female\", \"income\", \"rural\", \"chronic_disease\")\n\n# Convert education to dummy variables for inclusion in distance calculation\nedu_dummies &lt;- model.matrix(~ education - 1, data = dt_matched)\ncolnames(edu_dummies) &lt;- paste0(\"edu_\", levels(dt_matched$education))\n\n# Combine numeric variables with education dummies\nX &lt;- cbind(as.matrix(dt_matched[, ..match_vars]), edu_dummies)\n\n# Calculate covariance matrix and inverse\nS &lt;- cov(X)\nS_inv &lt;- tryCatch({\n  solve(S)  # Try to compute inverse\n}, error = function(e) {\n  cat(\"Covariance matrix is singular, using pseudoinverse...\\n\")\n  # Use pseudoinverse if matrix is singular\n  library(MASS)\n  ginv(S)\n})\n\nCovariance matrix is singular, using pseudoinverse...\n\n# 2. Calculate pairwise Mahalanobis distances between all units\nn &lt;- nrow(dt_matched)\ndist_matrix &lt;- matrix(0, n, n)\n\n# Store original IDs to maintain correct mapping\noriginal_ids &lt;- dt_matched$id\n\ncat(\"Calculating Mahalanobis distances between all units...\\n\")\n\nCalculating Mahalanobis distances between all units...\n\nfor (i in 1:(n-1)) {\n  for (j in (i+1):n) {\n    # Calculate vector of differences between units i and j\n    diff &lt;- X[i,] - X[j,]\n    \n    # Calculate Mahalanobis distance\n    dist_matrix[i,j] &lt;- dist_matrix[j,i] &lt;- sqrt(t(diff) %*% S_inv %*% diff)\n  }\n  \n  if (i %% 100 == 0) cat(\"Processed\", i, \"of\", n, \"units...\\n\")\n}\n\nProcessed 100 of 1000 units...\nProcessed 200 of 1000 units...\nProcessed 300 of 1000 units...\nProcessed 400 of 1000 units...\nProcessed 500 of 1000 units...\nProcessed 600 of 1000 units...\nProcessed 700 of 1000 units...\nProcessed 800 of 1000 units...\nProcessed 900 of 1000 units...\n\n# 3. Create optimal pairs using greedy algorithm\ncat(\"Creating optimal pairs...\\n\")\n\nCreating optimal pairs...\n\nunpaired_idx &lt;- 1:n  # These are indices, not IDs\npairs_idx &lt;- list()  # Store pairs of indices\n\nwhile (length(unpaired_idx) &gt;= 2) {\n  # Find the nearest neighbor for the first unpaired unit\n  current_idx &lt;- unpaired_idx[1]\n  distances &lt;- dist_matrix[current_idx, unpaired_idx[-1]]\n  nearest_pos &lt;- which.min(distances)\n  nearest_idx &lt;- unpaired_idx[nearest_pos + 1]  # +1 because we excluded current from unpaired\n  \n  # Create a new pair of indices\n  pairs_idx &lt;- c(pairs_idx, list(c(current_idx, nearest_idx)))\n  \n  # Remove these indices from unpaired\n  unpaired_idx &lt;- unpaired_idx[!unpaired_idx %in% c(current_idx, nearest_idx)]\n  \n  if (length(pairs_idx) %% 50 == 0) cat(\"Created\", length(pairs_idx), \"pairs...\\n\")\n}\n\nCreated 50 pairs...\nCreated 100 pairs...\nCreated 150 pairs...\nCreated 200 pairs...\nCreated 250 pairs...\nCreated 300 pairs...\nCreated 350 pairs...\nCreated 400 pairs...\nCreated 450 pairs...\nCreated 500 pairs...\n\n# Handle leftover unit if odd number of units\nif (length(unpaired_idx) == 1) {\n  cat(\"Note: Odd number of units. Unit index\", unpaired_idx, \"will be randomly assigned.\\n\")\n  # This unit will be handled separately\n}\n\n# 4. Convert index pairs to ID pairs and assign treatments\npairs &lt;- lapply(pairs_idx, function(p) original_ids[p])\n\n# Reset pair_id and treatment in the dataset\ndt_matched[, pair_id := NA_integer_]\ndt_matched[, treatment := 0]  # Initialize all to control\n\ncat(\"Assigning pair IDs and treatment...\\n\")\n\nAssigning pair IDs and treatment...\n\nfor (i in seq_along(pairs)) {\n  # Assign pair ID to both units in the pair\n  dt_matched[id %in% pairs[[i]], pair_id := i]\n  \n  # Randomly select one unit for treatment\n  treated_id &lt;- sample(pairs[[i]], 1)\n  dt_matched[id == treated_id, treatment := 1]\n}\n\n# Handle leftover unit if exists\nif (length(unpaired_idx) == 1) {\n  leftover_id &lt;- original_ids[unpaired_idx]\n  \n  # Assign to a special \"pair\" ID\n  dt_matched[id == leftover_id, pair_id := length(pairs) + 1]\n  \n  # Randomly assign to treatment or control\n  if (runif(1) &lt; 0.5) {\n    dt_matched[id == leftover_id, treatment := 1]\n  }\n}\n\n# Verify pair assignments\npair_counts &lt;- dt_matched[!is.na(pair_id), .N, by = pair_id]\ncat(\"\\nVerifying pair assignments:\\n\")\n\n\nVerifying pair assignments:\n\ncat(\"Number of unique pairs:\", nrow(pair_counts), \"\\n\")\n\nNumber of unique pairs: 500 \n\ncat(\"Distribution of pair sizes:\\n\")\n\nDistribution of pair sizes:\n\nprint(table(pair_counts$N))\n\n\n  2 \n500 \n\n# Check treatment balance\ncat(\"\\nTreatment balance:\\n\")\n\n\nTreatment balance:\n\ncat(\"Treatment group size:\", dt_matched[treatment == 1, .N], \"\\n\")\n\nTreatment group size: 500 \n\ncat(\"Control group size:\", dt_matched[treatment == 0, .N], \"\\n\")\n\nControl group size: 500 \n\n# Display the first 10 pairs, sorted by pair_id\ndt_matched[pair_id %in% 1:10][order(pair_id), .(id, pair_id, treatment, age, education, income)]\n\n       id pair_id treatment   age education    income\n    &lt;int&gt;   &lt;int&gt;     &lt;num&gt; &lt;int&gt;    &lt;fctr&gt;     &lt;num&gt;\n 1:     1       1         1    40 Secondary   5791.94\n 2:   692       1         0    40 Secondary   5220.68\n 3:     2       2         1    47 Secondary  12207.81\n 4:   490       2         0    47    Higher  12221.35\n 5:     3       3         0    21 Secondary   9696.58\n 6:   218       3         1    21 Secondary   9352.28\n 7:     4       4         0    23 Secondary   6120.62\n 8:    62       4         1    23   Primary   6932.93\n 9:     5       5         1    64    Higher  10606.00\n10:   713       5         0    64    Higher  11687.50\n11:     6       6         0    51   Primary  11727.67\n12:   205       6         1    51      None  12339.29\n13:     7       7         0    39 Secondary  14393.03\n14:   864       7         1    40   Primary  14013.66\n15:     8       8         1    41    Higher  40222.12\n16:   765       8         0    40   Primary  40175.39\n17:     9       9         1    61   Primary 125905.25\n18:   733       9         0    62 Secondary 121623.72\n19:    10      10         1    67 Secondary   8530.89\n20:   916      10         0    67 Secondary   7579.20\n       id pair_id treatment   age education    income\n\n\nAdvantages and Limitations\nAdvantages: - Achieves excellent balance on matching variables - Works well with continuous covariates - Reduces variance in treatment effect estimates\nLimitations: - Finding good matches becomes difficult with many covariates - May discard units that can’t be well-matched - Requires all baseline data before randomization - Analysis must account for pairing structure"
  },
  {
    "objectID": "unit-2/lec-2-2.html#verifying-balance-approaches-and-best-practices",
    "href": "unit-2/lec-2-2.html#verifying-balance-approaches-and-best-practices",
    "title": "Unit 2.2: Randomization Techniques",
    "section": "Verifying Balance: Approaches and Best Practices",
    "text": "Verifying Balance: Approaches and Best Practices\nAfter randomization, it’s crucial to verify that balance between the treatment and control group was achieved. Several approaches exist:\n\nIndividual Covariate Tests: Test balance one-by-one\nJoint Omnibus Test: Test that all covariates are jointly balanced (often combined with tests on each covariate)\nRegression-Based Tests: Regress treatment assignment on each covariate. Also allows you to control for strata or matched pair fixed effects.\n\nIndividual Covariate Tests\nThe most common approach is to test each covariate separately: - t-tests for continuous variables - Chi-square tests for categorical variables\nLet’s demonstrate this with data from the strata randomization example.\n\n# List of all baseline covariates to test\nbaseline_vars &lt;- c(\"age\", \"education\", \"female\", \"income\", \"rural\", \"chronic_disease\", \"satisfaction\")\n\n# Function to perform t-tests and create a formatted results table\nrun_ttests &lt;- function(dt, title) {\n  results &lt;- data.table(\n    variable = character(),\n    method = character(),\n    mean_control = numeric(),\n    mean_treated = numeric(),\n    diff = numeric(),\n    p_value = numeric(),\n    significant = character()\n  )\n  \n  for (var in baseline_vars) {\n    # For categorical variables (factor), we need to handle differently\n    if (is.factor(dt[[var]])) {\n      # For each level of the factor\n      for (level in levels(dt[[var]])) {\n        # Create temporary binary indicator\n        dt[, temp := as.numeric(get(var) == level)]\n        \n        # Calculate means\n        mean_control &lt;- dt[treatment == 0, mean(temp)]\n        mean_treated &lt;- dt[treatment == 1, mean(temp)]\n        \n        # Run t-test\n        t_result &lt;- t.test(dt[treatment == 1, temp], \n                           dt[treatment == 0, temp])\n        \n        # Add to results\n        results &lt;- rbind(results, data.table(\n          variable = paste0(var, \": \", level),\n          method = title,\n          mean_control = mean_control,\n          mean_treated = mean_treated,\n          diff = mean_treated - mean_control,\n          p_value = t_result$p.value,\n          significant = ifelse(t_result$p.value &lt; 0.05, \"*\", \"\")\n        ))\n        \n        # Remove temporary variable\n        dt[, temp := NULL]\n      }\n    } else {\n      # For continuous and binary variables\n      mean_control &lt;- dt[treatment == 0, mean(get(var), na.rm = TRUE)]\n      mean_treated &lt;- dt[treatment == 1, mean(get(var), na.rm = TRUE)]\n      \n      # Run t-test\n      t_result &lt;- t.test(\n        dt[treatment == 1, get(var)], \n        dt[treatment == 0, get(var)]\n      )\n      \n      # Add to results\n      results &lt;- rbind(results, data.table(\n        variable = var,\n        method = title,\n        mean_control = mean_control,\n        mean_treated = mean_treated,\n        diff = mean_treated - mean_control,\n        p_value = t_result$p.value,\n        significant = ifelse(t_result$p.value &lt; 0.05, \"*\", \"\")\n      ))\n    }\n  }\n  \n  return(results)\n}\n\n# Run t-tests for all randomization methods\nttest_strat &lt;- run_ttests(dt_strat, \"Stratified - t-test\")\n\n#Display Result\n\nJoint Omnibus Tests\nF-tests can test multiple covariates simultaneously, reducing the multiple testing problem.\n\n# Function to perform F-test\njoint_balance_test &lt;- function(dt) {\n  # Regress treatment on covariates\n  model &lt;- lm(treatment ~ age + income + female + education + chronic_disease, \n              data = dt)\n  \n  # Extract F-statistic and p-value for joint significance\n  f_test &lt;- summary(model)$fstatistic\n  f_value &lt;- f_test[1]\n  df1 &lt;- f_test[2]\n  df2 &lt;- f_test[3]\n  p_value &lt;- pf(f_value, df1, df2, lower.tail = FALSE)\n  \n  result &lt;- data.table(\n    f_value = f_value,\n    df1 = df1,\n    df2 = df2,\n    p_value = p_value\n  )\n  \n  return(result)\n}\n\n# Run joint test\njoint_test &lt;- joint_balance_test(dt_strat)\ncat(\"Joint balance test (F-test):\\n\")\n\nJoint balance test (F-test):\n\ncat(\"F(\", joint_test$df1, \",\", joint_test$df2, \") = \", \n    round(joint_test$f_value, 2), \", p = \", round(joint_test$p_value, 4), \"\\n\", sep=\"\")\n\nF(7,992) = 0.02, p = 1\n\n\nRegression-Based Tests\nRegress treatment assignment on each covariate; if randomization worked, coefficients should be insignificant.\nAgain, we’ll use the data created for the stratified randomization example. First, let’s look at this approach not controlling for strata fixed effects:\n\nlibrary(fixest) # For fixed effects\n\n# Function to perform OLS regressions without strata fixed effects\nrun_ols_no_strata &lt;- function(dt, title) {\n  results &lt;- data.table(\n    variable = character(),\n    method = character(),\n    coefficient = numeric(),\n    std_error = numeric(),\n    p_value = numeric(),\n    significant = character()\n  )\n  \n  for (var in baseline_vars) {\n    # Handle factor variables\n    if (is.factor(dt[[var]])) {\n      # Create one-hot encoding\n      for (level in levels(dt[[var]])[2:length(levels(dt[[var]]))]) {  # Skip first level (reference)\n        dt[, temp := as.numeric(get(var) == level)]\n        \n        # Run regression\n        reg &lt;- feols(temp ~ treatment, data = dt)\n        \n        # Add to results\n        results &lt;- rbind(results, data.table(\n          variable = paste0(var, \": \", level, \" vs \", levels(dt[[var]])[1]),\n          method = title,\n          coefficient = coef(reg)[\"treatment\"],\n          std_error = se(reg)[\"treatment\"],\n          p_value = pvalue(reg)[\"treatment\"],\n          significant = ifelse(pvalue(reg)[\"treatment\"] &lt; 0.05, \"*\", \"\")\n        ))\n        \n        # Remove temporary variable\n        dt[, temp := NULL]\n      }\n    } else {\n      # For continuous and binary variables\n      formula_str &lt;- paste0(var, \" ~ treatment\")\n      reg &lt;- feols(as.formula(formula_str), data = dt)\n      \n      # Add to results\n      results &lt;- rbind(results, data.table(\n        variable = var,\n        method = title,\n        coefficient = coef(reg)[\"treatment\"],\n        std_error = se(reg)[\"treatment\"],\n        p_value = pvalue(reg)[\"treatment\"],\n        significant = ifelse(pvalue(reg)[\"treatment\"] &lt; 0.05, \"*\", \"\")\n      ))\n    }\n  }\n  \n  return(results)\n}\n\n# Run OLS without strata for both randomization methods\nols_strat &lt;- run_ols_no_strata(dt_strat, \"Stratified  - OLS no Strata FE\")\n\n# Display results\nprint(ols_strat)\n\n                       variable                         method   coefficient\n                         &lt;char&gt;                         &lt;char&gt;         &lt;num&gt;\n1:                          age Stratified  - OLS no Strata FE  0.2090373784\n2:   education: Primary vs None Stratified  - OLS no Strata FE -0.0007680492\n3: education: Secondary vs None Stratified  - OLS no Strata FE -0.0060483871\n4:    education: Higher vs None Stratified  - OLS no Strata FE  0.0005440348\n5:                       female Stratified  - OLS no Strata FE -0.0009920635\n6:                       income Stratified  - OLS no Strata FE  2.8542677931\n7:                        rural Stratified  - OLS no Strata FE -0.0029441884\n8:              chronic_disease Stratified  - OLS no Strata FE -0.0026881720\n9:                 satisfaction Stratified  - OLS no Strata FE  0.0137288786\n      std_error   p_value significant\n          &lt;num&gt;     &lt;num&gt;      &lt;char&gt;\n1: 1.174695e+00 0.8587981            \n2: 2.895711e-02 0.9788449            \n3: 3.069808e-02 0.8438456            \n4: 2.605334e-02 0.9833443            \n5: 3.141113e-02 0.9748108            \n6: 2.807387e+03 0.9991890            \n7: 3.137833e-02 0.9252640            \n8: 2.881511e-02 0.9256916            \n9: 6.684218e-02 0.8373074            \n\n\nThese should be very close to the t-tests above.\nNow let’s do this controlling for strata. The reason we’d want to do this is because, as we’ll see later, we need to estimate the treatment effect following the way the randomization was done, period. In other words, these need to be aligned. So, if we want to test our balance on baseline covariates, estimate the treatment effect once we have the endline data, it makes sense to include strata fixed effects here.\n\n# Function to perform OLS regressions with strata fixed effects\nrun_ols_with_strata &lt;- function(dt, title) {\n  results &lt;- data.table(\n    variable = character(),\n    method = character(),\n    coefficient = numeric(),\n    std_error = numeric(),\n    p_value = numeric(),\n    significant = character()\n  )\n  \n  for (var in baseline_vars) {\n    # Handle factor variables\n    if (is.factor(dt[[var]])) {\n      for (level in levels(dt[[var]])[2:length(levels(dt[[var]]))]) {  # Skip first level (reference)\n        dt[, temp := as.numeric(get(var) == level)]\n        \n        # Run regression with strata fixed effects\n        reg &lt;- feols(temp ~ treatment | strata, data = dt)\n        \n        # Add to results\n        results &lt;- rbind(results, data.table(\n          variable = paste0(var, \": \", level, \" vs \", levels(dt[[var]])[1]),\n          method = title,\n          coefficient = coef(reg)[\"treatment\"],\n          std_error = se(reg)[\"treatment\"],\n          p_value = pvalue(reg)[\"treatment\"],\n          significant = ifelse(pvalue(reg)[\"treatment\"] &lt; 0.05, \"*\", \"\")\n        ))\n        \n        # Remove temporary variable\n        dt[, temp := NULL]\n      }\n    } else {\n      # For continuous and binary variables\n      formula_str &lt;- paste0(var, \" ~ treatment | strata\")\n      reg &lt;- feols(as.formula(formula_str), data = dt)\n      \n      # Add to results\n      results &lt;- rbind(results, data.table(\n        variable = var,\n        method = title,\n        coefficient = coef(reg)[\"treatment\"],\n        std_error = se(reg)[\"treatment\"],\n        p_value = pvalue(reg)[\"treatment\"],\n        significant = ifelse(pvalue(reg)[\"treatment\"] &lt; 0.05, \"*\", \"\")\n      ))\n    }\n  }\n  \n  return(results)\n}\n\n# Run OLS with strata/pair fixed effects\nols_strat_fe &lt;- run_ols_with_strata(dt_strat, \"Stratified - OLS with strata FE\")\n\n# Display results\nprint(ols_strat_fe)\n\n                       variable                          method   coefficient\n                         &lt;char&gt;                          &lt;char&gt;         &lt;num&gt;\n1:                          age Stratified - OLS with strata FE   0.226507575\n2:   education: Primary vs None Stratified - OLS with strata FE   0.000000000\n3: education: Secondary vs None Stratified - OLS with strata FE   0.000000000\n4:    education: Higher vs None Stratified - OLS with strata FE   0.000000000\n5:                       female Stratified - OLS with strata FE  -0.001311876\n6:                       income Stratified - OLS with strata FE -99.947370871\n7:                        rural Stratified - OLS with strata FE  -0.003836295\n8:              chronic_disease Stratified - OLS with strata FE  -0.003131371\n9:                 satisfaction Stratified - OLS with strata FE   0.014196760\n      std_error   p_value significant\n          &lt;num&gt;     &lt;num&gt;      &lt;char&gt;\n1: 2.319294e-01 0.3410275            \n2:          NaN        NA        &lt;NA&gt;\n3:          NaN        NA        &lt;NA&gt;\n4:          NaN        NA        &lt;NA&gt;\n5: 2.551271e-02 0.9595272            \n6: 3.422442e+03 0.9770068            \n7: 3.582832e-02 0.9158525            \n8: 2.681132e-02 0.9082499            \n9: 5.704633e-02 0.8061365            \n\n\n\n\n\n\n\n\nNote\n\n\n\nNotice the regressions for education variables are not computing. Why do you think this is?\n\n\nStandardized Differences\nFor large samples, p-values become less informative as tiny imbalances become statistically significant. Standardized mean differences (SMDs) provide a sample size-independent measure:\n\\[SMD = \\frac{\\bar{X}_{treatment} - \\bar{X}_{control}}{\\sqrt{\\frac{s^2_{treatment} + s^2_{control}}{2}}}\\]\nA common rule of thumb is that an absolute SMD less than 0.1 indicates negligible imbalance.\n\n# Calculate standardized differences \nstandardized_diff &lt;- function(dt) {\n  covariates &lt;- c(\"age\", \"income\", \"female\", \"chronic_disease\")\n  results &lt;- data.table(\n    covariate = character(),\n    mean_treated = numeric(),\n    mean_control = numeric(),\n    sd_treated = numeric(),\n    sd_control = numeric(),\n    std_diff = numeric(),\n    stringsAsFactors = FALSE\n  )\n  \n  for (cov in covariates) {\n    # Calculate means and SDs by group\n    treated &lt;- dt[treatment == 1, get(cov)]  # Use get() to retrieve column dynamically\n    control &lt;- dt[treatment == 0, get(cov)]  # Use get() to retrieve column dynamically\n    \n    mean_treated &lt;- mean(treated, na.rm = TRUE)\n    mean_control &lt;- mean(control, na.rm = TRUE)\n    sd_treated &lt;- sd(treated, na.rm = TRUE)\n    sd_control &lt;- sd(control, na.rm = TRUE)\n    \n    # Calculate standardized difference\n    pooled_sd &lt;- sqrt((sd_treated^2 + sd_control^2) / 2)\n    std_diff &lt;- (mean_treated - mean_control) / pooled_sd\n    \n    results &lt;- rbind(results, data.table(\n      covariate = cov,\n      mean_treated = mean_treated,\n      mean_control = mean_control,\n      sd_treated = sd_treated,\n      sd_control = sd_control,\n      std_diff = std_diff\n    ))\n  }\n  \n  # Handle education separately (categorical variable)\n  if (\"education\" %in% names(dt)) {\n    # For each level of education\n    edu_levels &lt;- levels(dt$education)\n    for (level in edu_levels) {\n      # Create indicator for this level\n      dt[, temp := as.numeric(education == level)]\n      \n      # Calculate means by group\n      treated &lt;- dt[treatment == 1, mean(temp, na.rm = TRUE)]\n      control &lt;- dt[treatment == 0, mean(temp, na.rm = TRUE)]\n      \n      # For proportions, the SD is sqrt(p*(1-p))\n      sd_treated &lt;- sqrt(treated * (1-treated))\n      sd_control &lt;- sqrt(control * (1-control))\n      \n      # Calculate standardized difference\n      pooled_sd &lt;- sqrt((sd_treated^2 + sd_control^2) / 2)\n      # Avoid division by zero\n      if (pooled_sd == 0) {\n        std_diff &lt;- 0\n      } else {\n        std_diff &lt;- (treated - control) / pooled_sd\n      }\n      \n      results &lt;- rbind(results, data.table(\n        covariate = paste0(\"education: \", level),\n        mean_treated = treated,\n        mean_control = control,\n        sd_treated = sd_treated,\n        sd_control = sd_control,\n        std_diff = std_diff\n      ))\n      \n      # Remove temporary variable\n      dt[, temp := NULL]\n    }\n  }\n  \n  return(results)\n}\n\n# Calculate standardized differences\nstd_diffs &lt;- standardized_diff(dt_strat)\nprint(std_diffs)\n\n              covariate mean_treated mean_control   sd_treated   sd_control\n                 &lt;char&gt;        &lt;num&gt;        &lt;num&gt;        &lt;num&gt;        &lt;num&gt;\n1:                  age 4.941468e+01 4.920565e+01 1.848326e+01 1.866369e+01\n2:               income 3.595715e+04 3.595430e+04 4.458545e+04 4.418499e+04\n3:               female 5.615079e-01 5.625000e-01 4.966953e-01 4.965792e-01\n4:      chronic_disease 2.916667e-01 2.943548e-01 4.549813e-01 4.562123e-01\n5:      education: None 1.111111e-01 1.048387e-01 3.142697e-01 3.063455e-01\n6:   education: Primary 2.976190e-01 2.983871e-01 4.572111e-01 4.575503e-01\n7: education: Secondary 3.750000e-01 3.810484e-01 4.841229e-01 4.856444e-01\n8:    education: Higher 2.162698e-01 2.157258e-01 4.117004e-01 4.113249e-01\n        std_diff\n           &lt;num&gt;\n1:  1.125449e-02\n2:  6.430607e-05\n3: -1.997561e-03\n4: -5.900326e-03\n5:  2.021185e-02\n6: -1.679234e-03\n7: -1.247388e-02\n8:  1.322037e-03\n\n\nComprehensive “Table 1”\nNow, let’s create a comprehensive “Table 1” for the stratified random sample\n\n# Install and load required packages\nlibrary(tidyr)\nlibrary(dplyr)\nlibrary(gt)\n\n# Calculate standardized differences for stratified randomization\nstd_diff_strat &lt;- standardized_diff(dt_strat)\n\n# Function to extract p-values from all balance tests\nextract_pvalues &lt;- function(test_results) {\n  results &lt;- test_results[, .(variable, p_value)]\n  setnames(results, \"variable\", \"covariate\")\n  return(results)\n}\n\n# Extract p-values from different testing approaches for stratified randomization\npvals_strat_ttest &lt;- extract_pvalues(ttest_strat)\npvals_strat_ttest[, approach := \"t-test\"]\n\npvals_strat_ols &lt;- extract_pvalues(ols_strat)\npvals_strat_ols[, approach := \"OLS (no strata FE)\"]\n\npvals_strat_ols_fe &lt;- extract_pvalues(ols_strat_fe)\npvals_strat_ols_fe[, approach := \"OLS (with strata FE)\"]\n\n# Combine all p-values for stratified randomization\nstrat_pvals &lt;- rbind(\n  pvals_strat_ttest,\n  pvals_strat_ols,\n  pvals_strat_ols_fe\n)\n\n# Create a wide format of p-values\npvals_wide &lt;- strat_pvals %&gt;%\n  as.data.frame() %&gt;%\n  pivot_wider(\n    id_cols = covariate,\n    names_from = approach,\n    values_from = p_value\n  )\n\n# Join with standardized differences\nstrat_balance_table &lt;- merge(\n  std_diff_strat, \n  pvals_wide,\n  by = \"covariate\", \n  all = TRUE\n)\n\n# Mark significant values with asterisks directly in the data\nstrat_balance_table &lt;- strat_balance_table %&gt;%\n  mutate(\n    std_diff_mark = ifelse(abs(std_diff) &gt; 0.25, \n                           paste0(format(round(std_diff, 3), nsmall=3), \"†\"), \n                           format(round(std_diff, 3), nsmall=3)),\n    `t-test_mark` = ifelse(`t-test` &lt; 0.05, \n                          paste0(format(round(`t-test`, 3), nsmall=3), \"*\"), \n                          format(round(`t-test`, 3), nsmall=3)),\n    `OLS (no strata FE)_mark` = ifelse(`OLS (no strata FE)` &lt; 0.05, \n                                      paste0(format(round(`OLS (no strata FE)`, 3), nsmall=3), \"*\"), \n                                      format(round(`OLS (no strata FE)`, 3), nsmall=3)),\n    `OLS (with strata FE)_mark` = ifelse(`OLS (with strata FE)` &lt; 0.05, \n                                        paste0(format(round(`OLS (with strata FE)`, 3), nsmall=3), \"*\"), \n                                        format(round(`OLS (with strata FE)`, 3), nsmall=3))\n  )\n\n# Create gt table with the marked columns\nstrat_balance_table %&gt;%\n  select(covariate, std_diff_mark, `t-test_mark`, `OLS (no strata FE)_mark`, `OLS (with strata FE)_mark`) %&gt;%\n  gt() %&gt;%\n  # Rename columns\n  cols_label(\n    covariate = \"Baseline Covariate\",\n    std_diff_mark = \"Standardized Difference\",\n    `t-test_mark` = \"t-test\",\n    `OLS (no strata FE)_mark` = \"OLS (standard)\",\n    `OLS (with strata FE)_mark` = \"OLS (with strata FE)\"\n  ) %&gt;%\n  # Add column spanners for organization\n  tab_spanner(\n    label = \"Balance Measure\",\n    columns = \"std_diff_mark\"\n  ) %&gt;%\n  tab_spanner(\n    label = \"P-values by Testing Approach\",\n    columns = c(\"t-test_mark\", \"OLS (no strata FE)_mark\", \"OLS (with strata FE)_mark\")\n  ) %&gt;%\n  # Add title and footnotes\n  tab_header(\n    title = \"Table 1: Balance Assessment in Stratified Randomization\",\n    subtitle = \"Comparison of Different Testing Approaches\"\n  ) %&gt;%\n  tab_footnote(\n    footnote = \"† Standardized differences &gt;0.25, indicating potential imbalance.\",\n    locations = cells_column_labels(columns = \"std_diff_mark\")\n  ) %&gt;%\n  tab_footnote(\n    footnote = \"* P-values &lt;0.05, indicating statistically significant differences.\",\n    locations = cells_column_spanners(spanners = \"P-values by Testing Approach\")\n  ) %&gt;%\n  tab_footnote(\n    footnote = \"OLS with strata fixed effects is the most appropriate approach given the stratified design.\",\n    locations = cells_column_labels(columns = \"OLS (with strata FE)_mark\")\n  ) %&gt;%\n  # Nicer formatting\n  tab_options(\n    column_labels.font.weight = \"bold\",\n    column_labels.background.color = \"#EEEEEE\",\n    table.width = pct(100),\n    heading.background.color = \"#E6F0FF\",\n    heading.title.font.size = px(16),\n    heading.subtitle.font.size = px(14)\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 1: Balance Assessment in Stratified Randomization\n\n\nComparison of Different Testing Approaches\n\n\nBaseline Covariate\n\nBalance Measure\n\n\nP-values by Testing Approach1\n\n\n\nStandardized Difference2\n\nt-test\nOLS (standard)\nOLS (with strata FE)3\n\n\n\n\n\nage\n0.011\n0.859\n0.859\n0.341\n\n\nchronic_disease\n-0.006\n0.926\n0.926\n0.908\n\n\neducation: Higher\n0.001\n0.983\nNA\nNA\n\n\neducation: Higher vs None\nNA\nNA\n0.983\nNA\n\n\neducation: None\n0.020\n0.750\nNA\nNA\n\n\neducation: Primary\n-0.002\n0.979\nNA\nNA\n\n\neducation: Primary vs None\nNA\nNA\n0.979\nNA\n\n\neducation: Secondary\n-0.012\n0.844\nNA\nNA\n\n\neducation: Secondary vs None\nNA\nNA\n0.844\nNA\n\n\nfemale\n-0.002\n0.975\n0.975\n0.960\n\n\nincome\n0.000\n0.999\n0.999\n0.977\n\n\nrural\nNA\n0.925\n0.925\n0.916\n\n\nsatisfaction\nNA\n0.837\n0.837\n0.806\n\n\n\n\n\n1 * P-values &lt;0.05, indicating statistically significant differences.\n\n\n\n2 † Standardized differences &gt;0.25, indicating potential imbalance.\n\n\n\n3 OLS with strata fixed effects is the most appropriate approach given the stratified design.\n\n\n\n\n\n\nInterpreting Balance Results\nWhen reviewing balance tables, it’s important to consider:\n\nStatistical vs. Practical Significance: With large samples, even tiny differences can be statistically significant. Focus on the magnitude of differences (SMD) rather than p-values.\nMultiple Testing: With many covariates, expect some to show “significant” differences by chance. This is why joint tests and SMDs are often more informative.\nKey Predictors: Pay special attention to covariates that strongly predict outcomes. Imbalance on these variables is more concerning.\nVisual Inspection: Sometimes graphical displays of covariate distributions can complement statistical tests. Consider density plots or boxplots comparing treatment and control.\nOverall Assessment: Look at the pattern of results rather than focusing on any single test. If multiple measures suggest imbalance on important predictors, consider covariate adjustment in analysis.\n\nLet’s create visual assessments of balance as well:"
  },
  {
    "objectID": "unit-2/lec-2-2.html#complex-experimental-designs",
    "href": "unit-2/lec-2-2.html#complex-experimental-designs",
    "title": "Unit 2.2: Randomization Techniques",
    "section": "Complex Experimental Designs",
    "text": "Complex Experimental Designs\nBeyond the basic approaches, several complex randomization designs address specific research challenges:\nUnits of Randomization & Spillovers\nA critical decision is choosing what entity (e.g. person or cluster) to randomize:\nKey considerations:\n\nMatch observational unit when possible\nAlign with treatment delivery\nMinimize spillovers\nConsider statistical power\n\nCommon units:\n\n\nIndividual: Patients, students\n\nCluster: Villages, clinics, schools\n\nTime periods: Days, weeks, shifts\n\nNetworks: Households, peer groups\n\nThe tradeoff is often between statistical power (favoring individual randomization) and internal validity (which may require cluster randomization to minimize spillovers).\nThe Spillover Problem\nSpillovers occur when treatment affects untreated units, through:\n\nDirect interaction between units\nGeneral equilibrium effects\nInformation diffusion\nResource competition\n\nWhen spillovers are a concern, randomization at a higher level (clustering) can help minimize unwanted contamination. Alternatively, a two-stage randomization design can help measure spillover effects explicitly.\nTwo-Stage Randomization for Measuring Spillovers\nThis approach:\n\nFirst randomizes clusters to high or low treatment intensity\nThen randomizes individuals within clusters to treatment or control\n\nThis design allows measurement of:\n\nDirect treatment effects\nWithin-cluster spillovers\nBetween-cluster spillovers\nCluster Randomization\nCluster randomization assigns groups rather than individuals to treatment conditions: (we saw this last unit)\nExamples of clusters:\n\nSchools\nClinics\nVillages\nNeighborhoods\n\nThe key parameter affecting statistical power is the intraclass correlation coefficient (ICC), which measures how correlated outcomes are within clusters. The design effect formula shows how clustering reduces effective sample size:\n\\[DE = 1 + (m-1) \\times ICC\\]\nwhere \\(m\\) is the average cluster size. A larger design effect means a larger required sample size to achieve the same power.\nAnalysis needs to account for clustering through one of:\n\nCluster-robust standard errors\nMixed-effects models\nGeneralized estimating equations (GEE)\nFactorial Designs: Testing Multiple Treatments\nFactorial designs test multiple interventions simultaneously:\n\n\nIntervention B\nNo Intervention A\nIntervention A\n\n\n\nNo\nControl\nA only\n\n\nYes\nB only\nA and B\n\n\n\nThis approach:\n\nTests multiple treatments simultaneously\nEstimates main effects AND interactions\nUses resources efficiently\n\nFor example, with two interventions (A and B), we have four groups:\n\nNo intervention (control)\nIntervention A only\nIntervention B only\nBoth A and B\n\nFactorial designs are particularly valuable when:\n\nYou want to test combinations of interventions\nYou suspect treatments might interact (enhance or interfere with each other)\nYou need to maximize efficiency with limited resources\n\nHere’s how we might implement a 2×2 factorial design in R:\n\nlibrary(data.table)\nset.seed(072311)\nn &lt;- 200  # Total sample size\n\n# Create data table\ndt &lt;- data.table(id = 1:n)\n\n# First randomization for factor A\ndt[, A := c(rep(1, n/2), rep(0, n/2))[sample(1:n)]]\n\n# Second randomization for factor B\ndt[, B := c(rep(1, n/2), rep(0, n/2))[sample(1:n)]]\n\n# Create combined treatment variable\ndt[, treatment := paste0(\"A\", A, \"B\", B)]\n\n# Check allocation\ndt[, .N, by = treatment]\n\n   treatment     N\n      &lt;char&gt; &lt;int&gt;\n1:      A1B1    49\n2:      A0B1    51\n3:      A1B0    51\n4:      A0B0    49\n\n\nRandomized Phase-in Designs\nIn randomized phase-in designs, all units eventually receive treatment, but the timing of treatment is randomized.\nAdvantages:\n\nImproves compliance since everyone gets treatment eventually\nWorks well with resource constraints\nOffers a politically appealing compromise\n\nLimitations:\n\nMay create anticipation effects\nLimits long-term impact measurement\nEventually loses the control group\nAdaptive Randomization\nAdaptive designs update assignment probabilities based on accumulated data, balancing:\n\n\nExploration: Learning which treatment works best\n\nExploitation: Assigning more units to better treatments\n\nThe key *ethical** advantage is that fewer participants receive inferior treatments as evidence accumulates. We’ll cover adaptive designs in more detail in a future lecture.\nA simple example of response-adaptive randomization might look like this:\n\n# Simplified response-adaptive randomization simulation\nsimulate_adaptive_randomization &lt;- function(n_total = 100, \n                                          true_success_A = 0.3, \n                                          true_success_B = 0.5) {\n  # Initialize data storage\n  results &lt;- data.frame(\n    id = 1:n_total,\n    treatment = NA,\n    outcome = NA\n  )\n  \n  # Initial equal assignment probabilities\n  prob_A &lt;- 0.5\n  \n  # Track successes and failures\n  success_A &lt;- 0\n  total_A &lt;- 0\n  success_B &lt;- 0\n  total_B &lt;- 0\n  \n  # Simulate sequential enrollment\n  for (i in 1:n_total) {\n    # Assign treatment based on current probability\n    treatment &lt;- sample(c(\"A\", \"B\"), 1, prob = c(prob_A, 1-prob_A))\n    results$treatment[i] &lt;- treatment\n    \n    # Generate outcome based on true success rates\n    if (treatment == \"A\") {\n      outcome &lt;- rbinom(1, 1, true_success_A)\n      success_A &lt;- success_A + outcome\n      total_A &lt;- total_A + 1\n    } else {\n      outcome &lt;- rbinom(1, 1, true_success_B)\n      success_B &lt;- success_B + outcome\n      total_B &lt;- total_B + 1\n    }\n    results$outcome[i] &lt;- outcome\n    \n    # Update probability for next assignment using Beta prior\n    if (i &lt; n_total) {  # No need to update after last patient\n      # Beta-Bernoulli update (adding 1 for prior)\n      prob_A &lt;- rbeta(1, success_A + 1, total_A - success_A + 1) /\n               (rbeta(1, success_A + 1, total_A - success_A + 1) + \n                rbeta(1, success_B + 1, total_B - success_B + 1))\n    }\n  }\n  \n  return(results)\n}"
  },
  {
    "objectID": "unit-2/lec-2-2.html#practical-implementation-tips",
    "href": "unit-2/lec-2-2.html#practical-implementation-tips",
    "title": "Unit 2.2: Randomization Techniques",
    "section": "Practical Implementation Tips",
    "text": "Practical Implementation Tips\nRegardless of which randomization approach you choose, follow these best practices:\n\nCreate a single entry per randomization unit\nSort the file in a reproducible way\nSet and preserve a random seed\nAssign treatments\nSave assignments securely\nTest balance extensively\n\nAlways document your randomization procedure thoroughly to enhance transparency and reproducibility.\nBelow is a template for implementing and documenting randomization:\n\n# Randomization implementation template\n\n# 1. Document parameters\nstudy_name &lt;- \"My Clinical Trial\"\nrandomization_date &lt;- \"2025-02-25\"\nrandomization_conducted_by &lt;- \"J. Smith\"\nrandom_seed &lt;- 072311  # Document why this seed was chosen\nallocation_ratio &lt;- 0.5  # Equal allocation\nstratification_variables &lt;- c(\"site\", \"gender\")\n\n# 2. Set up reproducible environment\nset.seed(random_seed)\nlibrary(tidyverse)\n\n# 3. Load and prepare data\ndata &lt;- read.csv(\"participant_data.csv\")\n\n# 4. Implement randomization\n# Example: stratified randomization\n\n\n# 5. Check balance\n# Generate Balance Table\n\n# 6. Save results securely\n# a) Save randomization details\nrandomization_log &lt;- data.frame(\n  study_name = study_name,\n  date = randomization_date,\n  conducted_by = randomization_conducted_by,\n  seed = random_seed,\n  method = \"stratified\",\n  stratification_variables = paste(stratification_variables, collapse = \", \")\n)\nwrite.csv(randomization_log, \"randomization_log.csv\", row.names = FALSE)\n\n# b) Save assignment data\nwrite.csv(\n  data %&gt;% select(id, treatment, stratum_id), \n  \"treatment_assignments.csv\", \n  row.names = FALSE\n)\n\n# c) Save balance checks\nwrite.csv(balance_table, \"balance_checks.csv\", row.names = FALSE)"
  },
  {
    "objectID": "unit-2/lec-2-2.html#ethical-considerations-in-randomization",
    "href": "unit-2/lec-2-2.html#ethical-considerations-in-randomization",
    "title": "Unit 2.2: Randomization Techniques",
    "section": "Ethical Considerations in Randomization",
    "text": "Ethical Considerations in Randomization\nRandomization raises several ethical considerations:\n\n\nLong-term benefits vs. short-term resource distribution\n\nEquity in who receives potentially beneficial treatments\n\nTransparency with participants about randomization\n\nMinimizing harm from potentially ineffective interventions\n\nThese issues must be carefully considered and addressed in the design phase. Specific approaches that can address ethical concerns include:\n\n\nRandomized phase-in designs: Ensures everyone eventually receives the intervention\n\nAdaptive randomization: Skews allocation toward better-performing treatments over time\n\nRisk-based randomization: Targets interventions toward those most likely to benefit (we’ll cover in the future if time)\n\nMinimization of control group size: Uses unequal allocation ratios to minimize the number of participants not receiving intervention (could add this consideration to optimal allocation)"
  },
  {
    "objectID": "unit-2/lec-2-2.html#conclusion-which-method-when",
    "href": "unit-2/lec-2-2.html#conclusion-which-method-when",
    "title": "Unit 2.2: Randomization Techniques",
    "section": "Conclusion: Which Method When?",
    "text": "Conclusion: Which Method When?\nThere is no one-size-fits-all approach. The best randomization strategy depends on:\n\nThe research question\nAvailable baseline data\nLogistical constraints\nExpected heterogeneity in treatment effects\n\n\n\n\n\n\n\n\nApproach\nWhen to Use\nKey Consideration\n\n\n\nSimple\nLarge samples\nSimplicity\n\n\nStratified\nStrong predictors known\nNumber of strata\n\n\nMatched-Pair\nSmall samples\nFinding good matches\n\n\nRe-randomization\nBalance is critical\nComplexity of inference\n\n\nCluster\nGroup-level intervention\nICC and number of clusters\n\n\n\nThe choice of randomization method should be guided by:\n\nThe unit of randomization (individual vs. cluster)\nThe importance of balance on specific variables\nFeasibility constraints\nAnalysis plans\n\nHappy randomizing!"
  },
  {
    "objectID": "unit-2/lec-2-2-slides.html",
    "href": "unit-2/lec-2-2-slides.html",
    "title": "Unit 2.2: Advanced Randomization Techniques",
    "section": "",
    "text": "Last session: Maximizing power through optimal experimental design\nToday: The art and science of randomization\nRandomization enables causal claims by balancing all factors:\n\nObservable characteristics\nUnobservable characteristics\nPotential outcomes\n\n\n\n\n\nEmphasize that randomization is what allows us to make causal claims - it’s the key distinction between experimental and observational studies."
  },
  {
    "objectID": "unit-2/lec-2-2-slides.html#randomization-the-foundation-of-causal-inference",
    "href": "unit-2/lec-2-2-slides.html#randomization-the-foundation-of-causal-inference",
    "title": "Unit 2.2: Advanced Randomization Techniques",
    "section": "",
    "text": "Last session: Maximizing power through optimal experimental design\nToday: The art and science of randomization\nRandomization enables causal claims by balancing all factors:\n\nObservable characteristics\nUnobservable characteristics\nPotential outcomes\n\n\n\n\n\nEmphasize that randomization is what allows us to make causal claims - it’s the key distinction between experimental and observational studies."
  },
  {
    "objectID": "unit-2/lec-2-2-slides.html#why-does-randomization-work",
    "href": "unit-2/lec-2-2-slides.html#why-does-randomization-work",
    "title": "Unit 2.2: Advanced Randomization Techniques",
    "section": "Why Does Randomization Work?",
    "text": "Why Does Randomization Work?\nBecause it ensures:\n\n\n\nNon-zero probability condition: Everyone has a chance of treatment\n\nIndividualism: Independence across units\n\nUnconfoundedness: Balance on observed/unobserved covariates\n\n\n\\[E[Y_i(0)|D_i=1] = E[Y_i(0)|D_i=0]\\]\n\nThe untreated potential outcomes are the same in both groups!"
  },
  {
    "objectID": "unit-2/lec-2-2-slides.html#what-you-need-for-randomization",
    "href": "unit-2/lec-2-2-slides.html#what-you-need-for-randomization",
    "title": "Unit 2.2: Advanced Randomization Techniques",
    "section": "What You Need for Randomization",
    "text": "What You Need for Randomization\n\n\n\nSample of units: Who or what will be randomized\n\nAllocation ratio: How many units to each condition\n\nRandomization device: Physical or computational\n\nBaseline covariates: (for some approaches)"
  },
  {
    "objectID": "unit-2/lec-2-2-slides.html#random-sampling-vs.-random-assignment",
    "href": "unit-2/lec-2-2-slides.html#random-sampling-vs.-random-assignment",
    "title": "Unit 2.2: Advanced Randomization Techniques",
    "section": "Random Sampling vs. Random Assignment",
    "text": "Random Sampling vs. Random Assignment\n\n\nRandom Sampling vs. Random Assignment"
  },
  {
    "objectID": "unit-2/lec-2-2-slides.html#classical-assignment-mechanisms-framework",
    "href": "unit-2/lec-2-2-slides.html#classical-assignment-mechanisms-framework",
    "title": "Unit 2.2: Advanced Randomization Techniques",
    "section": "Classical Assignment Mechanisms Framework",
    "text": "Classical Assignment Mechanisms Framework\n\n\n\n\n\nflowchart TD\n    %% Top nodes - conditions\n    A[\"Non-zero probability condition\"]:::gold\n    B[\"Individualism condition\"]:::gold\n    C[\"Unconfoundedness condition\"]:::gold\n    \n    %% Middle node - mechanisms\n    D{{\"Classical Random Assignment Mechanisms\"}}:::tarHeelRed\n    \n    %% Assignment types\n    E[\"Bernoulli Trial\"]:::carolinaBlue\n    F[\"Complete Randomized\\nExperiment (CRE)\"]:::carolinaBlue\n    G[\"Stratified Randomization\"]:::carolinaBlue\n    H[\"Rerandomization\"]:::carolinaBlue\n    I[\"Matched Pairs\"]:::carolinaBlue\n    \n    %% Bottom node - inference\n    J{{\"Design-conscious Inference\"}}:::uncGreen\n    \n    %% Connections\n    A --&gt; D\n    B --&gt; D\n    C --&gt; D\n    \n    D --&gt; E\n    D --&gt; F\n    D --&gt; G\n    D --&gt; H\n    D --&gt; I\n    \n    E --&gt; J\n    F --&gt; J\n    G --&gt; J\n    H --&gt; J\n    I --&gt; J\n    \n    %% UNC Brand Colors\n    classDef gold fill:#FFD100,stroke:#13294B,stroke-width:1px,color:#13294B\n    classDef tarHeelRed fill:#DC143C,stroke:#13294B,stroke-width:1px,color:#FFFFFF\n    classDef carolinaBlue fill:#4B9CD3,stroke:#13294B,stroke-width:1px,color:#FFFFFF\n    classDef uncGreen fill:#8DB434,stroke:#13294B,stroke-width:1px,color:#FFFFFF"
  },
  {
    "objectID": "unit-2/lec-2-2-slides.html#classical-assignment-mechanisms",
    "href": "unit-2/lec-2-2-slides.html#classical-assignment-mechanisms",
    "title": "Unit 2.2: Advanced Randomization Techniques",
    "section": "Classical Assignment Mechanisms",
    "text": "Classical Assignment Mechanisms\n\nBernoulli Trials\nComplete Randomization\nRe-randomization\nStratified Randomization\nMatched-Pair Designs"
  },
  {
    "objectID": "unit-2/lec-2-2-slides.html#bernoulli-trials",
    "href": "unit-2/lec-2-2-slides.html#bernoulli-trials",
    "title": "Unit 2.2: Advanced Randomization Techniques",
    "section": "Bernoulli Trials",
    "text": "Bernoulli Trials\n\nSimplest approach: independent coin flips\n\n\\(P(Z_i = 1) = p\\) for all units \\(i\\)\n\n\n\n\n\nAdvantages:\n\nSimple to implement\nCan randomize as participants arrive\nNo baseline data needed\n\n\n\n\nDisadvantages:\n\nRandom group sizes\nPotential imbalance on key covariates\nImplementation vulnerability"
  },
  {
    "objectID": "unit-2/lec-2-2-slides.html#case-study-the-canadian-national-breast-screening-study",
    "href": "unit-2/lec-2-2-slides.html#case-study-the-canadian-national-breast-screening-study",
    "title": "Unit 2.2: Advanced Randomization Techniques",
    "section": "Case Study: The Canadian National Breast Screening Study",
    "text": "Case Study: The Canadian National Breast Screening Study\n\n\nMajor randomized trial evaluating mammography screening effectiveness\nUsed alternating assignment (first to treatment, second to control)\nDesign flaw: clinical breast exams conducted before randomization\nNurses and physicians could (and did) influence group assignments"
  },
  {
    "objectID": "unit-2/lec-2-2-slides.html#cnbss-randomization-failures",
    "href": "unit-2/lec-2-2-slides.html#cnbss-randomization-failures",
    "title": "Unit 2.2: Advanced Randomization Techniques",
    "section": "CNBSS: Randomization Failures",
    "text": "CNBSS: Randomization Failures\n\n\n\nPre-randomization examination: Detected suspicious lumps before group assignment\n\nSelection bias: Women with palpable lumps disproportionately assigned to mammography group\n\nInadequate concealment: Study staff could influence group assignments\n\nEvidence of manipulation: Names overwritten, identities reversed, lines skipped\n\n\n\nResult: Mammography group had 68% higher incidence of advanced cancers at baseline!"
  },
  {
    "objectID": "unit-2/lec-2-2-slides.html#cnbss-impact-on-study-validity",
    "href": "unit-2/lec-2-2-slides.html#cnbss-impact-on-study-validity",
    "title": "Unit 2.2: Advanced Randomization Techniques",
    "section": "CNBSS: Impact on Study Validity",
    "text": "CNBSS: Impact on Study Validity\n\n\nStudy reported no mortality benefit from mammography screening\n\nHowever: The randomization bias likely masked true benefits\nHigher-risk patients concentrated in treatment group\nControl group contamination: ~25% received mammograms outside the study\n\n\n\nBroader lesson: Compromise in randomization can fundamentally undermine study validity and lead to decades of scientific controversy"
  },
  {
    "objectID": "unit-2/lec-2-2-slides.html#complete-randomization",
    "href": "unit-2/lec-2-2-slides.html#complete-randomization",
    "title": "Unit 2.2: Advanced Randomization Techniques",
    "section": "Complete Randomization",
    "text": "Complete Randomization\n\nset.seed(072311)  # Set seed for reproducibility\n\n# Parameters\nN &lt;- 100          # Total number of units\np &lt;- 0.5          # Proportion to assign to treatment\n\n# Generate random numbers and sort\nunits &lt;- data.frame(\n  id = 1:N,\n  random_num = runif(N)\n)\nunits &lt;- units[order(units$random_num),]\n\n# Assign first p% to treatment\nunits$treatment &lt;- 0\nunits$treatment[1:(N*p)] &lt;- 1\n\n# Check resulting allocation\ntable(units$treatment)\n\n\n 0  1 \n50 50 \n\n\nEach participant has fixed probability of assignment, with total group sizes fixed in advance."
  },
  {
    "objectID": "unit-2/lec-2-2-slides.html#chance-imbalance-with-complete-randomization",
    "href": "unit-2/lec-2-2-slides.html#chance-imbalance-with-complete-randomization",
    "title": "Unit 2.2: Advanced Randomization Techniques",
    "section": "Chance Imbalance with Complete Randomization",
    "text": "Chance Imbalance with Complete Randomization\n\n\nEven with perfect implementation, covariates may be imbalanced\n\nExample: In a study of 722 people (NLSY data):\n\n~45% of randomizations had all covariates balanced\n~30% had one imbalanced covariate\nRemaining had multiple imbalanced covariates\n\n\n\n\n\nThis raises two critical questions: 1. How can we ensure better balance in design? 2. What do we do if imbalance occurs?"
  },
  {
    "objectID": "unit-2/lec-2-2-slides.html#re-randomization",
    "href": "unit-2/lec-2-2-slides.html#re-randomization",
    "title": "Unit 2.2: Advanced Randomization Techniques",
    "section": "Re-randomization",
    "text": "Re-randomization\n\n\nGenerate multiple randomizations\nKeep only those with good balance\nApproach 1: All p-values &gt; threshold (e.g., 0.05)\nApproach 2: Choose iteration with best overall balance"
  },
  {
    "objectID": "unit-2/lec-2-2-slides.html#re-randomization-code",
    "href": "unit-2/lec-2-2-slides.html#re-randomization-code",
    "title": "Unit 2.2: Advanced Randomization Techniques",
    "section": "Re-randomization Code",
    "text": "Re-randomization Code\n\nbalance_check &lt;- function(data, treatment_var, balance_vars) {\n  # Initialize minimum p-value\n  min_pval &lt;- 1\n  \n  # Check each covariate\n  for (var in balance_vars) {\n    # T-test for continuous variables\n    t_result &lt;- t.test(data[[var]] ~ data[[treatment_var]])\n    min_pval &lt;- min(min_pval, t_result$p.value)\n  }\n  \n  return(min_pval)\n}\n\n# Re-randomization function\nrerandomize &lt;- function(data, p_threshold = 0.1, max_attempts = 1000) {\n  n &lt;- nrow(data)\n  n_treat &lt;- floor(n * 0.5)  # 50% to treatment\n  attempts &lt;- 0\n  \n  repeat {\n    # Generate a new randomization\n    treatment &lt;- rep(0, n)\n    treatment[sample(1:n, n_treat)] &lt;- 1\n    data$treatment &lt;- treatment\n    \n    # Check balance\n    pval &lt;- balance_check(data, \"treatment\", c(\"age\", \"income\", \"education\"))\n    \n    attempts &lt;- attempts + 1\n    \n    # Accept if p-value threshold met or max attempts reached\n    if (pval &gt; p_threshold || attempts &gt;= max_attempts) {\n      break\n    }\n  }\n  \n  return(list(treatment = data$treatment, attempts = attempts, min_pval = pval))\n}\n\nThis function repeats randomization until finding one where all p-values exceed our threshold."
  },
  {
    "objectID": "unit-2/lec-2-2-slides.html#drawbacks-of-re-randomization",
    "href": "unit-2/lec-2-2-slides.html#drawbacks-of-re-randomization",
    "title": "Unit 2.2: Advanced Randomization Techniques",
    "section": "Drawbacks of Re-randomization",
    "text": "Drawbacks of Re-randomization\n\n\nOpaque constraints: “Black box” process\nUnusual handling of outliers\nComputationally expensive\nCould run forever if criteria too strict\nStatistical inference must account for the procedure\nCannot balance on unobserved covariates"
  },
  {
    "objectID": "unit-2/lec-2-2-slides.html#stratified-block-randomization",
    "href": "unit-2/lec-2-2-slides.html#stratified-block-randomization",
    "title": "Unit 2.2: Advanced Randomization Techniques",
    "section": "Stratified (Block) Randomization",
    "text": "Stratified (Block) Randomization\n\n\n\n\nStratified Randomization\n\n\n\n\nDivide sample into strata based on covariates\nRandomize separately within each stratum\nPerfect balance on stratification variables"
  },
  {
    "objectID": "unit-2/lec-2-2-slides.html#implementing-stratified-randomization",
    "href": "unit-2/lec-2-2-slides.html#implementing-stratified-randomization",
    "title": "Unit 2.2: Advanced Randomization Techniques",
    "section": "Implementing Stratified Randomization",
    "text": "Implementing Stratified Randomization\n\nset.seed(072311)\n\n# Create example data with categorical covariates\ndata &lt;- data.frame(\n  id = 1:100,\n  gender = sample(c(\"Male\", \"Female\"), 100, replace = TRUE),\n  age_group = sample(c(\"Under 30\", \"30-50\", \"Over 50\"), 100, replace = TRUE),\n  stringsAsFactors = FALSE\n)\n\n# Create strata based on combinations of covariates\ndata$stratum &lt;- paste(data$gender, data$age_group, sep = \"_\")\n\n# Function for stratified randomization\nstratified_randomize &lt;- function(data, strata_var, p = 0.5) {\n  # Initialize assignment vector\n  assignment &lt;- rep(NA, nrow(data))\n  \n  # Get unique strata\n  strata &lt;- unique(data[[strata_var]])\n  \n  # Randomize within each stratum\n  for (s in strata) {\n    # Get indices for this stratum\n    indices &lt;- which(data[[strata_var]] == s)\n    n_stratum &lt;- length(indices)\n    \n    # Calculate number to assign to treatment\n    n_treat &lt;- round(n_stratum * p)\n    \n    # Ensure at least one in each group if possible\n    if (n_stratum &gt; 1) {\n      n_treat &lt;- min(max(n_treat, 1), n_stratum - 1)\n    } else {\n      n_treat &lt;- sample(0:1, 1)  # Random for singletons\n    }\n    \n    # Perform randomization within stratum\n    treat_indices &lt;- sample(indices, n_treat)\n    assignment[indices] &lt;- 0\n    assignment[treat_indices] &lt;- 1\n  }\n  \n  return(assignment)\n}\n\n# Apply stratified randomization\ndata$treatment &lt;- stratified_randomize(data, \"stratum\")\n\n# Check balance by strata\ntable(data$stratum, data$treatment)\n\n                 \n                   0  1\n  Female_30-50    11 10\n  Female_Over 50   8  8\n  Female_Under 30  7  7\n  Male_30-50       9  9\n  Male_Over 50     7  8\n  Male_Under 30    8  8\n\n\nThis code creates strata from combinations of gender and age group, then randomizes within each stratum."
  },
  {
    "objectID": "unit-2/lec-2-2-slides.html#selecting-stratification-variables",
    "href": "unit-2/lec-2-2-slides.html#selecting-stratification-variables",
    "title": "Unit 2.2: Advanced Randomization Techniques",
    "section": "Selecting Stratification Variables",
    "text": "Selecting Stratification Variables\n\n\n\nDiscrete variables are easier to implement\nPrioritize variables that strongly predict outcomes\n\nInclude variables where heterogeneous effects are expected\nBe careful of too many strata - causes “small cell” problems\n\n\n\nHandling “misfits” (when strata size not divisible by treatments):\n\nRemove units randomly\nCreate separate strata for misfits\nUse different randomization approach for misfits"
  },
  {
    "objectID": "unit-2/lec-2-2-slides.html#matched-pairs-randomization",
    "href": "unit-2/lec-2-2-slides.html#matched-pairs-randomization",
    "title": "Unit 2.2: Advanced Randomization Techniques",
    "section": "Matched Pairs Randomization",
    "text": "Matched Pairs Randomization\n\n\nCreate pairs of similar units\nRandomize one to treatment within each pair\nLike stratification taken to the extreme\nPerfect for continuous covariates"
  },
  {
    "objectID": "unit-2/lec-2-2-slides.html#implementing-matched-pairs",
    "href": "unit-2/lec-2-2-slides.html#implementing-matched-pairs",
    "title": "Unit 2.2: Advanced Randomization Techniques",
    "section": "Implementing Matched Pairs",
    "text": "Implementing Matched Pairs\n\nlibrary(optmatch)  # For optimal matching\n\n# Generate example data\nset.seed(072311)\ndata &lt;- data.frame(\n  id = 1:100,\n  age = rnorm(100, 45, 10),\n  income = rnorm(100, 50000, 15000),\n  health_score = rnorm(100, 70, 15)\n)\n\n# Create distance matrix based on covariates\nX &lt;- as.matrix(data[, c(\"age\", \"income\", \"health_score\")])\nX_scaled &lt;- scale(X)  # Standardize to have equal importance\ndist_matrix &lt;- dist(X_scaled)\n\n# Create optimal matches\nmatches &lt;- pairmatch(dist_matrix, data = data)\ndata$pair_id &lt;- matches\n\n# Randomize within pairs\npair_randomize &lt;- function(data, pair_var) {\n  # Initialize assignment vector\n  assignment &lt;- rep(NA, nrow(data))\n  \n  # Get unique pairs\n  pairs &lt;- unique(data[[pair_var]])\n  pairs &lt;- pairs[!is.na(pairs)]  # Remove NA pairs\n  \n  # Randomize within each pair\n  for (p in pairs) {\n    # Get indices for this pair\n    indices &lt;- which(data[[pair_var]] == p)\n    \n    # Skip if not exactly 2 units\n    if (length(indices) != 2) next\n    \n    # Randomly assign one to treatment\n    treat_index &lt;- sample(indices, 1)\n    assignment[indices] &lt;- 0\n    assignment[treat_index] &lt;- 1\n  }\n  \n  return(assignment)\n}\n\n# Apply pair randomization\ndata$treatment &lt;- pair_randomize(data, \"pair_id\")\n\nThis code matches participants based on age, income, and health score, then randomizes one member of each pair to treatment."
  },
  {
    "objectID": "unit-2/lec-2-2-slides.html#balance-tests-approaches-for-verification",
    "href": "unit-2/lec-2-2-slides.html#balance-tests-approaches-for-verification",
    "title": "Unit 2.2: Advanced Randomization Techniques",
    "section": "Balance Tests: Approaches for Verification",
    "text": "Balance Tests: Approaches for Verification\n\n\n\nIndividual covariate tests: t-tests, Chi-square tests\n\nJoint omnibus tests: F-tests testing multiple covariates simultaneously\n\nRegression-based tests: Regress treatment on covariates\n\nStandardized differences: Sample size-independent assessment"
  },
  {
    "objectID": "unit-2/lec-2-2-slides.html#standardized-differences-for-large-samples",
    "href": "unit-2/lec-2-2-slides.html#standardized-differences-for-large-samples",
    "title": "Unit 2.2: Advanced Randomization Techniques",
    "section": "Standardized Differences for Large Samples",
    "text": "Standardized Differences for Large Samples\n\n\nProblem with p-values: With large samples, tiny imbalances become “significant”\n\nSolution: Standardized mean difference (SMD)\n\n\\[SMD = \\frac{\\bar{X}_{treatment} - \\bar{X}_{control}}{\\sqrt{\\frac{s^2_{treatment} + s^2_{control}}{2}}}\\]\n\n\nScale-free measure of imbalance\nIndependent of sample size\nRule of thumb: |SMD| &lt; 0.1 is negligible imbalance"
  },
  {
    "objectID": "unit-2/lec-2-2-slides.html#the-table-1-debate",
    "href": "unit-2/lec-2-2-slides.html#the-table-1-debate",
    "title": "Unit 2.2: Advanced Randomization Techniques",
    "section": "The “Table 1” Debate",
    "text": "The “Table 1” Debate\n\n\nFor Including\n\nTransparency\nDemonstrates randomization effectiveness\nProvides context for readers\nShows extent of imbalance\n\n\nAgainst Including\n\nRandomization guarantees balance in expectation\nOveremphasis on statistically significant differences\nCan lead to inappropriate adjustments\nJournal space limitations\n\n\n\n\nCompromise: Report balance without p-values, use standardized differences"
  },
  {
    "objectID": "unit-2/lec-2-2-slides.html#design-based-inference",
    "href": "unit-2/lec-2-2-slides.html#design-based-inference",
    "title": "Unit 2.2: Advanced Randomization Techniques",
    "section": "Design-Based Inference",
    "text": "Design-Based Inference\n\n\nRandomization creates the foundation for inference\n\nFisher’s approach: Test sharp null that treatment has no effect on any unit\n\nRandomization distribution: Generate distribution of test statistics under all possible randomizations\n\nPermutation tests: Compare observed statistic to randomization distribution"
  },
  {
    "objectID": "unit-2/lec-2-2-slides.html#complex-experimental-designs",
    "href": "unit-2/lec-2-2-slides.html#complex-experimental-designs",
    "title": "Unit 2.2: Advanced Randomization Techniques",
    "section": "Complex Experimental Designs",
    "text": "Complex Experimental Designs\n\nUnits of Randomization & Spillovers\nCluster Randomization\nFactorial Designs\nFractional Factorial Designs\n\nWithin-Subject vs. Between-Subject\nRandomized Phase-in\nAdaptive Designs\nVariable Treatment Probabilities"
  },
  {
    "objectID": "unit-2/lec-2-2-slides.html#choosing-the-unit-of-randomization",
    "href": "unit-2/lec-2-2-slides.html#choosing-the-unit-of-randomization",
    "title": "Unit 2.2: Advanced Randomization Techniques",
    "section": "Choosing the Unit of Randomization",
    "text": "Choosing the Unit of Randomization\n\n\nKey Considerations\n\nShould match observational unit when possible\nMust align with treatment delivery\nNeed to minimize spillovers\nConsider statistical power\n\n\nCommon Units\n\n\nIndividual: Patients, students\n\nCluster: Villages, clinics, schools\n\nTime periods: Days, weeks, shifts\n\nNetworks: Households, peer groups\n\n\n\n\nCritical trade-off: Statistical power vs. internal validity"
  },
  {
    "objectID": "unit-2/lec-2-2-slides.html#the-spillover-problem",
    "href": "unit-2/lec-2-2-slides.html#the-spillover-problem",
    "title": "Unit 2.2: Advanced Randomization Techniques",
    "section": "The Spillover Problem",
    "text": "The Spillover Problem\n\n\nSpillovers occur when treatment affects untreated units\nTypes of spillovers:\n\nDirect interaction between units\nGeneral equilibrium effects\nInformation diffusion\nResource competition\n\n\n\n\n\nRandomization at a higher level (clustering) can minimize unwanted spillovers."
  },
  {
    "objectID": "unit-2/lec-2-2-slides.html#measuring-spillovers-two-stage-randomization",
    "href": "unit-2/lec-2-2-slides.html#measuring-spillovers-two-stage-randomization",
    "title": "Unit 2.2: Advanced Randomization Techniques",
    "section": "Measuring Spillovers: Two-Stage Randomization",
    "text": "Measuring Spillovers: Two-Stage Randomization\n\nFirst stage: Randomize clusters to high or low treatment intensity\nSecond stage: Randomize individuals within clusters to treatment or control\nAllows measurement of:\n\nDirect treatment effects\nWithin-cluster spillovers\nBetween-cluster spillovers\n\n\nProvides estimates of:\n\nTotal treatment effect (direct + spillover)\nIsolated direct effect\nSpillover effect on untreated individuals"
  },
  {
    "objectID": "unit-2/lec-2-2-slides.html#cluster-randomization",
    "href": "unit-2/lec-2-2-slides.html#cluster-randomization",
    "title": "Unit 2.2: Advanced Randomization Techniques",
    "section": "Cluster Randomization",
    "text": "Cluster Randomization\n\n\n\nDefinition: Randomize groups (clusters) rather than individuals\n\nCommon clusters: Schools, clinics, villages, neighborhoods\n\nKey parameter: Intraclass correlation coefficient (ICC)\n\nDesign effect: \\(DE = 1 + (m-1) \\times ICC\\)\n\n\n\\(m\\) = average cluster size\nIncreases required sample size\n\n\n\n\n\nAnalysis must account for clustering! * Cluster-robust standard errors * Mixed-effects models * GEE approaches"
  },
  {
    "objectID": "unit-2/lec-2-2-slides.html#factorial-designs-testing-multiple-treatments",
    "href": "unit-2/lec-2-2-slides.html#factorial-designs-testing-multiple-treatments",
    "title": "Unit 2.2: Advanced Randomization Techniques",
    "section": "Factorial Designs: Testing Multiple Treatments",
    "text": "Factorial Designs: Testing Multiple Treatments\n\n\n\n\nIntervention B\nNo Intervention A\nIntervention A\n\n\n\nNo\nControl\nA only\n\n\nYes\nB only\nA and B\n\n\n\n\n\n\nTest multiple treatments simultaneously\nEstimate main effects AND interactions\nEfficient use of resources\nExample: Two interventions with four groups\n\nNo intervention (control)\nIntervention A only\nIntervention B only\nBoth A and B"
  },
  {
    "objectID": "unit-2/lec-2-2-slides.html#randomized-phase-in-designs",
    "href": "unit-2/lec-2-2-slides.html#randomized-phase-in-designs",
    "title": "Unit 2.2: Advanced Randomization Techniques",
    "section": "Randomized Phase-in Designs",
    "text": "Randomized Phase-in Designs\n\n\nAll units eventually receive treatment\nRandomize the timing of treatment\nAdvantages:\n\nBetter compliance (everyone gets treatment eventually)\nClear for partners with resource constraints\nPolitically appealing compromise\n\n\n\n\n\nBut watch out for:\n\nAnticipation effects\nLimited long-term impact measurement"
  },
  {
    "objectID": "unit-2/lec-2-2-slides.html#adaptive-randomization",
    "href": "unit-2/lec-2-2-slides.html#adaptive-randomization",
    "title": "Unit 2.2: Advanced Randomization Techniques",
    "section": "Adaptive Randomization",
    "text": "Adaptive Randomization\n\n\nUpdates assignment probabilities based on accumulated data\nBalances:\n\n\nExploration: Learning which treatment works best\n\nExploitation: Assigning more units to better treatments\n\n\nKey ethical advantage: Fewer participants receive inferior treatments\n\n\n\nNote: We’ll cover adaptive designs in more detail in a future lecture"
  },
  {
    "objectID": "unit-2/lec-2-2-slides.html#practical-implementation",
    "href": "unit-2/lec-2-2-slides.html#practical-implementation",
    "title": "Unit 2.2: Advanced Randomization Techniques",
    "section": "Practical Implementation",
    "text": "Practical Implementation\n\n\nCreate single entry per randomization unit\nSort file in reproducible way\nSet and preserve random seed\nAssign treatments\nSave assignments securely\nTest balance extensively\n\n\nAlways document your randomization procedure thoroughly!"
  },
  {
    "objectID": "unit-2/lec-2-2-slides.html#ethical-considerations",
    "href": "unit-2/lec-2-2-slides.html#ethical-considerations",
    "title": "Unit 2.2: Advanced Randomization Techniques",
    "section": "Ethical Considerations",
    "text": "Ethical Considerations\n\n\n\nLong-term benefits vs. short-term resource distribution\n\nEquity in who receives potentially beneficial treatments\n\nTransparency with participants about randomization\n\nMinimize harm from potentially ineffective interventions"
  },
  {
    "objectID": "unit-2/lec-2-2-slides.html#combining-randomization-approaches",
    "href": "unit-2/lec-2-2-slides.html#combining-randomization-approaches",
    "title": "Unit 2.2: Advanced Randomization Techniques",
    "section": "Combining Randomization Approaches",
    "text": "Combining Randomization Approaches\n\n\nStratify on discrete variables\nRe-randomize on continuous variables within strata\nUse matched pairs for important continuous variables\nAdapt to your specific context and constraints\n\n\n\nThere is no one-size-fits-all approach!"
  },
  {
    "objectID": "unit-2/lec-2-2-slides.html#key-takeaways",
    "href": "unit-2/lec-2-2-slides.html#key-takeaways",
    "title": "Unit 2.2: Advanced Randomization Techniques",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\n\nChoose randomization unit carefully (individual vs. cluster)\nBalance variables that strongly predict outcomes\nUse re-randomization, stratification, or matching when feasible\nDocument your randomization procedure completely\nTest balance appropriately for your sample size\nAccount for your randomization procedure in analysis"
  },
  {
    "objectID": "unit-2/lec-2-2-slides.html#which-method-when",
    "href": "unit-2/lec-2-2-slides.html#which-method-when",
    "title": "Unit 2.2: Advanced Randomization Techniques",
    "section": "Which Method When?",
    "text": "Which Method When?\n\n\nApproach\nWhen to Use\nKey Consideration\n\n\n\nSimple\nLarge samples\nSimplicity\n\n\nStratified\nStrong predictors known\nNumber of strata\n\n\nMatched-Pair\nSmall samples\nFinding good matches\n\n\nRe-randomization\nBalance is critical\nComplexity of inference\n\n\nCluster\nGroup-level intervention\nICC and number of clusters"
  },
  {
    "objectID": "unit-2/lec-2-2-slides.html#coming-up-machine-learning-for-causal-inference",
    "href": "unit-2/lec-2-2-slides.html#coming-up-machine-learning-for-causal-inference",
    "title": "Unit 2.2: Advanced Randomization Techniques",
    "section": "Coming Up: Machine Learning for Causal Inference",
    "text": "Coming Up: Machine Learning for Causal Inference\nWe’ll explore how modern prediction algorithms can enhance our ability to:\n\n\nEstimate average treatment effects more precisely\nDiscover heterogeneous treatment effects\nSelect optimal treatments for individuals"
  },
  {
    "objectID": "unit-2/unit-2-design-2.html",
    "href": "unit-2/unit-2-design-2.html",
    "title": "Unit 2.2: How to Randomize",
    "section": "",
    "text": "Read:\n\nJ-Pal Web Guide on Randomization\nEGAP: 10 Things to Know about Randomization\nBruhn and McKenzie, “In Pursuit of Balance”\n\n\n\n\n\n\n[Lecture Notes: Randomization Techniques (Coming Soon)]\n\nLecture Slides\n\n\n\n\n\n\n[Comming Soon]",
    "crumbs": [
      "Unit 2: Design of Experiments",
      "Unit 2.2: How to Randomize"
    ]
  },
  {
    "objectID": "unit-2/unit-2-design-2.html#unit-overview",
    "href": "unit-2/unit-2-design-2.html#unit-overview",
    "title": "Unit 2.2: How to Randomize",
    "section": "",
    "text": "Read:\n\nJ-Pal Web Guide on Randomization\nEGAP: 10 Things to Know about Randomization\nBruhn and McKenzie, “In Pursuit of Balance”\n\n\n\n\n\n\n[Lecture Notes: Randomization Techniques (Coming Soon)]\n\nLecture Slides\n\n\n\n\n\n\n[Comming Soon]",
    "crumbs": [
      "Unit 2: Design of Experiments",
      "Unit 2.2: How to Randomize"
    ]
  },
  {
    "objectID": "unit-1/unit-1-internal-2.html",
    "href": "unit-1/unit-1-internal-2.html",
    "title": "Unit 1.2: Statistical Conclusion Validity",
    "section": "",
    "text": "Introduce statistical conclusion validity\nExplore sources of uncertainty in experiments\nIntroduce hypothesis testing\nExamine the role of statistical power in determining sample size\nUse simulation techniques for power calculations and design assessments\nDiscuss multiple hypothesis testing and its implications\n\n\n\n\nRead:\n\nWager Chapter 1\nDeclare Design Book: 5.1, 8.2, 10\n\nListen:\n\nExperimental Design and Multiple Hypothesis Testing Podcast\n\n\n\n\n\n\nLecture: Statistical Conclusion Validity\n\n\n\n\n\nLab 2: Power Calculation by Simulation\n\nDue by 11:59pm on Monday, February 17\nSubmit on Gradescope\nSolutions: lab-2-Power-sols.qmd",
    "crumbs": [
      "Unit 1: Internal Validity",
      "Unit 1.1: Statistical Conclusion Validity"
    ]
  },
  {
    "objectID": "unit-1/unit-1-internal-2.html#unit-overview",
    "href": "unit-1/unit-1-internal-2.html#unit-overview",
    "title": "Unit 1.2: Statistical Conclusion Validity",
    "section": "",
    "text": "Introduce statistical conclusion validity\nExplore sources of uncertainty in experiments\nIntroduce hypothesis testing\nExamine the role of statistical power in determining sample size\nUse simulation techniques for power calculations and design assessments\nDiscuss multiple hypothesis testing and its implications\n\n\n\n\nRead:\n\nWager Chapter 1\nDeclare Design Book: 5.1, 8.2, 10\n\nListen:\n\nExperimental Design and Multiple Hypothesis Testing Podcast\n\n\n\n\n\n\nLecture: Statistical Conclusion Validity\n\n\n\n\n\nLab 2: Power Calculation by Simulation\n\nDue by 11:59pm on Monday, February 17\nSubmit on Gradescope\nSolutions: lab-2-Power-sols.qmd",
    "crumbs": [
      "Unit 1: Internal Validity",
      "Unit 1.1: Statistical Conclusion Validity"
    ]
  },
  {
    "objectID": "unit-1/lec-1-2.html",
    "href": "unit-1/lec-1-2.html",
    "title": "Unit 1-2: Statistical Conclusion Validity",
    "section": "",
    "text": "This lecture delves into statistical conclusion validity, exploring how we can assess the reliability and accuracy of estimated treatment effects in experimental research. As we have seen, due to the fundamental nature of potential outcomes, it is inherently impossible to recover individual treatment effects since each unit reveals only one potential outcome. However, under specific assumptions and with an appropriate assignment mechanism—such as random assignment—we can consistently estimate an average treatment effect (ATE). This lecture provides a brief, selective statistical background on how to estimate the ATE and quantify the uncertainty in these estimates, ensuring statistical conclusion validity. To streamline the presentation, we focus on settings with a binary treatment and largely ignore the role of covariates."
  },
  {
    "objectID": "unit-1/lec-1-2.html#what-is-uncertainty",
    "href": "unit-1/lec-1-2.html#what-is-uncertainty",
    "title": "Unit 1-2: Statistical Conclusion Validity",
    "section": "What is Uncertainty?",
    "text": "What is Uncertainty?\nUncertainty in empirical research refers to the inherent imprecision that arises when we attempt to infer quantities that cannot be directly observed. Whether we are engaged in descriptive, causal, or generalization inference, our estimates come with uncertainty that must be quantified and communicated.\nTwo primary frameworks exist for this purpose: the Bayesian and the frequentist approaches.1\nBayesian Approach\nThe Bayesian framework uses Bayes’ rule to combine prior beliefs with the observed data, resulting in a posterior probability distribution over the parameter of interest, \\(\\theta\\). Mathematically, this is expressed as:\n\\[\n\\Pr(\\theta = \\theta' \\mid d = d') = \\frac{\\Pr(d = d' \\mid \\theta = \\theta')\\, \\Pr(\\theta = \\theta')}{\\sum_{\\theta''} \\Pr(d = d' \\mid \\theta = \\theta'')\\, \\Pr(\\theta = \\theta'')},\n\\]\nwhere:\n\n\\(d\\) represents data, and \\(d'\\) represents the observed data (or an “observed realization of the data”)\n\\(\\theta'\\) and \\(\\theta''\\) represent particular values of the parameter \\(\\theta\\).\n\nFrom this posterior distribution, the posterior mean serves as our best estimate of \\(\\theta\\), and the posterior variance quantifies the uncertainty associated with that estimate.\nBy applying Bayes’ rule over different values of \\(\\theta\\), we construct a complete probability distribution that represents all possible answers. This posterior distribution simultaneously provides our best estimate—often summarized by the posterior mean—and quantifies our uncertainty via the posterior variance.\nWhile intuitive, a challenge is that specifying prior uncertainty (\\(\\Pr(\\theta = \\theta')\\)) is often a subjective choice, and the posterior distribution is often difficult to interpret and communicate.\nFrequentist Approach\nIn contrast, the frequentist approach avoids specifying prior beliefs and focuses on the likelihood function, \\(\\Pr(d = d' \\mid \\theta = \\theta')\\), which describes the probability of observing the data \\(d'\\) given a specific value of \\(\\theta\\).2 I.e. instead of thinking of the strength of beliefs, we consider that \\(\\theta\\) generates the actual probability distriubtion over possible data \\(d\\).\nThis approach yields useful quantities:\nP-value: The p-value for a null hypothesis, \\(\\theta = \\theta_0\\), is defined as the probability of observing data as extreme as \\(d_{m*}\\) under the null hypothesis, or\n\\[\n\\Pr(d = d_{m*} \\mid \\theta = \\theta_0).\n\\]\nwhere \\(d_{m*}\\) is the test statistic.\nConfidence Interval: A 95% confidence interval is constructed such that, if the experiment were repeated many times, 95% of the intervals would contain the true parameter value. This approach provides a framework to rule out parameter values that are inconsistent with the observed data, or \\(Pr(d = d' \\mid \\theta = \\theta') \\leq 0.05\\)."
  },
  {
    "objectID": "unit-1/lec-1-2.html#where-does-uncertainty-come-from-in-an-experimental-study",
    "href": "unit-1/lec-1-2.html#where-does-uncertainty-come-from-in-an-experimental-study",
    "title": "Unit 1-2: Statistical Conclusion Validity",
    "section": "Where Does Uncertainty Come From in an Experimental Study?",
    "text": "Where Does Uncertainty Come From in an Experimental Study?\nBefore getting into estimation and the uncertainty of the estimate, we need to be precise about the source of uncertainty. In experimental studies, uncertainty is an inherent part of the inference process, arising from several key sources. Recognizing these sources is critical to designing robust experiments and correctly interpreting the results. The main contributors to uncertainty include:\n\nSampling Variation:\nUncertainty due to sampling variation stems from the fact that any sample drawn from a population is just one of many possible samples. Consequently, the same treatment might yield different results if applied to a different sample, reflecting random fluctuations in the selection process.\nVariance in Potential Outcomes:\nThe natural variability in the potential outcomes (i.e., the outcomes that would be observed under different treatment conditions) can lead to uncertainty. High variance makes it more challenging to detect a true treatment effect because the noise in the data can obscure the signal, thereby reducing the study’s power to reject a false null hypothesis.\nMeasurement Error:\nMeasurement error occurs when there are inaccuracies in recording or assessing the potential outcomes. Such errors introduce additional variability and can bias the estimated treatment effect, further contributing to uncertainty in the experimental results.\n\nTo vizualize this, the diagram below shows a “directed acyclic graph” (DAG) representation of the “data strategy” framework discussed in Chapter 8 of Research Design in the Social Sciences.\n\n\nData Strategy DAG, Source: Research Design in the Social Sciences\n\n\nIn Figure 8.1, we illustrate these three elements of data strategies: sampling (S), treatment assignment (Z), and measurement (Q). These nodes are highlighted by blue boxes to emphasize that they are in the control of the researcher. No arrows go into the S, Z, or Q nodes; they are set by the researcher. In each case, the strategy selected by the researcher affects a corresponding endogenous variable. The sampling procedure causes changes in the endogenous response (R), which represents whether participants provide outcome data, for example responding to survey questions. R is not under the full control of the researchers: it is affected by S, the sampling procedure, but also by the idiosyncratic choices of participants who have higher and lower interest and ability to respond and participate in the study (U). Similarly, the endogenous variable treatment D represents whether participants actually receive the treatment, regardless of their assignment Z. D is affected by the treatment assignment procedure (Z) of course. But except in cases when Z fully determines D (no noncompliance), we are concerned that it will be affected by unobserved idiosyncratic features of individuals U. The third researcher node is Q, the measurement procedure. Q affects Y, the observed outcome, measured by the researcher. Y is also affected by a latent variable Y*, which cannot be directly observed. The measurement procedure provides an imperfect measurement of that latent variable, which is (potentially) affected by treatment D and unobserved heterogeneity U. In the robustness section at the end of the chapter, we explore further variations of this DAG that incorporate threats to inference from noncompliance, attrition, excludability violations, and interference.\n\n\n\n\n\n\n\nTip\n\n\n\nCan you draw four arrows representing the four exclusion restrictions?"
  },
  {
    "objectID": "unit-1/lec-1-2.html#statistical-conclusion-validity",
    "href": "unit-1/lec-1-2.html#statistical-conclusion-validity",
    "title": "Unit 1-2: Statistical Conclusion Validity",
    "section": "Statistical Conclusion Validity",
    "text": "Statistical Conclusion Validity\nGiven that our exclusion restrictions are satisfied, we can estimate the average treatment effect (ATE). The question is then: How can we ensure valid statistical conclusions from our estimate of the ATE?\nWe need to consider the uncertainty in our estimate, and whether we can reject the null hypothesis that the treatment has no effect."
  },
  {
    "objectID": "unit-1/lec-1-2.html#the-super-population-approach",
    "href": "unit-1/lec-1-2.html#the-super-population-approach",
    "title": "Unit 1-2: Statistical Conclusion Validity",
    "section": "The Super-Population Approach",
    "text": "The Super-Population Approach\nThe super-population approach assumes that the study sample is drawn from a larger, hypothetical infinite population represented by a probability distribution, \\(Q\\). This perspective views potential outcomes—both with and without treatment—as stochastic variables drawn from \\(Q\\). The primary goal in this framework is to estimate a feature of this distribution, typically the expected treatment effect:\n\\[\nE[Y(1) - Y(0)]\n\\]\nwhere \\(Y(1)\\) and \\(Y(0)\\) denote the potential outcomes under treatment and control conditions, respectively. Under this approach, each sample is considered an independent and identically distributed (i.i.d.) draw from the distribution \\(Q\\), meaning the researcher is interested in making generalizable inferences beyond the study sample.\nA key implication of this framework is that two sources of variance affect our estimation of treatment effects: 1. Sampling variance—arising from differences between one sample and another. 2. Assignment mechanism variance—introduced by the randomness in treatment assignment.\nThe super-population approach is useful when researchers aim to extend their findings to a broader population, such as in policy recommendations or clinical trials. However, it requires strong assumptions about how well the study sample represents the population."
  },
  {
    "objectID": "unit-1/lec-1-2.html#the-finite-population-approach",
    "href": "unit-1/lec-1-2.html#the-finite-population-approach",
    "title": "Unit 1-2: Statistical Conclusion Validity",
    "section": "The Finite-Population Approach",
    "text": "The Finite-Population Approach\nIn contrast, the finite-population approach considers the sample as a fixed, well-defined group rather than a subset of an infinite population. Here, the researcher is not making inferences beyond the observed sample but instead treating the units as the entire relevant population. This approach is common in evaluations of specific interventions where the focus is on estimating the finite-population average treatment effect (ATE) (also called the sample average treatemnt effect, or SATE):3\n\\[\n\\tau_{fp} = \\frac{1}{N} \\sum_{i=1}^{N} \\left[Y_i(1) - Y_i(0)\\right]\n\\]\nwhere \\(N\\) is the total number of units in the study. Unlike in the super-population approach, the potential outcomes in a finite population framework are fixed, not random. The treatment effect is then viewed as an empirical quantity to be estimated within the sample, rather than a parameter of an underlying distribution.\nA practical distinction between the two approaches is in their implications for statistical inference:\n\nIn the super-population approach, standard errors reflect both sampling variability and randomization-induced variation.\nIn the finite-population approach, standard errors are based only on the variation within the observed sample, without assuming a broader distribution.\n\nThis framework is particularly relevant when researchers are concerned with internal validity over generalizability, such as in program evaluations or field experiments."
  },
  {
    "objectID": "unit-1/lec-1-2.html#subpopulations-in-the-super-population-framework",
    "href": "unit-1/lec-1-2.html#subpopulations-in-the-super-population-framework",
    "title": "Unit 1-2: Statistical Conclusion Validity",
    "section": "Subpopulations in the Super-Population Framework",
    "text": "Subpopulations in the Super-Population Framework\nWithin the super-population framework, researchers often refine their analysis by considering subpopulations to account for heterogeneous treatment effects. One important example is the Conditional Average Treatment Effect (CATE):\n\\[\nE[Y(1, X) - Y(0, X) | X]\n\\]\nwhere \\(X\\) represents observed covariates that influence treatment effects. This approach allows for differentiated insights across groups, such as demographic segments in public health interventions."
  },
  {
    "objectID": "unit-1/lec-1-2.html#choosing-between-the-two-approaches",
    "href": "unit-1/lec-1-2.html#choosing-between-the-two-approaches",
    "title": "Unit 1-2: Statistical Conclusion Validity",
    "section": "Choosing Between the Two Approaches",
    "text": "Choosing Between the Two Approaches\nThe choice between these sampling frameworks depends on the research question:\n\nIf the goal is to make generalizable claims about a broader population, the super-population approach is preferred.\nIf the study focuses on a specific, finite group of units, the finite-population approach is more appropriate.\n\nBoth perspectives provide valuable insights, and many empirical studies incorporate elements of both frameworks, particularly when considering external validity and policy relevance."
  },
  {
    "objectID": "unit-1/lec-1-2.html#statistical-significance-and-the-t-statistic",
    "href": "unit-1/lec-1-2.html#statistical-significance-and-the-t-statistic",
    "title": "Unit 1-2: Statistical Conclusion Validity",
    "section": "Statistical Significance and the t-Statistic",
    "text": "Statistical Significance and the t-Statistic\nTo formally test the null hypothesis, we construct a t-statistic:\n\\[\nt = \\frac{\\hat{\\tau}}{\\text{SE}(\\hat{\\tau})}\n\\]\nwhere SE(\\(\\hat{\\tau}\\)) represents the standard error of the difference-in-means estimator. As the sample size grows, this t-statistic follows a standard normal distribution (or Student’s t-distribution for small samples). A large absolute value of $ t $ provides evidence against the null hypothesis.\nTo determine whether the result is statistically significant, we compare the t-statistic to a critical value determined by our chosen significance level (\\(\\alpha\\), commonly set at 0.05). If the absolute value of the t-statistic exceeds this threshold, we reject the null hypothesis."
  },
  {
    "objectID": "unit-1/lec-1-2.html#type-i-and-type-ii-errors",
    "href": "unit-1/lec-1-2.html#type-i-and-type-ii-errors",
    "title": "Unit 1-2: Statistical Conclusion Validity",
    "section": "Type I and Type II Errors",
    "text": "Type I and Type II Errors\nWhile hypothesis testing provides a structured approach to evaluating treatment effects, errors can still occur:\n\n\nType I Error (\\(\\alpha\\)): Rejecting the null hypothesis when it is actually true (false positive).\n\nControlled by setting the significance level (\\(\\alpha\\)), which determines the probability of mistakenly rejecting \\(H_0\\).\nLower \\(\\alpha\\) reduces false positives but increases the risk of missing real effects.\n\n\n\nType II Error (\\(\\beta\\)): Failing to reject the null hypothesis when it is actually false (false negative).\n\nRelated to statistical power, which is the probability of detecting an effect when it truly exists.\n\n\n\n\n\nType 1 and Type 2 Errors"
  },
  {
    "objectID": "unit-1/lec-1-2.html#power-calculation-ensuring-detectability",
    "href": "unit-1/lec-1-2.html#power-calculation-ensuring-detectability",
    "title": "Unit 1-2: Statistical Conclusion Validity",
    "section": "Power Calculation: Ensuring Detectability",
    "text": "Power Calculation: Ensuring Detectability\nStatistical power refers to the ability of a test to correctly reject the null hypothesis when a true effect exists. Mathematically:\n\\[\n\\text{Power} = 1 - \\beta\n\\]\nFactors influencing power:\n\n\nEffect size (\\(\\tau\\)): Larger effects are easier to detect.\n\nSample size (\\(N\\)): Larger samples reduce variability, increasing power.\n\nSignificance level (\\(\\alpha\\)): Lowering \\(\\alpha\\) increases the risk of missing true effects.\n\nStandard deviation of outcomes: Higher variability in outcomes reduces power.\n\nTo achieve a well-powered experiment, researchers conduct power calculations before data collection to determine the minimum sample size required to detect an effect with reasonable confidence.\nA Simple Example\nLet’s consider a simple example of a power simulation using a simple random assignment to a treatment and control group, where the estimate is the difference-in-means estimator.\n\nlibrary(data.table)\n\nStep 1: Define Simulation Parameters\n\nLet’s assume we have a total sample size of \\(N = 200\\) individuals.\nThe treatment group receives an intervention, while the control group does not.\nThe true treatment effect is set to \\(\\tau = 2\\).\nThe outcome variable follows a normal distribution with mean 10 and standard deviation 4.\n\n\nset.seed(072111)  # Ensures reproducibility\n\n# Define parameters\nN &lt;- 200  # Total sample size\np &lt;- 0.5  # Probability of assignment to treatment\ntrue_tau &lt;- 2  # True treatment effect\nsigma &lt;- 4  # Standard deviation of outcome\n\nStep 2: Simulate Data\nNow we can simulate the data using data.table:\n\n# Simulate data using data.table\ndt &lt;- data.table(id = 1:N)\ndt[, treatment := rbinom(.N, 1, p)]\ndt[, outcome := 10 + true_tau * treatment + rnorm(.N, mean = 0, sd = sigma)]\n\n#display first few rows\nhead(dt)\n\n      id treatment  outcome\n   &lt;int&gt;     &lt;int&gt;    &lt;num&gt;\n1:     1         0 10.39758\n2:     2         1 17.78058\n3:     3         0 11.46925\n4:     4         0 14.56752\n5:     5         1 10.55347\n6:     6         1 14.99933\n\n\nStep 3: Estimate Treatment Effect\n\ndiff_means &lt;- dt[treatment == 1, mean(outcome)] - dt[treatment == 0, mean(outcome)]\nSE &lt;- sqrt(dt[treatment == 1, var(outcome)] / dt[treatment == 1, .N] +\n           dt[treatment == 0, var(outcome)] / dt[treatment == 0, .N])\n\nt_stat &lt;- diff_means / SE  # Compute t-statistic\np_value &lt;- 2 * (1 - pt(abs(t_stat), df = N - 2))  # Two-tailed test\n\n# Display results\ncat(\"Estimated Treatment Effect:\", diff_means, \"\\n\")\n\nEstimated Treatment Effect: 2.453868 \n\ncat(\"p-value:\", p_value, \"\\n\")\n\np-value: 1.088986e-05 \n\n\nStep 4: Power Simulation\nFirst, let’s consider how we will interpret the results:\n\nIf the p-value is less than 0.05, we reject the null hypothesis and conclude that the treatment has a significant effect.\nIf the p-value is greater than 0.05, we fail to reject the null, meaning we do not have enough evidence to confirm a treatment effect.\n\nNow, let’s simulate a power simulation:\nFirst, define the power simulation function:\n\nsimulate_power &lt;- function(N, true_tau, sigma, p, alpha, reps = 1000) {\n  rejections &lt;- 0\n  \n  for (i in 1:reps) {\n    dt &lt;- data.table(id = 1:N)\n    dt[, treatment := rbinom(.N, 1, p)]\n    dt[, outcome := 10 + true_tau * treatment + rnorm(.N, mean = 0, sd = sigma)]\n    diff_means &lt;- dt[treatment == 1, mean(outcome)] - dt[treatment == 0, mean(outcome)]\n    SE &lt;- sqrt(dt[treatment == 1, var(outcome)] / dt[treatment == 1, .N] +\n               dt[treatment == 0, var(outcome)] / dt[treatment == 0, .N])\n    t_stat &lt;- diff_means / SE\n    p_value &lt;- 2 * (1 - pt(abs(t_stat), df = N - 2))\n    \n    if (p_value &lt; alpha) {\n      rejections &lt;- rejections + 1\n    }\n  }\n  return(rejections / reps)\n}\n\nNow, simulate this experiment 1,000 times:\n\n# Run power simulation\npower &lt;- simulate_power(N = 200, true_tau = 2, sigma = 4, p = 0.5, alpha = 0.05, reps = 1000)\ncat(\"Estimated Power:\", power, \"\\n\")\n\nEstimated Power: 0.938 \n\n\nStep 5: Interpreting Power Calculation\n\nThe power of the test is the proportion of simulations in which we correctly reject the null hypothesis when the treatment effect is truly \\(\\tau = 2\\).\nA power value close to 0.80 or higher indicates that the experiment is well-powered.\n\n\n\n\n\n\n\nTip\n\n\n\nTry changing the sample size or effect size, to see how the power changes."
  },
  {
    "objectID": "unit-1/lec-1-2.html#the-mida-framework-for-simulation",
    "href": "unit-1/lec-1-2.html#the-mida-framework-for-simulation",
    "title": "Unit 1-2: Statistical Conclusion Validity",
    "section": "The MIDA Framework for Simulation",
    "text": "The MIDA Framework for Simulation\nA structured way to think about research design and simulation is the MIDA Framework,4(https://book.declaredesign.org/declaration-diagnosis-redesign/research-design.html)] which consists of:\n\n\nM: The Model - The underlying data-generating process that defines the inquiry.\n\nI: The Inquiry - The specific research question we are trying to answer.\n\nD: The Data Strategy - The way we collect and structure data, including sampling and treatment assignment.\n\nA: The Answer Strategy - The statistical method we use to estimate the effect.\n\n\n\nMIDA Framework, Source: Declaration Design\n\n\n\nElements of Research Design, Source: Declaration Design\n\nIn a real-world study, we can only observe a single realization of the design and generate one empirical answer \\(a_d\\). However, through simulation, we can consider many possible data realizations by repeatedly drawing from different models \\(m_1, m_2, ..., m_k\\) within the model space \\(M\\). This allows us to assess how our research design performs across different scenarios."
  },
  {
    "objectID": "unit-1/lec-1-2.html#diagnosing-research-designs-with-simulation",
    "href": "unit-1/lec-1-2.html#diagnosing-research-designs-with-simulation",
    "title": "Unit 1-2: Statistical Conclusion Validity",
    "section": "Diagnosing Research Designs with Simulation",
    "text": "Diagnosing Research Designs with Simulation\nWhen we simulate a research design, we evaluate its performance by considering:\n\n\nBias: How close is the empirical answer \\(a_d\\) to the true answer \\(a_m\\)?\n\nVariance: How much do the empirical answers fluctuate across different realizations?\n\nCoverage: How often do confidence intervals include the true effect?\n\nPower: How frequently does the study correctly reject the null hypothesis when a true effect exists?\n\nThe bottom half of the figure below illustrates how simulation allows us to examine the research design across multiple models (\\(m_1, ..., m_k\\)), generating different answers (\\(a_{m_1}, a_{m_2}, ..., a_{m_k}\\)) and associated datasets (\\(d_1, d_2, ..., d_k\\)). The simulated research design does not have direct access to the true answer but can assess performance across the models under consideration.\n\n\nSimulations in the MIDA Framework"
  },
  {
    "objectID": "unit-1/lec-1-2.html#the-challenge-of-multiple-hypothesis-testing",
    "href": "unit-1/lec-1-2.html#the-challenge-of-multiple-hypothesis-testing",
    "title": "Unit 1-2: Statistical Conclusion Validity",
    "section": "The Challenge of Multiple Hypothesis Testing",
    "text": "The Challenge of Multiple Hypothesis Testing\nIn many empirical settings, researchers conduct multiple hypothesis tests rather than a single test. This introduces the risk of false positives, or mistakenly finding significant effects simply due to chance. Multiple hypothesis testing arises naturally in at least three key scenarios:\n\n\nMultiple Outcomes: When we examine several outcomes (\\(Y_i\\)) to determine whether any are affected by the treatment.\n\nHeterogeneous Treatment Effects (CATEs): When treatment effects vary across subgroups, and we want to assess which subgroups exhibit an effect.\n\nMultiple Treatments: When we compare multiple interventions (\\(D_i\\)) and want to test their effects relative to a control group or to each other.\n\nTo properly interpret results, we need statistical techniques that adjust for multiple comparisons and control the probability of false discoveries."
  },
  {
    "objectID": "unit-1/lec-1-2.html#types-of-multiple-hypothesis-tests",
    "href": "unit-1/lec-1-2.html#types-of-multiple-hypothesis-tests",
    "title": "Unit 1-2: Statistical Conclusion Validity",
    "section": "Types of Multiple Hypothesis Tests",
    "text": "Types of Multiple Hypothesis Tests\nThere are two broad types of statistical hypothesis testing frameworks when dealing with multiple comparisons:\n\n\nJoint Tests: These assess whether at least one hypothesis is true (e.g., “Is at least one treatment effective?”).\n\nSimultaneous Tests: These examine whether multiple hypotheses are true at the same time (e.g., “Are both Treatment A and Treatment B effective?”).\n\nWhen conducting multiple tests, researchers need to control for an increased family-wise error rate (FWER), which is the probability of making at least one Type I error (false positive)."
  },
  {
    "objectID": "unit-1/lec-1-2.html#controlling-the-family-wise-error-rate-fwer",
    "href": "unit-1/lec-1-2.html#controlling-the-family-wise-error-rate-fwer",
    "title": "Unit 1-2: Statistical Conclusion Validity",
    "section": "Controlling the Family-Wise Error Rate (FWER)",
    "text": "Controlling the Family-Wise Error Rate (FWER)\nA single hypothesis test controls the probability of a Type I error at a given significance level (\\(\\alpha\\)). However, with multiple tests, the probability of making at least one false rejection increases:\n\\[\n\\text{FWER} = 1 - (1 - \\alpha)^k\n\\]\nwhere \\(k\\) is the number of tests. For example, if we conduct 5 tests at \\(\\alpha = 0.05\\), the probability of making no Type I errors across all tests is:\n\\[\n(1 - 0.05)^5 = 0.7738\n\\]\nThus, the probability of making at least one Type I error is:\n\\[\nFWER = 1 - 0.7738 = 0.2262\n\\]\nThis means there is a 22.62% chance of mistakenly rejecting at least one null hypothesis across the five tests. If we perform 20 tests, the FWER increases to 64%."
  },
  {
    "objectID": "unit-1/lec-1-2.html#approaches-to-controlling-the-fwer",
    "href": "unit-1/lec-1-2.html#approaches-to-controlling-the-fwer",
    "title": "Unit 1-2: Statistical Conclusion Validity",
    "section": "Approaches to Controlling the FWER",
    "text": "Approaches to Controlling the FWER\nTo mitigate this issue, researchers employ various multiple testing corrections, such as:\n\n\nBonferroni Correction: Adjusts the significance level by dividing \\(\\alpha\\) by the number of tests: \\(\\alpha^* = \\alpha / k\\). This is simple but conservative.\n\nHolm Method: A stepwise procedure that ranks p-values and adjusts them sequentially to control the FWER more efficiently.\n\nModern Approaches (e.g., Westfall-Young, Benjamini-Hochberg FDR control): These methods control for false discovery rates and are widely used in large-scale testing."
  },
  {
    "objectID": "unit-1/lec-1-2.html#simulating-multiple-hypothesis-testing-in-r",
    "href": "unit-1/lec-1-2.html#simulating-multiple-hypothesis-testing-in-r",
    "title": "Unit 1-2: Statistical Conclusion Validity",
    "section": "Simulating Multiple Hypothesis Testing in R",
    "text": "Simulating Multiple Hypothesis Testing in R\nTo illustrate the impact of multiple testing and FWER correction, let’s do a simulation in R:\n\nlibrary(data.table)\nset.seed(072111)  # Ensures reproducibility\n\n# Define parameters\nN &lt;- 200  # Sample size\nk &lt;- 10  # Number of hypothesis tests\nalpha &lt;- 0.05  # Significance level\n\n# Simulate k independent hypothesis tests\ndt &lt;- data.table(test_id = 1:k)\ndt[, p_value := runif(.N, min = 0, max = 1)]  # Generate uniform random p-values\n\ndt[, bonferroni := p_value &lt; (alpha / k)]  # Bonferroni correction\ndt[, holm := p.adjust(p_value, method = \"holm\") &lt; alpha]  # Holm correction\n\ndt[, naive_reject := p_value &lt; alpha]  # Standard test (without correction)\n\n# Count false positives\nfalse_discoveries &lt;- dt[, sum(naive_reject)]\nadjusted_false_discoveries &lt;- dt[, sum(bonferroni)]\nholm_false_discoveries &lt;- dt[, sum(holm)]\n\ncat(\"False positives (no correction):\", false_discoveries, \"\\n\")\n\nFalse positives (no correction): 2 \n\ncat(\"False positives (Bonferroni correction):\", adjusted_false_discoveries, \"\\n\")\n\nFalse positives (Bonferroni correction): 0 \n\ncat(\"False positives (Holm correction):\", holm_false_discoveries, \"\\n\")\n\nFalse positives (Holm correction): 0 \n\n\nInterpreting the Simulation\n\n\nWithout correction, we expect around \\(\\alpha \\times k\\) false discoveries.\n\nBonferroni correction sharply reduces false positives but may be overly conservative.\n\nHolm correction provides a better balance, controlling FWER while maintaining statistical power.\n\n\n\n\n\n\n\nTip\n\n\n\nNote: In the lab, we’ll cover some modern approaches that are more powerful but a bit more complex."
  },
  {
    "objectID": "unit-1/lec-1-2.html#footnotes",
    "href": "unit-1/lec-1-2.html#footnotes",
    "title": "Unit 1-2: Statistical Conclusion Validity",
    "section": "Footnotes",
    "text": "Footnotes\n\nReference: Research Design in the Social Sciences↩︎\nCan you explain in words how this differs from the Bayesian probability above?↩︎\nWager Chapter 1 refers to this as SATE.↩︎\nReference: Declaration Design↩︎"
  }
]