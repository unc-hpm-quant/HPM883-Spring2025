{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Unit 2: Design of Experiments\"\n",
        "Subtitle: \"Optimal Experimental Design\"\n",
        "author: \"Sean Sylvia, Ph.D.\"\n",
        "date: February 18, 2025\n",
        "format:\n",
        "  html:\n",
        "    toc: true\n",
        "    toc-depth: 2\n",
        "execute:\n",
        "  echo: true\n",
        "  warning: false\n",
        "  message: false\n",
        "draft: true\n",
        "---\n",
        "\n",
        "\n",
        "## Introduction\n",
        "\n",
        "Welcome, dear students, to another joyous adventure in the realm of experimental design! In our previous sessions, we tackled the fundamentals of **statistical conclusion validity** and dipped our toes into **power calculations** and **simulation**. We even brushed up against the perils of **clustering**, which appears A LOT in health services research since this is how health care is organized -- think hospitals, clinics, communities, social networks, etc.\n",
        "\n",
        "Now, we turn the tables. Instead of passively accepting whatever nature (or large administrative database) throws at us, we’re in control. Unlike our secondary data analysis friends, we get to *design* our experiments to make the most of the resources we have. Specifically, we're interested in maximizing our abilitly to learn (i.e. statistical power) subject to our constraints (e.g. budget, logistical). \n",
        "\n",
        "Of course, nothing is for free; this is both a blessing and a curse. Tradeoffs abound as usual. We need a framework for thinking about how to optimally weigh costs and benefits. If only there was an entire discipline devoted to this.....OH, WAIT! (Yes, my friends, you are all economists now. You’re welcome.) As an experimentalist, you can optimize your design choices in ways our secondary-data-using colleagues can only dream of (or envy, or curse, depending on their temperament). So let’s dig in, shall we?\n",
        "\n",
        "::: {.callout-note}\n",
        "**Note:** Suboptimal design choices won’t necessarily ruin your study’s internal validity, but they will keep you from making the *best* inference possible. And that might cost you that sweet, sweet grant renewal next year.\n",
        ":::\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Optimal Experimental Design: Insights from Econ 101\n",
        "\n",
        "### 1.1 Our Objective\n",
        "\n",
        "Thinking back to Econ 101,^[If you were daydreaming in Econ 101, fear not. We’ll keep things simple.] recall that we can pose an optimization problem as maximizing (or minimizing) an objective function subject to constraints. In our case, we’ll use this to set up our experimental design problem, i.e,\n",
        "\n",
        "> **Objective function:** Statistical power,  \n",
        "> **Subject to:** Budget constraints.\n",
        "\n",
        "In other words, we want to choose our design to maximize power subject to our budget (or other) constraints. It turns out that there are loads of things in our control; usually the only things that aren't are feasibility and the budget we have to work with.\n",
        "\n",
        "**Concept Map**\n",
        "\n",
        "\n",
        "\n",
        "```{markdown}\n",
        "#| label: \"Concept Map\"\n",
        "#| code-fold: true\n",
        "#| code-summary: \"Expand Code\"\n",
        "\n",
        "flowchart LR\n",
        "    A((\"To calculate optimal sample sizes, consider:\")):::redBubble\n",
        "    A --> B[\"Desired significance level\"]:::greenBubble\n",
        "    A --> C[\"Desired statistical power\"]:::greenBubble\n",
        "    A --> D[\"Minimum detectable effect size\"]:::greenBubble\n",
        "    A --> E[\"Experimental budget and treatment costs\"]:::greenBubble\n",
        "    A --> F[\"How the data will be analyzed\"]:::greenBubble\n",
        "    A --> G[\"Available pre-treatment covariates\"]:::greenBubble\n",
        "    \n",
        "    F --> H[\"Unit of assignment\"]:::greenOval\n",
        "    F --> I[\"Number of distinct outcomes of interest\"]:::greenOval\n",
        "    \n",
        "    H --> J([\"Design with clustered assignment\"]):::blueBlock\n",
        "    I --> K([\"Design for multiple hypothesis testing\"]):::blueBlock\n",
        "\n",
        "    classDef redBubble fill:#fbbbbb,stroke:#900,stroke-width:1px,color:#000,margin:8px\n",
        "    classDef greenBubble fill:#c7ecc7,stroke:#080,stroke-width:1px,color:#000,margin:8px\n",
        "    classDef greenOval fill:#c7ecc7,stroke:#080,stroke-width:2px,stroke-dasharray:3,margin:8px\n",
        "    classDef blueBlock fill:#c1e1f9,stroke:#048,stroke-width:1px,color:#000,margin:8px\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "### 1.2 A Simple Setup\n",
        "\n",
        "To make this concrete, imagine you’ve received a research grant (yay!) or have a wealthy aunt who’s willing to bankroll your next foray into experimental design. You have two arms in your study:\n",
        "\n",
        "1. A **control** group (no intervention).  \n",
        "2. A **treatment** group (some fancy new health intervention).\n",
        "\n",
        "And you have a single, continuous outcome measure, say:  \n",
        "$$\n",
        "\\text{Health and Happiness Index}\n",
        "$$\n",
        "\n",
        "Now, your big question: **How many participants do you need?** How do you split that precious sample between treatment and control?  \n",
        "\n",
        "### 1.3 The Big Three Elements\n",
        "\n",
        "When computing your required sample size (or deciding the “optimal” split), there are three main ingredients:\n",
        "\n",
        "1. **Significance level** ($\\alpha$): The probability of a false positive (rejecting the null when it’s actually true).  \n",
        "2. **Minimum Detectable Effect (MDE)**: The smallest true effect size you want to be able to detect with high probability.  \n",
        "3. **Power** ($1 - \\beta$): The probability of detecting a true effect (i.e., rejecting the null when it’s false).\n",
        "\n",
        "> **Important:** The MDE is *not* the effect you *expect* to see. It’s the smallest effect you *care* to rule in or rule out. People often mix these up, leading to underpowered studies, heartbreak, and wasted coffee budgets.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. The Variance of the Average Treatment Effect (ATE)\n",
        "\n",
        "Before we get into fancy cost functions, we need the formula for the variance of the ATE estimator. Let’s do a quick recap:\n",
        "\n",
        "\\[\n",
        "\\widehat{\\text{ATE}} = \\overline{Y}_1 - \\overline{Y}_0,\n",
        "\\]\n",
        "\n",
        "where \\(\\overline{Y}_1\\) and \\(\\overline{Y}_0\\) are sample means in the treatment and control groups, respectively. The variance depends on:\n",
        "\n",
        "- The underlying outcome variance(s).  \n",
        "- The total number of units \\(N\\).  \n",
        "- The distribution of treatment indicators \\(D\\).  \n",
        "\n",
        "If we let \\(p\\) be the fraction of participants assigned to treatment, then \\(\\mathrm{Var}(D) = p(1 - p)\\). The simpler the design, the easier the formula.\n"
      ],
      "id": "ff8bcaef"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}