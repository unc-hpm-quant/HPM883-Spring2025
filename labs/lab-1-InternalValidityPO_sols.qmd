---
title: Lab 1 Solutions
subtitle: The Hospital of Uncertain Outcomes
author: Sean Sylvia
date: "2025-01-22"
slug: lab-1
categories: 
    - Lab
    - Internal Validity
description: "Internal Validity, Potential Outcomes"
draft: false
format: html
editor: visual
execute:
  eval: true
---

::: callout-note
## Results may differ

This is only one of many possible ways to complete this lab. Your final code may look different, which is fine! In fact, it is good practice to experiment with different approaches.
:::

# Lab 1: The Hospital of Uncertain Outcomes

## Overview and Learning Objectives

In this lab, we will explore **internal validity** and the **potential outcomes framework** using simulated health data from the endlessly eventful **St. Null’s Memorial Hospital**. Specifically, we will recreate a scenario where a new intervention (putting patients on ventilators) may or may not reduce patient mortality. As you’ll discover, chaos at the hospital has made it far from straightforward to identify causal effects.

By the end of this lab, you will be able to:

-   Understand the concept of **potential outcomes** and **causal effects**.
-   Apply **randomization inference** to estimate treatment effects.
-   Identify **threats to internal validity** and explore possible solutions.
-   Implement basic **difference-in-means estimation** in R.
-   Use the **WebR** package to interactively run and modify code.

## Instructions

1.  Open this `.qmd` file in RStudio or another Quarto-supported editor.
2.  Follow the guided coding prompts below, completing the missing code blocks.
3.  Submit your completed lab on **Gradescope** \[Insert Link Here\] by **\[Insert Deadline Here\]**.

## Scenario

### The Hospital of Uncertain Outcomes

Welcome to St. Null’s Memorial Hospital—an institution where the only constant is confusion. The hospital board—led by the well-meaning but trend-obsessed CEO, **Barnaby Beta**—changes policies so often that nobody knows what’s going on.

Worse yet is Dr. P-Hacker, a “data guru” who prefers p-values to patients. He mines the electronic health records (EHR) until something (anything!) is “significant.” Meanwhile, **Nurse Random** tries to keep everything on track, pointing out that good causal methods can be more important than good vibes. Lastly, **Dr. Doub R. Obust** lurks in the background, waiting for a chance to champion doubly robust methods that might someday save everyone’s sanity.

::: {style="float: right; width: 480px; margin: 1em;"}
<iframe src="https://drive.google.com/file/d/1-F2hSgH6J7OKh0a_VwE8pXz3v9fFZAgT/preview" width="480" height="270" allow="autoplay">

</iframe>
:::

You and your team of budding methodologists are the new consultants hired to impose some order on this bedlam. In each module, you’ll tackle another fiasco at St. Null’s—from overfitted AI catastrophes to weird missing-data mishaps—and attempt to restore some semblance of methodological rigor. Good luck!

## Your Mission

### The Pandemic Mystery at St. Null’s

A mysterious respiratory illness has swept through St. Null’s, leaving every ward scrambling. The question at hand is whether putting these patients on ventilators prolongs their lifespans. CEO Barnaby Beta wants quick answers (“If TikTok can do it, so can we!”). Dr. P-Hacker gleefully promises “instant significance,” claiming all he needs is the hospital’s EHR from the past week.

But Nurse Random, unimpressed, insists that the hospital’s chaotic, ad hoc ventilator assignments will cloud any conclusions. Dr. Doub R. Obust nods knowingly. They call in your team for an unbiased, data-driven approach.

In this lab, you’ll **simulate a dataset** of 100,000 patients that captures both “potential outcomes” (i.e., what would happen if a patient were ventilated vs. not ventilated). This magical glimpse at parallel universes is impossible in real-world data, but here it will let us see exactly how different analytic approaches fare in the face of selection bias.

# Install Packages if not installed

```{r}
required_packages = c("fixest", "data.table")
for(pkg in required_packages){
  if(!require(pkg, character.only = TRUE)) install.packages(pkg)
}
```

::: callout-note
#### **Explanation:**

1.  This code first defines a vector `required_packages` containing the package names `"fixest"` and `"data.table"`.

2.  Then, it loops through each package:

    -   It checks if the package is already installed using `require(pkg, character.only = TRUE)`.

    -   If the package is not installed, it installs it using `install.packages(pkg)`.

3.  This ensures that all necessary packages are available for use.
:::

## Step 1: Simulating the Dataset

Because the EHR system at St. Null’s is, in a word, “unreliable” (or, in two words, “utterly broken”), we’ll create our own dataset in R.

Run the following code to generate 100,000 patient records along with **potential outcomes** (`y0` if no ventilator, `y1` if ventilated). Each outcome is the patient’s lifespan (in some made-up units). Note that lifespans below zero are set to zero—any negative numbers would just be an artifact of Dr. P-Hacker’s bizarre data extraction methods.

```{r}
library(fixest)
library(data.table)
set.seed(072121)

# 100,000 people with differing levels of covid symptoms
N_people = 100000
df = data.table(person = 1:N_people)

# Potential outcomes (Y0): life-span if no vent
df[, y0 := rnorm(N_people, 9.4, 4)]
df[y0 < 0, y0 := 0]

# Potential outcomes (Y1): life-span if assigned to vents
df[, y1 := rnorm(N_people, 10, 4)]
df[y1 < 0, y1 := 0]
```

::: callout-note
#### **Explanation:**

1.  The necessary packages (`fixest` and `data.table`) are loaded.

2.  The `set.seed(072121)` ensures reproducibility, meaning the random numbers generated will be the same every time the code is run.

3.  A dataset `df` is created using `data.table` with 100,000 individuals (each represented by a row).

4.  **Simulating Potential Outcomes:**

    -   `y0` represents the expected lifespan (in years) if no ventilator treatment is given. It is drawn from a normal distribution with:

        -   Mean: 9.4 years

        -   Standard deviation: 4 years

    -   Any `y0` values below 0 (i.e., negative lifespans) are set to zero.

    -   `y1` represents the expected lifespan if assigned to a ventilator. It is similarly drawn from a normal distribution but with a slightly higher mean of 10 years.

    -   Again, negative lifespan values are replaced with 0.
:::

We also define the **individual treatment effect** for each patient. Dr. Doub R. Obust is thrilled, because for once, we have both `y0` and `y1` simultaneously—an impossible dream in real life!

```{r}
# Define individual treatment effect
df[, delta := y1 - y0]
```

::: callout-note
#### **Explanation:**

1.  A new column `delta` is created, which represents the individual treatment effect (ITE) for each person.

2.  The ITE is calculated as the difference between the potential outcome under treatment (`y1`) and the potential outcome under control (`y0`).

3.  This measures how much additional lifespan (if any) the ventilator treatment provides.
:::

::: callout-note
## Alternative Approach

-   The column can be created using `mutate()` from `dplyr` if using `tidyverse`:

    ``` r
    library(dplyr)
    df <- df %>% mutate(delta = y1 - y0)
    ```
:::

## Step 2: The Two Doctors

St. Null’s has two very different doctors assigning ventilators:

-   **Dr. Perfect:** A mystical being who can see into each patient’s future and only gives ventilators to those who would benefit from them.
-   **Dr. Bad:** The name says it all. This doctor assigns ventilators randomly—could be beneficial, could not be. Who knows?

### Step 2a: Assigning Doctors

First, we randomly assign each patient to one of these two doctors. (No wonder this hospital is chaotic...)

```{r}
# Assign doctors randomly
df[, doctor := sample(c("perfect", "bad"), N_people, replace = TRUE)]
```

::: callout-note
#### **Explanation**

1.  **`sample(c("perfect", "bad"), N_people, replace = TRUE)`**

    -   `sample(x, size, replace)` randomly selects `size` elements from the vector `x`.

    -   Here, `x = c("perfect", "bad")`, meaning we are choosing between `"perfect"` and `"bad"`.

    -   `size = N_people` ensures that we assign a doctor to all `N_people` individuals.

    -   `replace = TRUE` allows values to be selected independently, meaning each person is assigned a doctor without affecting others.

2.  **`df[, doctor := ...]` (Data.table Syntax)**

    -   `df[, column_name := value]` is `data.table`’s syntax for adding or modifying a column.

    -   Here, `doctor` is created and populated with `"perfect"` or `"bad"` based on the `sample()` function.
:::

::: callout-note
#### **Alternative Approach:**

-   Using `mutate()` from `dplyr`:

    ```         
    df <- df %>%
      mutate(doctor = sample(c("perfect", "bad"), N_people, replace = TRUE))
    ```

-   Instead of using `sample()`, one could use `runif()` :

    ```         
    df[, doctor := ifelse(runif(N_people) > 0.5, "perfect", "bad")]
    ```

    This approach provides more flexibility if you want to adjust the probability of assigning each type of doctor.
:::

### Step 2b: Assigning Ventilators

Next, each doctor does what they do best. Dr. Perfect uses clairvoyance to treat only those who stand to gain (`delta > 0`). Dr. Bad flips a metaphorical coin:

```{r}
# Perfect doctor assigns vents only to those who benefit
df[doctor == "perfect", vents := (delta > 0)]

# Random doctor assigns vents randomly
df[doctor == "bad", vents := sample(c(TRUE, FALSE), .N, replace = TRUE)]
```

::: callout-note
#### **Explanation:**

-   **`df[doctor == "perfect", vents := (delta > 0)]`**

    -   `df[...]` is `data.table`'s way of selecting rows where `doctor == "perfect"`.

    -   `vents := (delta > 0)` assigns `TRUE` if `delta > 0`, meaning treatment is given if the patient benefits.

    -   The `:=` operator modifies the column **in place**, making it more memory-efficient than base R.

-   **`df[doctor == "bad", vents := sample(c(TRUE, FALSE), .N, replace = TRUE)]`**

    -   `.N` represents the number of rows in the subset (`doctor == "bad"`), ensuring the right number of values is generated.

    -   `sample(c(TRUE, FALSE), .N, replace = TRUE)` randomly assigns `TRUE` (ventilator given) or `FALSE` (no ventilator) to these individuals.

    -   Each person is assigned independently due to `replace = TRUE`.
:::

It’s not exactly a model of ethical clarity, but it certainly demonstrates the complications of “treatment assignment” in the real world (or the real St. Null’s world).

## Step 3: Computing Causal Parameters

Now, let’s calculate the key causal parameters:

-   **Average Treatment Effect (ATE)**: The overall difference in outcomes if everyone were ventilated vs. if no one were ventilated.
-   **Average Treatment Effect on the Treated (ATT)**: The effect of ventilation on those who actually *received* ventilation.
-   **Average Treatment Effect on the Untreated (ATU)**: The effect of ventilation on those who were *not* ventilated.

```{r}
# Calculate all aggregate Causal Parameters (ATE, ATT, ATU)
ate = df[, mean(delta)]
att = df[vents == TRUE, mean(delta)]
atu = df[vents == FALSE, mean(delta)]

cat(sprintf("ATE = %.03f
", ate))
cat(sprintf("ATT = %.03f
", att))
cat(sprintf("ATU = %.03f
", atu))
```

::: callout-note
#### **Explanation:**

1.  **`df[, mean(delta)]`**

    -   `mean(delta)` computes the average of `delta`, which represents the **Average Treatment Effect (ATE)**.

    -   Since no filtering is applied, it considers all individuals in the dataset.

2.  **`df[vents == TRUE, mean(delta)]`**

    -   `df[...]` selects rows where `vents == TRUE` (patients who received ventilation).

    -   `mean(delta)` then computes the **Average Treatment Effect on the Treated (ATT)**.

3.  **`df[vents == FALSE, mean(delta)]`**

    -   This selects individuals **who were not treated** (`vents == FALSE`).

    -   `mean(delta)` calculates the **Average Treatment Effect on the Untreated (ATU)**.

4.  **`cat(sprintf("ATE = %.03f\n", ate))`**

    -   `sprintf("ATE = %.03f\n", ate)` formats `ate` to **3 decimal places**.

    -   `cat()` prints the formatted result to the console.
:::

Dr. P-Hacker would stop right here and rejoice: “We have all the significance we need!” But hold your celebratory balloon drop—there’s more to do.

## Step 4: Selection Bias and Realized Outcomes

Although the dataset has both `y0` and `y1`, in the real world, a patient’s outcome is observed under only one condition (treated or untreated). Let’s capture *which* outcome we’d actually see based on the ventilator assignment:

```{r}
# Use the switching equation to select realized outcomes from potential outcomes based on treatment assignment
df[, y := vents * y1 + (1 - vents) * y0]
```

::: callout-note
####  **Explanation :**

1.  **`vents * y1 + (1 - vents) * y0`**

    -   This applies the **switching equation**, which determines the observed outcome (`y`) based on whether an individual received treatment.

    -   If `vents == TRUE` (1), the observed outcome is `y1` (the treated potential outcome).

    -   If `vents == FALSE` (0), the observed outcome is `y0` (the untreated potential outcome).

    -   Mathematically, this follows:

        -   y=vents×y1+(1−vents)×y0

2.  **`df[, y := ...]`**

    -   This is `data.table` syntax for creating or modifying a column in place.

    -   The new column `y` represents the **observed lifespan** for each individual.
:::

### Selection Bias Calculation

We’ll see if there is selection bias by comparing the expected lifespan of ventilated patients *had they not been ventilated* to the expected lifespan of non-ventilated patients.

```{r}
# Calculate EY0 for vent group and no vent group
ey01 = df[vents == TRUE, mean(y0)]  
ey00 = df[vents == FALSE, mean(y0)] 

# Calculate selection bias
selection_bias = (ey01 - ey00)

cat(sprintf(
  "Selection Bias = %.03f - %.03f = %.03f 
", 
  ey01, ey00, selection_bias
))
```

::: callout-note
#### **Detailed Explanation of Commands:**

1.  **`df[vents == TRUE, mean(y0)]`**

    -   This calculates the expected `y0` (lifespan without ventilation) **for people who actually received a ventilator**.

    -   It measures the **counterfactual** lifespan if the treated group had not received treatment.

2.  **`df[vents == FALSE, mean(y0)]`**

    -   This calculates the expected `y0` **for people who did not receive a ventilator**.

    -   It represents their actual untreated lifespan.

3.  **Selection Bias Calculation**

    -   `selection_bias = (ey01 - ey00)` compares these two means.

    -   If selection into treatment is **not random**, then the untreated potential outcome (`y0`) may differ between the treated and untreated groups.

4.  **Formatted Output Using `sprintf()`**

    -   `sprintf()` formats the output to three decimal places.

    -   `cat()` prints the formatted text to the console.
:::

If Dr. Perfect is involved, we’d expect some big differences here. Dr. P-Hacker would probably ignore that and claim victory anyway. (He likes ignoring inconvenient truths.)

## Step 5: Comparing Outcomes Between Groups

One of the simplest ways to estimate the treatment effect is to look at the **Simple Difference in Outcomes (SDO)**—the difference in the observed mean outcome between those who got the treatment (ventilators) and those who did not.

```{r}
# Calculate the share of units treated with vents (pi)
pi = mean(df$vents)

# Manually calculate the simple difference in mean health outcomes
ey1 = df[vents == TRUE, mean(y)]
ey0 = df[vents == FALSE, mean(y)]
sdo = ey1 - ey0

cat(sprintf(
  "Simple Difference-in-Outcomes = %.03f - %.03f = %.03f 
", 
  ey1, ey0, sdo
))
```

::: callout-note
####  **Explanation:**

1.  **`pi = mean(df$vents)`**

    -   `df$vents` extracts the `vents` column as a vector.

    -   `mean(df$vents)` computes the proportion of individuals who received ventilation (`TRUE` is treated as 1, `FALSE` as 0).

    -   This provides **π (pi), the treatment probability**.

2.  **`df[vents == TRUE, mean(y)]`**

    -   Calculates the mean observed outcome `y` (lifespan) **for the treated group**.

3.  **`df[vents == FALSE, mean(y)]`**

    -   Calculates the mean observed outcome `y` **for the untreated group**.

4.  **Simple Difference in Outcomes (SDO)**

    -   `sdo = ey1 - ey0` computes the naive difference in means.

    -   This is an **unadjusted estimate of the treatment effect**, which may be biased if treatment selection was non-random.
:::

Dr. P-Hacker would run around shouting: “Aha! This difference proves the intervention works!” or “Aha! It’s not significant!” depending on the p-value. Let’s see if we can do better.

## Step 6: Estimating the Effect with Regression

To confirm our findings, let’s do an **Ordinary Least Squares (OLS)** regression of the realized outcome on the ventilator indicator:

```{r}
# Estimate the treatment effect using OLS
reg = feols(
  y ~ vents, data = df, 
  vcov = "hc1"
)

cat("
")
print(reg)
```

::: callout-note
####  **Explanation :**

1.  **`feols(y ~ vents, data = df, vcov = "hc1")`**

    -   `feols()` from the `fixest` package estimates an **Ordinary Least Squares (OLS) regression**.

    -   The formula `y ~ vents` models the observed outcome (`y`) as a function of treatment (`vents`).

    -   This provides an estimate of the **average treatment effect (ATE)** under the assumption of unconfoundedness (i.e., no omitted variables affecting both treatment and outcome).

    -   **`vcov = "hc1"`** specifies robust standard errors (Huber-White correction), which adjust for heteroskedasticity in the residuals.
:::

::: callout-note
#### **Alternative Approach:**

-   Using `tidyverse` with `broom`:

    ```         
    library(broom)
    reg %>% tidy()
    ```

-   If clustering standard errors is needed:

    ```         
    reg = feols(y ~ vents, data = df, vcov = "cluster")
    ```
:::

This approach, while straightforward, is still subject to any biases from non-random assignment—like, say, Dr. Perfect or Dr. Bad being in charge.

## Step 7: Validating the Decomposition

At this point, we’d like to see if the **Simple Difference in Outcomes (SDO)** can be broken down into our measured parameters. You’ll fill in a table comparing **Dr. Perfect** to **Dr. Bad**, computing the ATE, ATT, ATU, selection bias, SDO, and so on.

Below is a quick consistency check to see if the SDO lines up with our theoretical decomposition:

```{r}
# Decomposition check
sdo_check = ate + selection_bias + (1 - pi) * (att - atu)

cat(sprintf("SDO Check = %.03f 
", sdo_check))
```

::: callout-note
#### **Detailed Explanation of Commands:**

1.  **Decomposition Formula:**

    -   The simple difference in outcomes (SDO) can be decomposed into:

        \text{SDO} = \text{ATE} + \text{Selection Bias} + (1 - \pi) \times (\text{ATT} - \text{ATU})

    -   This checks whether our earlier calculations are consistent with theoretical expectations.

2.  **`sdo_check = ate + selection_bias + (1 - pi) * (att - atu)`**

    -   `ate`: The overall average treatment effect.

    -   `selection_bias`: The difference in untreated potential outcomes between treated and untreated individuals.

    -   `(1 - pi) * (att - atu)`: Adjusts for differences in treatment assignment proportions.
:::

When you fill out the table, you should include:

| Perfect Doctor       | Bad Doctor | Causal Parameter |
|----------------------|------------|------------------|
| ATE                  |            |                  |
| ATT                  |            |                  |
| ATU                  |            |                  |
| Selection bias terms |            |                  |
| E\[Y0 \| D=1\]       |            |                  |
| E\[Y0 \| D=0\]       |            |                  |
| Selection bias       |            |                  |
| Calculations         |            |                  |
| Pi (share on vents)  |            |                  |
| SDO manual           |            |                  |
| SDO OLS              |            |                  |
| SDO Decomposition    |            |                  |
| Obs                  |            |                  |

### Reflection

Finally, reflect on these questions:

-   Is the **Simple Difference in Outcomes (SDO)** enough to identify the **ATE**, **ATT**, or **ATU**?
-   How does **selection bias** play into interpreting the SDO?
-   What lessons would Nurse Random want Dr. P-Hacker to learn about data and design?

(Extra credit if you can imagine the dramatic showdown when Dr. Doub R. Obust finally unveils a doubly robust method that saves the day—but that’s a story for a future lab!)

That’s it! You’ve run the gauntlet of the Hospital of Uncertain Outcomes and lived to tell the tale. Now submit your work, and good luck diagnosing more of St. Null’s methodological maladies!